{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   page_likes  page_checkin  daily_crowd  page_category   F1          F2  \\\n",
      "0      400487             0        57833             13  414   26.635492   \n",
      "1     2667410           141       111855             18  793  185.317073   \n",
      "2     2411555             0        61797              9  560  103.860465   \n",
      "3        1013             0            1             85   16    2.224299   \n",
      "4      367390             0         2678            100  110   11.828175   \n",
      "\n",
      "      F3          F4  F5   F6   ...     c4  c5  base_time  post_length  \\\n",
      "0   15.0   44.941598   0  300   ...     43  43         24          100   \n",
      "1  157.0  164.739179   0  419   ...    216 -29         68          118   \n",
      "2   78.0  107.011971   0  325   ...      0   0          0          119   \n",
      "3    1.0    3.489124   0    7   ...      1   1          1            0   \n",
      "4    6.0   15.430200   0   96   ...      9   9         19           53   \n",
      "\n",
      "   share_count  promotion  h_target   post_day  basetime_day  target  \n",
      "0           20          0        24  wednesday      thursday       2  \n",
      "1           71          0        24   saturday       tuesday       3  \n",
      "2          207          0        24     friday        friday       3  \n",
      "3            1          0        24    tuesday       tuesday       1  \n",
      "4            4          0        24     friday      saturday       3  \n",
      "\n",
      "[5 rows x 25 columns]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "32000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('./../data/train.csv')\n",
    "train_mean=0\n",
    "train_std=0\n",
    "print(df.head())\n",
    "len(df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessData(df):\n",
    "    df['post_day'] = df['post_day'].factorize(sort=True)[0]\n",
    "    df['basetime_day'] = df['basetime_day'].factorize(sort=True)[0]\n",
    "    df = df.sample(frac=1).reset_index(drop=True)\n",
    "    df=df.head(2000)\n",
    "#     print(df)\n",
    "    df_norm=df.drop(labels='target', axis=1)\n",
    "    print(df_norm.mean())\n",
    "    print(df_norm.std())\n",
    "    df_norm=(df_norm-df_norm.mean())/df_norm.std()\n",
    "    global train_mean\n",
    "    global train_std\n",
    "    train_mean=df_norm.mean()\n",
    "    train_std=df_norm.std()\n",
    "#     df_norm=(df_norm-df_norm.min())/(df_norm.max()-df_norm.min())\n",
    "    df_norm.fillna(0,inplace=True)\n",
    "    print(df_norm.head(5))\n",
    "    df_norm=pd.concat([df_norm, df['target']], axis=1)\n",
    "#     print(df_norm)\n",
    "#     df_norm=df_norm[['page_likes','daily_crowd','target']]\n",
    "    print(df_norm)\n",
    "    return df_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_likes       1.562306e+06\n",
      "page_checkin     4.534792e+03\n",
      "daily_crowd      4.891166e+04\n",
      "page_category    2.444850e+01\n",
      "F1               4.864565e+02\n",
      "F2               5.659028e+01\n",
      "F3               3.605875e+01\n",
      "F4               6.867570e+01\n",
      "F5               9.200000e-02\n",
      "F6               3.814870e+02\n",
      "F7               2.209677e+01\n",
      "F8               7.237000e+00\n",
      "c1               6.185450e+01\n",
      "c2               2.146650e+01\n",
      "c3               2.263100e+01\n",
      "c4               5.803050e+01\n",
      "c5              -1.164500e+00\n",
      "base_time        3.533450e+01\n",
      "post_length      1.594690e+02\n",
      "share_count      1.372305e+02\n",
      "promotion        0.000000e+00\n",
      "h_target         2.379850e+01\n",
      "post_day         3.086000e+00\n",
      "basetime_day     3.079500e+00\n",
      "dtype: float64\n",
      "page_likes       1.130892e+07\n",
      "page_checkin     2.035203e+04\n",
      "daily_crowd      1.335142e+05\n",
      "page_category    1.991760e+01\n",
      "F1               5.446901e+02\n",
      "F2               8.925000e+01\n",
      "F3               7.323778e+01\n",
      "F4               8.337938e+01\n",
      "F5               1.872509e+00\n",
      "F6               4.549797e+02\n",
      "F7               3.655434e+01\n",
      "F8               1.983478e+01\n",
      "c1               1.552976e+02\n",
      "c2               7.974153e+01\n",
      "c3               7.601619e+01\n",
      "c4               1.434324e+02\n",
      "c5               9.679854e+01\n",
      "base_time        2.090357e+01\n",
      "post_length      2.691188e+02\n",
      "share_count      6.016892e+02\n",
      "promotion        0.000000e+00\n",
      "h_target         1.656282e+00\n",
      "post_day         2.054438e+00\n",
      "basetime_day     1.996540e+00\n",
      "dtype: float64\n",
      "   page_likes  page_checkin  daily_crowd  page_category        F1        F2  \\\n",
      "0   -0.117628     -0.222818    -0.328944      -0.022518 -0.729693 -0.496875   \n",
      "1   -0.135873     -0.222818    -0.339482      -1.127069 -0.782934 -0.382480   \n",
      "2   -0.135748     -0.222818    -0.349413       0.178310 -0.845355 -0.586190   \n",
      "3    0.903758     -0.222769    -0.280799      -0.524586  0.349820  0.666701   \n",
      "4   -0.081468     -0.222818     0.264963      -1.177276 -0.726021 -0.504619   \n",
      "\n",
      "         F3        F4        F5        F6      ...             c3        c4  \\\n",
      "0 -0.355810 -0.678899 -0.049132 -0.678024      ...      -0.297713 -0.348809   \n",
      "1 -0.191960 -0.679965 -0.049132 -0.721982      ...      -0.231937 -0.320921   \n",
      "2 -0.451389 -0.765848 -0.049132 -0.792314      ...      -0.297713 -0.404584   \n",
      "3  0.401995  0.803422 -0.049132  0.442905      ...       1.162502  0.780643   \n",
      "4 -0.383119 -0.673684 -0.049132 -0.667034      ...      -0.218782 -0.320921   \n",
      "\n",
      "         c5  base_time  post_length  share_count  promotion  h_target  \\\n",
      "0  0.094676  -1.307648    -0.116933    -0.199822        0.0  0.121658   \n",
      "1  0.032692  -0.303034     0.031700    -0.223089        0.0  0.121658   \n",
      "2  0.012030  -1.546841     0.908636    -0.209794        0.0  0.121658   \n",
      "3 -0.494176  -0.207357    -0.027754     0.526467        0.0  0.121658   \n",
      "4  0.012030  -0.446551    -0.351031     0.001279        0.0  0.121658   \n",
      "\n",
      "   post_day  basetime_day  \n",
      "0 -0.041861     -1.041552  \n",
      "1 -1.015363      0.961914  \n",
      "2 -0.528612     -0.540685  \n",
      "3  0.931642      0.461048  \n",
      "4 -1.502114     -0.540685  \n",
      "\n",
      "[5 rows x 24 columns]\n",
      "      page_likes  page_checkin  daily_crowd  page_category        F1  \\\n",
      "0      -0.117628     -0.222818    -0.328944      -0.022518 -0.729693   \n",
      "1      -0.135873     -0.222818    -0.339482      -1.127069 -0.782934   \n",
      "2      -0.135748     -0.222818    -0.349413       0.178310 -0.845355   \n",
      "3       0.903758     -0.222769    -0.280799      -0.524586  0.349820   \n",
      "4      -0.081468     -0.222818     0.264963      -1.177276 -0.726021   \n",
      "5      -0.130880     -0.222818    -0.337969      -0.524586 -0.815980   \n",
      "6      -0.059347     -0.222818     0.428766      -0.775621 -0.305599   \n",
      "7      -0.135884     -0.123417    -0.352597       0.379137 -0.557118   \n",
      "8      -0.107757     -0.222818    -0.264404      -0.022518 -0.586492   \n",
      "9      -0.110744     -0.222179    -0.325656       0.077896 -0.211967   \n",
      "10     -0.137343     -0.222818    -0.366078      -0.825827 -0.821488   \n",
      "11     -0.115411     -0.222818    -0.362685      -0.825827 -0.804965   \n",
      "12     -0.121624     -0.212696    -0.316870      -0.323759  1.278789   \n",
      "13     -0.135873     -0.222818    -0.339482      -1.127069 -0.782934   \n",
      "14     -0.133346     -0.222818    -0.330165       2.136377 -0.424932   \n",
      "15      0.033736     -0.222818     0.256492      -0.775621  0.331461   \n",
      "16     -0.136427     -0.222818    -0.354522      -0.323759 -0.700318   \n",
      "17      0.038809     -0.222818    -0.358454      -1.127069 -0.536923   \n",
      "18     -0.135266     -0.216479    -0.361637       1.282861 -0.819652   \n",
      "19     -0.030679     -0.222818     0.045668      -0.825827  0.654580   \n",
      "20      0.155134     -0.083176    -0.298078       0.529758 -0.814145   \n",
      "21     -0.019590      1.444584     0.003598      -0.775621  0.191932   \n",
      "22     -0.081468     -0.222818     0.264963      -1.177276 -0.726021   \n",
      "23     -0.082173     -0.213826     0.898843      -0.323759  0.959341   \n",
      "24     -0.001198      0.307989     0.080031      -0.775621  1.822217   \n",
      "25     -0.087540     -0.222818    -0.162257      -0.022518 -0.230326   \n",
      "26     -0.136200     -0.222818    -0.349039      -0.022518 -0.821488   \n",
      "27     -0.137251     -0.222523    -0.366228      -0.273552 -0.889417   \n",
      "28      0.156970      1.143385    -0.278440       0.780792 -0.470830   \n",
      "29     -0.137315      0.157980    -0.364925       0.479551 -0.887581   \n",
      "...          ...           ...          ...            ...       ...   \n",
      "1970   -0.135884     -0.123417    -0.352597       0.379137 -0.557118   \n",
      "1971   -0.130291     -0.222818    -0.342605      -1.127069 -0.839847   \n",
      "1972    0.389835      2.386062     1.390034      -0.775621  1.521128   \n",
      "1973   -0.128899     -0.222818    -0.363966       0.579965 -0.123844   \n",
      "1974   -0.116703     -0.206947    -0.361090      -1.026655 -0.669108   \n",
      "1975   -0.136816     -0.222818    -0.366340      -0.976448 -0.878401   \n",
      "1976   -0.126337     -0.222818    -0.196621       0.680378 -0.160562   \n",
      "1977   -0.135990     -0.222818    -0.361113       0.178310 -0.703990   \n",
      "1978   -0.138075     -0.222818    -0.366183      -0.323759 -0.880237   \n",
      "1979   -0.135748     -0.222818    -0.349413       0.178310 -0.845355   \n",
      "1980   -0.133560     -0.222818    -0.333565       2.086170 -0.588328   \n",
      "1981   -0.136536     -0.222818    -0.365734      -1.026655 -0.659928   \n",
      "1982   -0.127908     -0.222818    -0.366176       1.082033 -0.810473   \n",
      "1983   -0.101620     -0.222818    -0.221779      -0.022518 -0.155054   \n",
      "1984   -0.126552     -0.222818     0.035010       2.086170 -0.193608   \n",
      "1985   -0.128041     -0.222818    -0.056119      -0.022518  1.379763   \n",
      "1986    0.156970      1.143385    -0.278440       0.780792 -0.470830   \n",
      "1987   -0.045733     -0.222818    -0.105934      -0.424173  1.078675   \n",
      "1988   -0.101620     -0.222818    -0.221779      -0.022518 -0.155054   \n",
      "1989    0.087326      8.934501     0.960073      -0.775621  1.018090   \n",
      "1990   -0.100935     -0.215546    -0.361802       0.730585 -0.261537   \n",
      "1991   -0.108172     -0.222818    -0.365382      -0.424173 -0.781098   \n",
      "1992   -0.101620     -0.222818    -0.221779      -0.022518 -0.155054   \n",
      "1993   -0.117202     -0.165231    -0.096452       0.780792 -0.044900   \n",
      "1994   -0.138029     -0.222818    -0.366003      -0.424173 -0.891253   \n",
      "1995   -0.129578     -0.217904    -0.332674      -0.825827 -0.368019   \n",
      "1996   -0.137284     -0.222818    -0.348387      -0.424173 -0.788442   \n",
      "1997   -0.109004     -0.222818    -0.281923      -1.026655 -0.665436   \n",
      "1998   -0.127920     -0.222818    -0.360761      -1.177276 -0.841683   \n",
      "1999   -0.135873     -0.222818    -0.339482      -1.127069 -0.782934   \n",
      "\n",
      "            F2        F3        F4        F5        F6   ...          c4  \\\n",
      "0    -0.496875 -0.355810 -0.678899 -0.049132 -0.678024   ...   -0.348809   \n",
      "1    -0.382480 -0.191960 -0.679965 -0.049132 -0.721982   ...   -0.320921   \n",
      "2    -0.586190 -0.451389 -0.765848 -0.049132 -0.792314   ...   -0.404584   \n",
      "3     0.666701  0.401995  0.803422 -0.049132  0.442905   ...    0.780643   \n",
      "4    -0.504619 -0.383119 -0.673684 -0.049132 -0.667034   ...   -0.320921   \n",
      "5    -0.545269 -0.410427 -0.737739 -0.049132 -0.763742   ...   -0.383669   \n",
      "6    -0.219369 -0.164652 -0.305782 -0.049132 -0.262621   ...   -0.223314   \n",
      "7    -0.503999 -0.492352 -0.426372 -0.049132 -0.478015   ...   -0.404584   \n",
      "8    -0.344029 -0.273885 -0.481321 -0.049132 -0.497796   ...   -0.355781   \n",
      "9    -0.540314 -0.424081 -0.556205 -0.049132 -0.310974   ...   -0.306977   \n",
      "10   -0.561452 -0.437735 -0.732004 -0.049132 -0.768138   ...   -0.376697   \n",
      "11   -0.531794 -0.465043 -0.665187 -0.049132 -0.776929   ...   -0.397612   \n",
      "12   -0.198968 -0.410427  0.873417 -0.049132  0.812153   ...   -0.376697   \n",
      "13   -0.382480 -0.191960 -0.679965 -0.049132 -0.721982   ...   -0.306977   \n",
      "14   -0.523048 -0.437735 -0.524248 -0.049132 -0.278006   ...   -0.404584   \n",
      "15    2.051743  2.689066  1.232200 -0.049132  0.438510   ...    1.310509   \n",
      "16   -0.443218 -0.328502 -0.618442 -0.049132 -0.634066   ...   -0.306977   \n",
      "17   -0.355585 -0.287539 -0.433327 -0.049132 -0.504390   ...   -0.174511   \n",
      "18   -0.598365 -0.465043 -0.757260 -0.049132 -0.774731   ...   -0.404584   \n",
      "19    1.949502  1.787892  1.389680  1.552997  0.291250   ...    0.048591   \n",
      "20   -0.585091 -0.465043 -0.741711 -0.049132 -0.746159   ...   -0.404584   \n",
      "21    0.023124 -0.055419  0.160488 -0.049132  0.414333   ...   -0.104792   \n",
      "22   -0.504619 -0.383119 -0.673684 -0.049132 -0.667034   ...   -0.404584   \n",
      "23    0.643068  0.074296  1.066766 -0.049132  0.440708   ...   -0.383669   \n",
      "24    0.245034  0.040160  0.807147 -0.049132  2.100122   ...    2.948912   \n",
      "25   -0.272543 -0.260231 -0.286942 -0.049132 -0.315370   ...   -0.272118   \n",
      "26   -0.537254 -0.424081 -0.711633 -0.049132 -0.759346   ...   -0.313949   \n",
      "27   -0.630757 -0.492352 -0.817168 -0.049132 -0.836272   ...   -0.404584   \n",
      "28   -0.526425 -0.451389 -0.561829 -0.049132 -0.436255   ...    1.157127   \n",
      "29   -0.629263 -0.492352 -0.814380 -0.049132 -0.831877   ...   -0.404584   \n",
      "...        ...       ...       ...       ...       ...   ...         ...   \n",
      "1970 -0.503999 -0.492352 -0.426372 -0.049132 -0.478015   ...   -0.313949   \n",
      "1971 -0.561611 -0.424081 -0.758473 -0.049132 -0.783523   ...   -0.369725   \n",
      "1972  0.866698  0.163048  1.929419 -0.049132  1.344045   ...   -0.153595   \n",
      "1973 -0.094117 -0.191960 -0.021449 -0.049132  0.001128   ...   -0.404584   \n",
      "1974 -0.585541 -0.465043 -0.715928 -0.049132 -0.614285   ...   -0.341837   \n",
      "1975 -0.623583 -0.492352 -0.805089 -0.049132 -0.825283   ...   -0.397612   \n",
      "1976 -0.294594 -0.260231 -0.261788 -0.049132 -0.489004   ...    2.112281   \n",
      "1977 -0.577395 -0.465043 -0.707150 -0.049132 -0.741763   ...   -0.376697   \n",
      "1978 -0.617067 -0.478698 -0.801741 -0.049132 -0.827481   ...   -0.404584   \n",
      "1979 -0.586190 -0.451389 -0.765848 -0.049132 -0.792314   ...   -0.390640   \n",
      "1980 -0.318452 -0.164652 -0.496370 -0.049132 -0.493400   ...   -0.404584   \n",
      "1981 -0.481006 -0.383119 -0.601825 -0.049132 -0.625274   ...   -0.300005   \n",
      "1982 -0.535911 -0.396773 -0.736821 -0.049132 -0.750554   ...   -0.362753   \n",
      "1983 -0.213060 -0.164652 -0.279784 -0.049132 -0.176902   ...   -0.083876   \n",
      "1984  0.305607  0.313243  0.121473 -0.049132 -0.060414   ...    0.139226   \n",
      "1985  0.267133  0.217664  0.791671 -0.049132  1.629772   ...   -0.320921   \n",
      "1986 -0.526425 -0.451389 -0.561829 -0.049132 -0.436255   ...   -0.334865   \n",
      "1987 -0.333049 -0.451389  0.581248 -0.049132  1.157223   ...   -0.348809   \n",
      "1988 -0.213060 -0.164652 -0.279784 -0.049132 -0.176902   ...   -0.334865   \n",
      "1989  1.913982  1.897125  1.541599 -0.049132  1.231952   ...    0.452963   \n",
      "1990 -0.412021 -0.301194 -0.506270 -0.049132 -0.339547   ...   -0.376697   \n",
      "1991 -0.535604 -0.424081 -0.697759 -0.049132 -0.735169   ...   -0.397612   \n",
      "1992 -0.213060 -0.164652 -0.279784 -0.049132 -0.176902   ...   -0.069932   \n",
      "1993 -0.340663 -0.369464 -0.362184 -0.049132 -0.227454   ...   -0.397612   \n",
      "1994 -0.631160 -0.492352 -0.818397 -0.049132 -0.836272   ...   -0.404584   \n",
      "1995 -0.457980 -0.383119 -0.429605 -0.049132 -0.587910   ...   -0.355781   \n",
      "1996 -0.604023 -0.478698 -0.755026 -0.049132 -0.719784   ...   -0.383669   \n",
      "1997 -0.504038 -0.396773 -0.631849 -0.049132 -0.682419   ...   -0.341837   \n",
      "1998 -0.615103 -0.492352 -0.778103 -0.049132 -0.783523   ...   -0.397612   \n",
      "1999 -0.382480 -0.191960 -0.679965 -0.049132 -0.721982   ...   -0.306977   \n",
      "\n",
      "            c5  base_time  post_length  share_count  promotion  h_target  \\\n",
      "0     0.094676  -1.307648    -0.116933    -0.199822        0.0  0.121658   \n",
      "1     0.032692  -0.303034     0.031700    -0.223089        0.0  0.121658   \n",
      "2     0.012030  -1.546841     0.908636    -0.209794        0.0  0.121658   \n",
      "3    -0.494176  -0.207357    -0.027754     0.526467        0.0  0.121658   \n",
      "4     0.012030  -0.446551    -0.351031     0.001279        0.0  0.121658   \n",
      "5     0.043022  -1.355486     0.195196    -0.219765        0.0  0.121658   \n",
      "6    -0.018962   0.845095     0.302955    -0.219765        0.0  0.121658   \n",
      "7     0.012030   0.797256     1.588633    -0.224751        0.0  0.121658   \n",
      "8    -0.029293   0.701579    -0.076059    -0.224751        0.0  0.121658   \n",
      "9     0.156660  -0.542228    -0.362178     0.024547        0.0  0.121658   \n",
      "10   -0.029293   0.605901    -0.455074    -0.224751        0.0  0.121658   \n",
      "11    0.022361  -1.211970     0.250934    -0.224751        0.0  0.121658   \n",
      "12    0.053353  -0.877099    -0.466222    -0.226413        0.0  0.121658   \n",
      "13    0.156660  -1.355486    -0.410484    -0.226413        0.0  0.121658   \n",
      "14    0.012030  -0.159518    -0.592560    -0.068525        0.0  0.121658   \n",
      "15    2.553391  -1.211970    -0.053764     1.174642        0.0  0.121658   \n",
      "16    0.001699   1.466998    -0.592560    -0.224751        0.0  0.121658   \n",
      "17    0.001699   1.658353     0.202628    -0.204808        0.0  0.121658   \n",
      "18    0.012030  -1.355486     0.217491    -0.226413        0.0  0.121658   \n",
      "19    0.683528  -1.451164    -0.440211    -0.153286        0.0  0.121658   \n",
      "20    0.012030  -0.016002    -0.258135    -0.204808        0.0  0.121658   \n",
      "21   -0.008631   1.323482     0.302955    -0.199822        0.0  0.121658   \n",
      "22    0.012030   0.031837     0.139459    -0.196498        0.0  0.121658   \n",
      "23    0.043022  -1.499003    -0.154092    -0.224751        0.0  0.121658   \n",
      "24   -0.018962   1.419160     0.704265     0.946950        0.0  0.121658   \n",
      "25   -0.173923   0.558063     0.165470    -0.226413        0.0  0.121658   \n",
      "26    0.146330  -0.972777     1.978795    -0.141652        0.0  0.121658   \n",
      "27    0.012030  -0.781422    -0.592560    -0.194836        0.0  0.121658   \n",
      "28   -0.049954   1.754031    -0.403053    -0.221427        0.0  0.121658   \n",
      "29    0.012030  -1.690357    -0.484801    -0.226413        0.0  0.121658   \n",
      "...        ...        ...          ...          ...        ...       ...   \n",
      "1970  0.146330  -1.642519    -0.243272    -0.226413        0.0  0.121658   \n",
      "1971 -0.029293   0.653740    -0.391905    -0.223089        0.0  0.121658   \n",
      "1972  0.383937  -0.590067     0.440441    -0.176554        0.0  0.121658   \n",
      "1973  0.012030  -1.690357    -0.165239    -0.226413        0.0  0.121658   \n",
      "1974  0.105007  -0.733583     2.718246    -0.201484        0.0  0.121658   \n",
      "1975  0.001699  -0.303034    -0.529391    -0.224751        0.0  0.121658   \n",
      "1976 -2.704953   0.318869    -0.116933    -0.085144        0.0  0.121658   \n",
      "1977 -0.008631  -0.446551     0.247218    -0.224751        0.0  0.121658   \n",
      "1978  0.012030   0.271030    -0.302725    -0.226413        0.0  0.121658   \n",
      "1979  0.032692  -1.164131    -0.250703    -0.223089        0.0  0.121658   \n",
      "1980  0.012030  -1.690357    -0.172671    -0.226413        0.0  0.121658   \n",
      "1981  0.166991  -1.307648    -0.592560    -0.216441        0.0  0.121658   \n",
      "1982 -0.049954   0.462385    -0.577697    -0.133342        0.0  0.121658   \n",
      "1983 -0.287561   0.366708    -0.035185     0.260549        0.0  0.121658   \n",
      "1984 -0.111939   0.892934    -0.202398    -0.224751        0.0  0.121658   \n",
      "1985  0.012030   1.610515    -0.455074    -0.198160        0.0  0.121658   \n",
      "1986  0.001699   1.658353    -0.120649     1.359123        0.0  0.121658   \n",
      "1987 -0.039624   0.462385    -0.592560     0.483255        0.0  0.121658   \n",
      "1988  0.115337  -1.594680    -0.131797    -0.033623        0.0  0.121658   \n",
      "1989 -0.824759  -0.207357     0.068858     0.172464        0.0  0.121658   \n",
      "1990  0.012030   1.562676    -0.306441    -0.204808        0.0  0.121658   \n",
      "1991  0.001699   0.366708    -0.592560    -0.199822        0.0  0.121658   \n",
      "1992 -0.349546   0.079675    -0.302725    -0.184864        0.0  0.121658   \n",
      "1993  0.022361   0.845095    -0.310157    -0.166582        0.0  0.121658   \n",
      "1994  0.012030  -0.446551     0.971805    -0.226413        0.0  0.121658   \n",
      "1995  0.043022  -0.350873    -0.116933    -0.224751        0.0  0.121658   \n",
      "1996  0.043022  -0.637906    -0.139228    -0.224751        0.0  0.121658   \n",
      "1997  0.105007  -1.116293     0.042847    -0.091792        0.0  0.121658   \n",
      "1998  0.001699   1.562676    -0.477369    -0.226413        0.0  0.121658   \n",
      "1999  0.012030  -0.207357     0.607654    -0.223089        0.0 -8.934772   \n",
      "\n",
      "      post_day  basetime_day  target  \n",
      "0    -0.041861     -1.041552       3  \n",
      "1    -1.015363      0.961914       2  \n",
      "2    -0.528612     -0.540685       1  \n",
      "3     0.931642      0.461048       3  \n",
      "4    -1.502114     -0.540685       1  \n",
      "5     0.931642      0.961914       1  \n",
      "6    -0.528612     -1.041552       1  \n",
      "7     0.444891     -0.540685       1  \n",
      "8     0.931642      0.461048       1  \n",
      "9    -0.528612     -0.039819       3  \n",
      "10    0.931642      0.461048       1  \n",
      "11   -1.015363      0.961914       1  \n",
      "12   -1.502114     -0.540685       1  \n",
      "13    1.418393      1.462780       3  \n",
      "14   -0.041861      0.961914       1  \n",
      "15    0.931642      1.462780       3  \n",
      "16    1.418393     -0.540685       1  \n",
      "17   -0.528612      0.961914       1  \n",
      "18   -0.041861     -1.041552       1  \n",
      "19    1.418393      1.462780       3  \n",
      "20    1.418393     -1.542418       1  \n",
      "21   -0.528612      0.961914       1  \n",
      "22   -1.502114     -0.039819       1  \n",
      "23    0.444891      0.461048       3  \n",
      "24   -0.041861      0.961914       1  \n",
      "25   -1.502114     -0.039819       2  \n",
      "26   -0.528612     -0.540685       1  \n",
      "27   -0.528612     -0.039819       1  \n",
      "28   -0.041861      1.462780       1  \n",
      "29   -1.502114     -1.542418       1  \n",
      "...        ...           ...     ...  \n",
      "1970 -1.015363     -1.041552       3  \n",
      "1971 -0.528612     -1.041552       1  \n",
      "1972 -1.502114     -0.540685       3  \n",
      "1973  0.931642      0.961914       3  \n",
      "1974  0.931642      1.462780       1  \n",
      "1975  0.444891     -1.542418       1  \n",
      "1976  1.418393     -1.542418       3  \n",
      "1977 -1.015363      0.961914       3  \n",
      "1978  0.931642      0.461048       1  \n",
      "1979 -0.041861     -1.041552       1  \n",
      "1980 -1.502114     -1.542418       3  \n",
      "1981  1.418393      1.462780       2  \n",
      "1982 -0.041861      0.961914       1  \n",
      "1983  0.444891     -0.540685       2  \n",
      "1984 -1.502114     -0.039819       1  \n",
      "1985 -1.502114     -1.041552       1  \n",
      "1986 -0.528612      0.961914       2  \n",
      "1987 -1.502114     -0.039819       1  \n",
      "1988  0.444891      0.461048       3  \n",
      "1989  0.444891     -0.540685       2  \n",
      "1990  0.444891     -0.039819       1  \n",
      "1991 -1.015363      1.462780       1  \n",
      "1992  1.418393     -1.542418       2  \n",
      "1993 -1.502114     -0.039819       1  \n",
      "1994  1.418393      0.461048       1  \n",
      "1995  0.444891     -1.542418       1  \n",
      "1996 -1.015363      0.961914       1  \n",
      "1997  0.931642      0.961914       3  \n",
      "1998 -0.528612      0.961914       1  \n",
      "1999  1.418393      0.461048       1  \n",
      "\n",
      "[2000 rows x 25 columns]\n"
     ]
    }
   ],
   "source": [
    "val_percent=0.1\n",
    "\n",
    "train_data=preprocessData(df)\n",
    "val_size=int(val_percent*len(train_data.index))\n",
    "val_data=train_data.tail(val_size)\n",
    "train_data=train_data.head(len(train_data.index)-val_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehotencoding(y):\n",
    "    y=y.astype(int)\n",
    "#     print(y.shape)\n",
    "    max_y=max(y)\n",
    "#     max_y=3\n",
    "    encode_mat=np.zeros((y.shape[0],max_y))\n",
    "    rows=np.arange(y.shape[0])\n",
    "    #array([0, 1, 2, 3, 4])\n",
    "    encode_mat[rows,y-1]=1\n",
    "    return encode_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class nn:\n",
    "    #nn for classifier works for only one layer\n",
    "    def __init__(self, no_of_inputs, no_of_outputs, HUs,no_of_hidden_layers=1, activation=[\"sigmoid\",\"sigmoid\"],regu=0, dropout=0, weights_seed=0):\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.no_of_inputs=no_of_inputs\n",
    "        self.no_of_hidden_layers=no_of_hidden_layers\n",
    "        self.no_of_outputs=no_of_outputs\n",
    "        self.HUs=HUs #hidden units array\n",
    "        self.act=activation\n",
    "        self.dropout=dropout\n",
    "        self.weights_seed=weights_seed\n",
    "        self.regu=regu\n",
    "        #sanity checks\n",
    "        if(len(self.HUs) != self.no_of_hidden_layers):\n",
    "            print(\"error mismatch hidden units and layers\")\n",
    "        if((self.no_of_hidden_layers+1)!=len(activation)):\n",
    "            print(\"error mismatch activations and layers\")\n",
    "        self.initialize_weights()\n",
    "    \n",
    "    def initialize_weights(self):\n",
    "        \n",
    "        np.random.seed(self.weights_seed)\n",
    "        self.weights=[]\n",
    "        weights0=0.01 * np.random.randn(self.no_of_inputs+1,self.HUs[0])\n",
    "        self.weights.append(weights0);\n",
    "        # to add multiple layers\n",
    "        for i in range(1,self.no_of_hidden_layers):\n",
    "            weightsh=0.01 * np.random.randn(self.HUs[i-1]+1,self.HUs[i])\n",
    "            self.weights.append(np.copy(weightsh));\n",
    "              \n",
    "            \n",
    "        weightsl=0.01 * np.random.randn(self.HUs[-1]+1,self.no_of_outputs)\n",
    "        self.weights.append(weightsl);\n",
    "    \n",
    "    def get_weights(self):\n",
    "        return self.weights\n",
    "    \n",
    "    def sigmoid_forward(self,z):\n",
    "#         print(\"sig forward\")\n",
    "        return 1/(1+np.exp(-z))\n",
    "        \n",
    "    def sigmoid_backward(self,z):\n",
    "#         print(\"sig bckward\")\n",
    "        return z*(1-z)\n",
    "    \n",
    "    def tanh_forward(self,z):\n",
    "#         print(\"tanh forwad\")\n",
    "        return np.tanh(z) \n",
    "    \n",
    "    def tanh_backward(self,z):\n",
    "#         print(\"tanh backward\")\n",
    "        return 1-z**2\n",
    "    \n",
    "    #not working relu\n",
    "    def relu_forward(self,z):\n",
    "#         print(\"relu forwad\")\n",
    "        return z * (z > 0) \n",
    "    \n",
    "    def relu_backward(self,z):\n",
    "#         print(\"relu backward\")\n",
    "        \n",
    "        return 1 * (z > 0)\n",
    "    \n",
    "    def softplus_forward(self,z):\n",
    "        return np.log(1+np.exp(z)) \n",
    "    \n",
    "    def softplus_backward(self,z): \n",
    "        return 1-1/np.exp(z)\n",
    "    \n",
    "    def activation_forward(self,z,act_index):\n",
    "        if(self.act[act_index]==\"sigmoid\"):\n",
    "            return self.sigmoid_forward(z)\n",
    "        elif (self.act[act_index]==\"softplus\"):\n",
    "            return self.softplus_forward(z)\n",
    "        elif (self.act[act_index]==\"tanh\"):\n",
    "            return self.tanh_forward(z)\n",
    "        elif (self.act[act_index]==\"relu\"):\n",
    "            return self.relu_forward(z)\n",
    "        else:\n",
    "            print(\"error invalid activation\")\n",
    "    \n",
    "    def activation_backward(self,z,act_index):\n",
    "        if(self.act[act_index]==\"sigmoid\"):\n",
    "            return self.sigmoid_backward(z)\n",
    "        elif (self.act[act_index]==\"softplus\"):\n",
    "            return self.softplus_backward(z)\n",
    "        elif (self.act[act_index]==\"tanh\"):\n",
    "            return self.tanh_backward(z)\n",
    "        elif (self.act[act_index]==\"relu\"):\n",
    "            return self.relu_backward(z)\n",
    "        else:\n",
    "            print(\"error invalid activation\")\n",
    "    \n",
    "    def feed_forward(self, input_arr,dropout=0):\n",
    "#         print(input_arr.shape)\n",
    "        p=1-dropout\n",
    "        \n",
    "        \n",
    "        self.inps = np.ones((input_arr.shape[0],self.no_of_inputs+1))\n",
    "        self.inps[:,1:self.no_of_inputs+1]=input_arr\n",
    "        self.layers=[]\n",
    "        self.layers.append(self.inps)\n",
    "        u1 = np.random.binomial(1, p, size=self.layers[-1].shape) / p\n",
    "        self.layers[-1] = self.layers[-1]*u1\n",
    "        layer0nodes=np.matmul(self.layers[-1],self.weights[0])\n",
    "        layer0nodes=self.activation_forward(layer0nodes,0)\n",
    "        layer0nodes_bias=np.ones((layer0nodes.shape[0],layer0nodes.shape[1]+1))\n",
    "        layer0nodes_bias[:,1:layer0nodes.shape[1]+1]=layer0nodes\n",
    "        #for all the internal layer nodes\n",
    "        self.layers.append(layer0nodes_bias)\n",
    "        u1 = np.random.binomial(1, p, size=self.layers[-1].shape) / p\n",
    "        self.layers[-1] = self.layers[-1]*u1\n",
    "        for i in range(1,self.no_of_hidden_layers):\n",
    "            layerinodes=np.matmul(self.layers[-1],self.weights[i])\n",
    "            layerinodes=self.activation_forward(layerinodes,i)\n",
    "            layerinodes_bias=np.ones((layerinodes.shape[0],layerinodes.shape[1]+1))\n",
    "            layerinodes_bias[:,1:layerinodes.shape[1]+1]=layerinodes\n",
    "            self.layers.append(np.copy(layerinodes_bias))\n",
    "            u1 = np.random.binomial(1, p, size=self.layers[-1].shape) / p\n",
    "            self.layers[-1] = self.layers[-1]*u1\n",
    "            \n",
    "        #for the last layer\n",
    "        layer1nodes=np.matmul(self.layers[-1],self.weights[-1])\n",
    "        layer1nodes=self.activation_forward(layer1nodes,-1)\n",
    "        self.layers.append(layer1nodes)\n",
    "#         print(len(self.layers))\n",
    "        return self.layers[-1]\n",
    "        \n",
    "    def get_layers(self):\n",
    "        return self.layers\n",
    "    \n",
    "    #can use with sigmoid\n",
    "    def compute_cross_entropy_err(self,y_hat,y):\n",
    "        #cross entropy, expects one hot encoded y label\n",
    "        loss=0\n",
    "        #ylog(y_hat)+(1-y)log(1-y_hat)\n",
    "        loss_matrix=np.multiply(y,np.log(y_hat))+np.multiply(1-y,np.log(1-y_hat))\n",
    "        loss_matrix=np.nan_to_num(loss_matrix)\n",
    "        #computing sum of squares of weights\n",
    "        sum_square=0\n",
    "        for i in range(0,len(self.weights)):\n",
    "            sum_square=sum_square+np.sum((np.array(self.weights[i]))**2)\n",
    "        \n",
    "        loss=-1*np.sum(loss_matrix)*(1/y.shape[0])+self.regu*(1/y.shape[0])*sum_square\n",
    "        return loss\n",
    "    \n",
    "    def compute_gradients(self,y_hat,y):\n",
    "        #dE/d(sigma)\n",
    "        self.gradients=[]\n",
    "        dE_dsigmaL=(-1)*(np.multiply(y,1/y_hat)-np.multiply(1-y,1/(1-y_hat)))\n",
    "#         dE_dsigmaL=np.multiply(dE_dsigmaL_init,y)-np.multiply(dE_dsigmaL_init,1-y)\n",
    "        dE_dsigmaL=np.nan_to_num(dE_dsigmaL)\n",
    "        dE_dsigmaL_dsumL=np.multiply(dE_dsigmaL,self.activation_backward(self.layers[-1],-1))\n",
    "        dE_dsigmaL_dsumL=np.nan_to_num(dE_dsigmaL_dsumL)\n",
    "        #average\n",
    "        #gradient for the last layer\n",
    "        #update weight with this gradient\n",
    "        dE_dsigmaL_dsumL_dw=np.matmul(self.layers[-2].T,dE_dsigmaL_dsumL)*(1/self.layers[-2].shape[0])\n",
    "        self.gradients.insert(0,dE_dsigmaL_dsumL_dw)\n",
    "        \n",
    "        for i in range(self.no_of_hidden_layers,0,-1):\n",
    "            #weights without bias\n",
    "            weights_wo_bias=self.weights[i][1:,:]\n",
    "            dE_dsigmaL_dsumL_dsigmal=np.matmul(dE_dsigmaL_dsumL,weights_wo_bias.T)\n",
    "            layer_wo_bias=self.layers[i][:,1:]\n",
    "            dE_dsigmaL_dsumL=np.multiply(dE_dsigmaL_dsumL_dsigmal,self.activation_backward(layer_wo_bias,i-1))\n",
    "            dE_dsigmaL_dsumL=np.nan_to_num(dE_dsigmaL_dsumL)\n",
    "            dE_dsigmal_dsuml_dw=np.matmul(self.layers[i-1].T,dE_dsigmaL_dsumL)*(1/self.layers[i-1].shape[0])\n",
    "            self.gradients.insert(0,np.copy(dE_dsigmal_dsuml_dw))\n",
    "        \n",
    "        return self.gradients\n",
    "    \n",
    "    def update_weights(self, gradients,learning_rate=0.001):\n",
    "        self.lr=learning_rate\n",
    "        for i in range(0,len(self.weights)):\n",
    "            self.weights[i]=np.copy(self.weights[i])-gradients[i]*learning_rate-self.regu*learning_rate*np.copy(self.weights[i])\n",
    "    \n",
    "    #dont use predict for now\n",
    "    def predict(self,input_arr):\n",
    "        inps = np.ones((input_arr.shape[0],self.no_of_inputs+1))\n",
    "        inps[:,1:self.no_of_inputs+1]=input_arr\n",
    "        self.layers=[]\n",
    "#         self.layers.append(self.inps)\n",
    "        layer0nodes=np.matmul(self.inps,self.weights[0])\n",
    "        layer0nodes=self.sigmoid_forward(layer0nodes)\n",
    "        layer0nodes_bias=np.ones((layer0nodes.shape[0],layer0nodes.shape[1]+1))\n",
    "        layer0nodes_bias[:,1:layer0nodes.shape[1]+1]=layer0nodes\n",
    "        #output layer nodes\n",
    "#         self.layers.append(layer0nodes_bias)\n",
    "        layer1nodes=np.matmul(layer0nodes_bias,self.weights[1])\n",
    "        layer1nodes=self.sigmoid_forward(layer1nodes)\n",
    "#         self.layers.append(layer1nodes)\n",
    "#         print(len(self.layers))\n",
    "        return layer1nodes\n",
    "        \n",
    "        \n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape\n",
    "\n",
    "network1=nn(no_of_inputs=24,no_of_outputs=3,HUs=[100],regu=0,no_of_hidden_layers=1, activation=[\"tanh\",\"sigmoid\"])\n",
    "train_data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data=train_data.values\n",
    "input_x=input_data[:,:-1]\n",
    "y_label=input_data[:,-1]\n",
    "y_encod=onehotencoding(y_label)\n",
    "val_input=val_data.values\n",
    "val_x=val_input[:,:-1]\n",
    "y_val_label=val_input[:,-1]\n",
    "y_val_encod=onehotencoding(y_val_label)\n",
    "# y_encod=y_encod[[0],:]\n",
    "# input_x=input_x[[0],:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_encod;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(input_x,y_encod,no_of_epochs=10000,batchSize=100):\n",
    "    print(input_x.shape)\n",
    "    print(y_encod.shape)\n",
    "    no_of_batches=int(input_x.shape[0]/batchSize)\n",
    "    lr=0.01\n",
    "    for i in range(0,no_of_epochs):\n",
    "        loss=0\n",
    "        for batch_no in range(0,no_of_batches):\n",
    "            idx = np.random.randint(input_x.shape[0], size=batchSize)\n",
    "            input_x_train=input_x[idx,:]\n",
    "            y_encod_train=y_encod[idx,:]\n",
    "            y_pred=network1.feed_forward(input_x_train,dropout=0.5)\n",
    "\n",
    "            loss=loss+network1.compute_cross_entropy_err(y_pred,y_encod_train)\n",
    "        #         print(y_pred)\n",
    "            grads=network1.compute_gradients(y_pred,y_encod_train)\n",
    "        #     if(i%1000==0):\n",
    "        #         print(grads)\n",
    "        #         print(network1.get_weights())\n",
    "            network1.update_weights(grads,learning_rate= lr)\n",
    "        if i%50==0:\n",
    "            loss=loss*1/no_of_batches\n",
    "            print(\"train loss: \"+ str(loss))\n",
    "            \n",
    "            #checking val loss\n",
    "            y_pred_val=network1.feed_forward(val_x)\n",
    "            val_loss=network1.compute_cross_entropy_err(y_pred_val,y_val_encod)\n",
    "            print(\"val loss: \"+ str(val_loss))\n",
    "        if (i+1)%1000==0:\n",
    "            lr=lr*0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1800, 24)\n",
      "(1800, 3)\n",
      "train loss: 2.0734420904719637\n",
      "val loss: 2.0584255618773457\n",
      "train loss: 1.5140111728670278\n",
      "val loss: 1.5020817557835613\n",
      "train loss: 1.4574987279066134\n",
      "val loss: 1.4227334059935641\n",
      "train loss: 1.442188650206538\n",
      "val loss: 1.391892825946885\n",
      "train loss: 1.4812457613432395\n",
      "val loss: 1.3728256228936357\n",
      "train loss: 1.4586795157052903\n",
      "val loss: 1.3710421102244157\n",
      "train loss: 1.4063561190681915\n",
      "val loss: 1.367036237735373\n",
      "train loss: 1.4291534373106092\n",
      "val loss: 1.3663684927960775\n",
      "train loss: 1.472606354026049\n",
      "val loss: 1.3582201492010058\n",
      "train loss: 1.355411725776159\n",
      "val loss: 1.3586346799326157\n",
      "train loss: 1.3957341356687898\n",
      "val loss: 1.3588414270866396\n",
      "train loss: 1.450246203555632\n",
      "val loss: 1.3487249068692269\n",
      "train loss: 1.427918544594484\n",
      "val loss: 1.349156008388359\n",
      "train loss: 1.3602592059450962\n",
      "val loss: 1.3471608928070407\n",
      "train loss: 1.4201154180508193\n",
      "val loss: 1.349424808533767\n",
      "train loss: 1.4199296419910628\n",
      "val loss: 1.3462665383929369\n",
      "train loss: 1.3697837371300245\n",
      "val loss: 1.3520088814841882\n",
      "train loss: 1.4401988368155954\n",
      "val loss: 1.336507874207729\n",
      "train loss: 1.4374473477951615\n",
      "val loss: 1.341807229758399\n",
      "train loss: 1.3426483743357274\n",
      "val loss: 1.3520007325296877\n",
      "train loss: 1.3806353812505243\n",
      "val loss: 1.3404990420213279\n",
      "train loss: 1.4056740577518163\n",
      "val loss: 1.3360064929066704\n",
      "train loss: 1.3773014350842792\n",
      "val loss: 1.3389235455762645\n",
      "train loss: 1.3065243234735089\n",
      "val loss: 1.34013478926103\n",
      "train loss: 1.3973888470161828\n",
      "val loss: 1.3422735132310692\n",
      "train loss: 1.4059310046553068\n",
      "val loss: 1.3391357445670826\n",
      "train loss: 1.3890108176964764\n",
      "val loss: 1.3392264343850957\n",
      "train loss: 1.3798541143587144\n",
      "val loss: 1.3382890176360258\n",
      "train loss: 1.443689172212488\n",
      "val loss: 1.3388922288108995\n",
      "train loss: 1.3953855951208534\n",
      "val loss: 1.3364066368316418\n",
      "train loss: 1.409784461541323\n",
      "val loss: 1.3407557982419502\n",
      "train loss: 1.381059927946421\n",
      "val loss: 1.3406126474877404\n",
      "train loss: 1.412517779289812\n",
      "val loss: 1.3406350871066328\n",
      "train loss: 1.4044000025809213\n",
      "val loss: 1.3429471878411368\n",
      "train loss: 1.4572484463447772\n",
      "val loss: 1.3361013625718199\n",
      "train loss: 1.4263750870017835\n",
      "val loss: 1.338333509481188\n",
      "train loss: 1.3686243340098867\n",
      "val loss: 1.3414363648061487\n",
      "train loss: 1.4094740521527676\n",
      "val loss: 1.3433913495822976\n",
      "train loss: 1.360437854616757\n",
      "val loss: 1.3366280730009805\n",
      "train loss: 1.3867582454354541\n",
      "val loss: 1.333471388657713\n",
      "train loss: 1.4068977132954243\n",
      "val loss: 1.3331292235115404\n",
      "train loss: 1.3923092727076807\n",
      "val loss: 1.334608427188768\n",
      "train loss: 1.4270952736027607\n",
      "val loss: 1.3355125305742537\n",
      "train loss: 1.4204521411229234\n",
      "val loss: 1.3344707596068124\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-3c496ba7fc14>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_encod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mno_of_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-19-413af731f88d>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(input_x, y_encod, no_of_epochs, batchSize)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch_no\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mno_of_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatchSize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0minput_x_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m             \u001b[0my_encod_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_encod\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0my_pred\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnetwork1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_x_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(input_x,y_encod,no_of_epochs=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights=network1.get_weights()\n",
    "np.asarray(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28800, 3)\n",
      "0.7351388888888889\n"
     ]
    }
   ],
   "source": [
    "def trainAccuracy(input_x,y_encod):\n",
    "    y_pred=network1.feed_forward(input_x)\n",
    "    y_pred_encod=(y_pred == y_pred.max(axis=1)[:,None]).astype(int)\n",
    "    print(y_pred_encod.shape)\n",
    "#     print(np.argmax(y_pred_encod,axis=1).shape)\n",
    "    correct_pred_encod=np.multiply(y_pred_encod,y_encod)\n",
    "    accuracy=np.sum(correct_pred_encod)/input_x.shape[0]\n",
    "    print(accuracy)\n",
    "    \n",
    "trainAccuracy(input_x,y_encod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessData_test(df):\n",
    "    df['post_day'] = df['post_day'].factorize(sort=True)[0]\n",
    "    df['basetime_day'] = df['basetime_day'].factorize(sort=True)[0]\n",
    "#     df=df.head(5)\n",
    "    df_norm=(df-train_mean)/train_std\n",
    "    df_norm.fillna(0,inplace=True)\n",
    "    return df_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('./../data/test.csv')\n",
    "test_data=preprocessData_test(df_test)\n",
    "input_data=test_data.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:42: RuntimeWarning: overflow encountered in exp\n"
     ]
    }
   ],
   "source": [
    "def testOutput(input_x_test):\n",
    "    y_pred=network1.feed_forward(input_x_test)\n",
    "    y_pred_arr=np.argmax(y_pred,axis=1)\n",
    "    y_pred_arr=y_pred_arr+1\n",
    "    file=open(\"submission.csv\",\"w\")\n",
    "    file.write(\"Id,predicted_class\\n\")\n",
    "    print(len(y_pred_arr))\n",
    "    for i in range(0,len(y_pred_arr)):\n",
    "        file.write(str(i+1)+\",\"+str(y_pred_arr[i])+\"\\n\")\n",
    "    file.close()\n",
    "\n",
    "testOutput(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 2)\n",
      "(5, 2)\n",
      "[[0.49765536 0.50368654 0.49381384]\n",
      " [0.49764957 0.50367189 0.49383668]\n",
      " [0.49764852 0.50367562 0.4938206 ]\n",
      " [0.49765452 0.5036914  0.49379497]\n",
      " [0.49765335 0.50368932 0.4937968 ]]\n",
      "[array([[ 0.01624345, -0.00611756, -0.00528172],\n",
      "       [-0.01072969,  0.00865408, -0.02301539],\n",
      "       [ 0.01744812, -0.00761207,  0.00319039]]), array([[-0.0024937 ,  0.01462108, -0.02060141],\n",
      "       [-0.00322417, -0.00384054,  0.01133769],\n",
      "       [-0.01099891, -0.00172428, -0.00877858],\n",
      "       [ 0.00042214,  0.00582815, -0.01100619]])]\n",
      "[array([[ 1.        , -0.60865379,  0.23565675],\n",
      "       [ 1.        ,  1.18539118,  1.39296744],\n",
      "       [ 1.        ,  0.98290729,  0.32057733],\n",
      "       [ 1.        , -0.92479789, -1.00327539],\n",
      "       [ 1.        , -0.63484679, -0.94592614]]), array([[1.        , 0.50672107, 0.49670536, 0.50236961],\n",
      "       [1.        , 0.50695686, 0.49838439, 0.4929705 ],\n",
      "       [1.        , 0.50282263, 0.49998708, 0.49328017],\n",
      "       [1.        , 0.50216523, 0.49837905, 0.50320046],\n",
      "       [1.        , 0.50163763, 0.49889722, 0.50157791]]), array([[0.49765536, 0.50368654, 0.49381384],\n",
      "       [0.49764957, 0.50367189, 0.49383668],\n",
      "       [0.49764852, 0.50367562, 0.4938206 ],\n",
      "       [0.49765452, 0.5036914 , 0.49379497],\n",
      "       [0.49765335, 0.50368932, 0.4937968 ]])]\n"
     ]
    }
   ],
   "source": [
    "print(input_x.shape)\n",
    "input_x_part=input_x\n",
    "print(input_x_part.shape)\n",
    "y_pred=network1.feed_forward(input_x_part)\n",
    "print(y_pred)\n",
    "weights=network1.get_weights()\n",
    "print(weights)\n",
    "layers=network1.get_layers()\n",
    "print(layers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.99066523 -1.98536177 -1.97555777]\n",
      " [-1.99064225 -2.01479624 -2.02496096]\n",
      " [-1.99063813 -2.01481138 -2.02502692]\n",
      " [-2.00942613 -2.01487543 -1.97548412]\n",
      " [-1.99065726 -2.01486698 -2.0251245 ]]\n"
     ]
    }
   ],
   "source": [
    "dE_dsigmaL=(-1)*(np.multiply(y_encod,1/y_pred)+np.multiply(1-y_encod,1/(1-y_pred)))\n",
    "print(dE_dsigmaL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.         -1.98536177 -0.        ]\n",
      " [-0.         -0.         -2.02496096]\n",
      " [-0.         -0.         -2.02502692]\n",
      " [-2.00942613 -0.         -0.        ]\n",
      " [-0.         -0.         -2.0251245 ]]\n",
      "[[-0.         -0.49631346 -0.        ]\n",
      " [-0.         -0.         -0.50616332]\n",
      " [-0.         -0.         -0.5061794 ]\n",
      " [-0.50234548 -0.         -0.        ]\n",
      " [-0.         -0.         -0.5062032 ]]\n",
      "[[0.2499945  0.24998641 0.24996173]\n",
      " [0.24999448 0.24998652 0.24996201]\n",
      " [0.24999447 0.24998649 0.24996181]\n",
      " [0.2499945  0.24998637 0.2499615 ]\n",
      " [0.24999449 0.24998639 0.24996152]]\n",
      "(4, 3)\n"
     ]
    }
   ],
   "source": [
    "dE_dsigmaL=np.multiply(dE_dsigmaL,y_encod)\n",
    "print(dE_dsigmaL)\n",
    "dE_dsigmaL_dsumL=np.multiply(dE_dsigmaL,network1.sigmoid_backward(layers[-1]))\n",
    "print(dE_dsigmaL_dsumL)\n",
    "print(network1.sigmoid_backward(layers[-1]))\n",
    "print(weights[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.50672107 0.49670536 0.50236961]\n",
      " [1.         0.50695686 0.49838439 0.4929705 ]\n",
      " [1.         0.50282263 0.49998708 0.49328017]\n",
      " [1.         0.50216523 0.49837905 0.50320046]\n",
      " [1.         0.50163763 0.49889722 0.50157791]]\n",
      "[[1.         1.         1.         1.         1.        ]\n",
      " [0.50672107 0.50695686 0.50282263 0.50216523 0.50163763]\n",
      " [0.49670536 0.49838439 0.49998708 0.49837905 0.49889722]\n",
      " [0.50236961 0.4929705  0.49328017 0.50320046 0.50157791]]\n",
      "[[-0.         -0.49631346 -0.        ]\n",
      " [-0.         -0.         -0.50616332]\n",
      " [-0.         -0.         -0.5061794 ]\n",
      " [-0.50234548 -0.         -0.        ]\n",
      " [-0.         -0.         -0.5062032 ]]\n",
      "[[-0.1004691  -0.09926269 -0.30370918]\n",
      " [-0.05045209 -0.0502985  -0.1530104 ]\n",
      " [-0.05007169 -0.04930431 -0.15157809]\n",
      " [-0.0505561  -0.04986656 -0.15062244]]\n",
      "(5, 3)\n"
     ]
    }
   ],
   "source": [
    "print(layers[-2])\n",
    "print(layers[-2].T)\n",
    "print(dE_dsigmaL_dsumL)\n",
    "dE_dsigmaL_dsumL_dw=np.matmul(layers[-2].T,dE_dsigmaL_dsumL)*(1/layers[-2].shape[0])\n",
    "print(dE_dsigmaL_dsumL_dw)\n",
    "#update weight with this gradient\n",
    "print(dE_dsigmaL_dsumL.shape)\n",
    "#weights without bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.         -0.49631346 -0.        ]\n",
      " [-0.         -0.         -0.49383668]\n",
      " [-0.         -0.         -0.4938206 ]\n",
      " [-0.49765452 -0.         -0.        ]\n",
      " [-0.         -0.         -0.4937968 ]]\n",
      "[[-0.00322417 -0.01099891  0.00042214]\n",
      " [-0.00384054 -0.00172428  0.00582815]\n",
      " [ 0.01133769 -0.00877858 -0.01100619]]\n",
      "[[ 0.00190611  0.00085578 -0.00289259]\n",
      " [-0.00559897  0.00433519  0.00543526]\n",
      " [-0.00559879  0.00433505  0.00543508]\n",
      " [ 0.00160452  0.00547366 -0.00021008]\n",
      " [-0.00559852  0.00433484  0.00543482]]\n",
      "(5, 3)\n",
      "(5, 4)\n"
     ]
    }
   ],
   "source": [
    "print(dE_dsigmaL_dsumL)\n",
    "weights_wo_bias=weights[1][1:,:]\n",
    "print(weights_wo_bias.T)\n",
    "dE_dsigmaL_dsumL_dsigmal=np.matmul(dE_dsigmaL_dsumL,weights_wo_bias.T)\n",
    "print(dE_dsigmaL_dsumL_dsigmal)\n",
    "print(dE_dsigmaL_dsumL_dsigmal.shape)\n",
    "print(network1.sigmoid_backward(layers[-2]).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2.01468662 -2.0021407  -1.99843484]\n",
      " [-2.0147238  -1.99790979 -2.00170064]]\n",
      "[[-0.50364489 -0.5005346  -0.4996084 ]\n",
      " [-0.50365405 -0.4994769  -0.5004248 ]]\n",
      "[[1.         0.50537419 0.49874734]\n",
      " [1.         0.50550913 0.48727871]]\n",
      "(3, 3)\n",
      "[[-0.50364947 -0.50000575 -0.5000166 ]\n",
      " [-0.25456542 -0.2527237  -0.25272925]\n",
      " [-0.24830572 -0.24651238 -0.24651236]]\n",
      "(2, 3)\n",
      "[[ 0.00423022 -0.00211824]\n",
      " [ 0.00426253 -0.00213153]]\n",
      "(2, 2)\n",
      "(2, 3)\n",
      "[[1.         0.50537419 0.49874734]\n",
      " [1.         0.50550913 0.48727871]]\n",
      "[[ 0.00105743 -0.00052956]\n",
      " [ 0.0010655  -0.00053254]]\n",
      "(2, 3)\n",
      "(2, 2)\n",
      "[[ 0.00106147 -0.00053105]\n",
      " [ 0.00030971 -0.00015447]\n",
      " [ 0.0008667  -0.0004333 ]]\n",
      "[[ 0.01624345 -0.00611756]\n",
      " [-0.00528172 -0.01072969]\n",
      " [ 0.00865408 -0.02301539]]\n"
     ]
    }
   ],
   "source": [
    "dE_dsigmaL=(-1)*(np.multiply(y_encod,1/y_pred)+np.multiply(1-y_encod,1/(1-y_pred)))\n",
    "print(dE_dsigmaL)\n",
    "\n",
    "dE_dsigmaL_dsumL=np.multiply(dE_dsigmaL,network1.sigmoid_backward(layers[-1]))\n",
    "print(dE_dsigmaL_dsumL)\n",
    "print(layers[1])\n",
    "print(weights[1].shape)\n",
    "#average\n",
    "dE_dsigmaL_dsumL_dw=np.matmul(layers[-2].T,dE_dsigmaL_dsumL)*(1/layers[-2].shape[0])\n",
    "print(dE_dsigmaL_dsumL_dw)\n",
    "#update weight with this gradient\n",
    "print(dE_dsigmaL_dsumL.shape)\n",
    "#weights without bias\n",
    "weights_wo_bias=weights[1][1:,:]\n",
    "dE_dsigmaL_dsumL_dsigmal=np.matmul(dE_dsigmaL_dsumL,weights_wo_bias.T)\n",
    "print(dE_dsigmaL_dsumL_dsigmal)\n",
    "print(dE_dsigmaL_dsumL_dsigmal.shape)\n",
    "print(network1.sigmoid_backward(layers[-2]).shape)\n",
    "#layer without bias\n",
    "print(layers[-2])\n",
    "layer_wo_bias=layers[-2][:,1:]\n",
    "dE_dsigmal_dsuml=np.multiply(dE_dsigmaL_dsumL_dsigmal,network1.sigmoid_backward(layer_wo_bias))\n",
    "print(dE_dsigmal_dsuml)\n",
    "print(layers[0].shape)\n",
    "print(dE_dsigmal_dsuml.shape)\n",
    "dE_dsigmal_dsuml_dw=np.matmul(layers[0].T,dE_dsigmal_dsuml)*(1/layers[0].shape[0])\n",
    "print(dE_dsigmal_dsuml_dw)\n",
    "print(weights[0])\n",
    "# dE_dsigmaL_dsumL_dw_mean=np.mean(dE_dsigmaL_dsumL_dw,axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.73105858 0.73105858 0.73105858 0.73105858]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "range(0, 10)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "range(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 3., 3., 1., 3.])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=np.array([])\n",
    "a=np.append(a,2)\n",
    "a=np.append(a,3)\n",
    "a.shape[0]\n",
    "a[-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "9\n",
      "8\n",
      "7\n",
      "6\n",
      "5\n",
      "4\n",
      "3\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "for i in range(10,1,-1):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def a(b=[2,3]):\n",
    "    c=b[0]\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'builtin_function_or_method' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-7daed7379c46>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'builtin_function_or_method' object is not subscriptable"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   page_likes  page_checkin  daily_crowd  page_category   F1          F2  \\\n",
      "0      400487             0        57833             13  414   26.635492   \n",
      "1     2667410           141       111855             18  793  185.317073   \n",
      "2     2411555             0        61797              9  560  103.860465   \n",
      "3        1013             0            1             85   16    2.224299   \n",
      "4      367390             0         2678            100  110   11.828175   \n",
      "\n",
      "      F3          F4  F5   F6   ...     c4  c5  base_time  post_length  \\\n",
      "0   15.0   44.941598   0  300   ...     43  43         24          100   \n",
      "1  157.0  164.739179   0  419   ...    216 -29         68          118   \n",
      "2   78.0  107.011971   0  325   ...      0   0          0          119   \n",
      "3    1.0    3.489124   0    7   ...      1   1          1            0   \n",
      "4    6.0   15.430200   0   96   ...      9   9         19           53   \n",
      "\n",
      "   share_count  promotion  h_target   post_day  basetime_day  target  \n",
      "0           20          0        24  wednesday      thursday       2  \n",
      "1           71          0        24   saturday       tuesday       3  \n",
      "2          207          0        24     friday        friday       3  \n",
      "3            1          0        24    tuesday       tuesday       1  \n",
      "4            4          0        24     friday      saturday       3  \n",
      "\n",
      "[5 rows x 25 columns]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "32000"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('./../data/train.csv')\n",
    "train_mean=0\n",
    "train_std=0\n",
    "print(df.head())\n",
    "len(df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessData(df):\n",
    "    df['post_day'] = df['post_day'].factorize(sort=True)[0]\n",
    "    df['basetime_day'] = df['basetime_day'].factorize(sort=True)[0]\n",
    "    df = df.sample(frac=1).reset_index(drop=True)\n",
    "#     df=df.head(100)\n",
    "#     print(df)\n",
    "    df_norm=df.drop(labels='target', axis=1)\n",
    "    print(df_norm.mean())\n",
    "    print(df_norm.std())\n",
    "    df_norm=(df_norm-df_norm.mean())/df_norm.std()\n",
    "    global train_mean\n",
    "    global train_std\n",
    "    train_mean=df_norm.mean()\n",
    "    train_std=df_norm.std()\n",
    "#     df_norm=(df_norm-df_norm.min())/(df_norm.max()-df_norm.min())\n",
    "    df_norm.fillna(0,inplace=True)\n",
    "    print(df_norm.head(5))\n",
    "    df_norm=pd.concat([df_norm, df['target']], axis=1)\n",
    "#     print(df_norm)\n",
    "#     df_norm=df_norm[['page_likes','daily_crowd','target']]\n",
    "    print(df_norm)\n",
    "    return df_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_likes       1.343977e+06\n",
      "page_checkin     4.624117e+03\n",
      "daily_crowd      4.449958e+04\n",
      "page_category    2.442909e+01\n",
      "F1               4.762900e+02\n",
      "F2               5.528131e+01\n",
      "F3               3.506795e+01\n",
      "F4               6.726324e+01\n",
      "F5               8.284375e-02\n",
      "F6               3.731562e+02\n",
      "F7               2.122784e+01\n",
      "F8               7.013547e+00\n",
      "c1               5.541925e+01\n",
      "c2               2.125191e+01\n",
      "c3               2.030247e+01\n",
      "c4               5.224150e+01\n",
      "c5              -3.827500e-01\n",
      "base_time        3.560847e+01\n",
      "post_length      1.639106e+02\n",
      "share_count      1.179058e+02\n",
      "promotion        1.250000e-04\n",
      "h_target         2.377538e+01\n",
      "post_day         3.055375e+00\n",
      "basetime_day     3.016969e+00\n",
      "dtype: float64\n",
      "page_likes       8.226959e+06\n",
      "page_checkin     2.035713e+04\n",
      "daily_crowd      1.132647e+05\n",
      "page_category    2.003522e+01\n",
      "F1               5.318801e+02\n",
      "F2               8.608946e+01\n",
      "F3               6.864825e+01\n",
      "F4               8.215990e+01\n",
      "F5               2.309889e+00\n",
      "F6               4.430340e+02\n",
      "F7               5.676682e+01\n",
      "F8               2.052160e+01\n",
      "c1               1.367967e+02\n",
      "c2               7.314642e+01\n",
      "c3               7.551940e+01\n",
      "c4               1.277610e+02\n",
      "c5               2.500114e+02\n",
      "base_time        2.092035e+01\n",
      "post_length      3.635404e+02\n",
      "share_count      7.969809e+02\n",
      "promotion        1.117982e-02\n",
      "h_target         1.844610e+00\n",
      "post_day         2.051783e+00\n",
      "basetime_day     2.003215e+00\n",
      "dtype: float64\n",
      "   page_likes  page_checkin  daily_crowd  page_category        F1        F2  \\\n",
      "0   -0.123635      -0.22715     0.422501      -0.320890  0.281473  0.361314   \n",
      "1   -0.132635      -0.22715    -0.392828      -0.121241 -0.880443 -0.638125   \n",
      "2   -0.145234      -0.22715    -0.391601      -0.520538 -0.677389 -0.614223   \n",
      "3   -0.016988      -0.22715    -0.336217      -1.169395 -0.698071 -0.574555   \n",
      "4   -0.092896      -0.22715     0.417504      -0.021417 -0.041908  0.805920   \n",
      "\n",
      "         F3        F4        F5        F6      ...             c3        c4  \\\n",
      "0  0.290350  0.423033 -0.035865  0.130563      ...      -0.136421 -0.299321   \n",
      "1 -0.510835 -0.805110 -0.035865 -0.828732      ...      -0.268838 -0.408900   \n",
      "2 -0.496268 -0.704525 -0.035865 -0.645901      ...      -0.202630 -0.361937   \n",
      "3 -0.467134 -0.680507 -0.035865 -0.623330      ...      -0.255596 -0.401073   \n",
      "4  1.062402  0.343487 -0.035865 -0.061296      ...      -0.268838  0.060727   \n",
      "\n",
      "         c5  base_time  post_length  share_count  promotion  h_target  \\\n",
      "0 -0.006469   0.209917    -0.423366     0.076657  -0.011181  0.121774   \n",
      "1  0.001531  -0.698290    -0.186804    -0.146686  -0.011181  0.121774   \n",
      "2 -0.014468  -0.124686     0.008498    -0.145431  -0.011181  0.121774   \n",
      "3  0.001531   0.592320    -0.445372    -0.001137  -0.011181  0.121774   \n",
      "4  0.241520  -1.319694    -0.398609     0.054072  -0.011181  0.121774   \n",
      "\n",
      "   post_day  basetime_day  \n",
      "0 -0.514370     -1.006866  \n",
      "1 -0.026989     -1.006866  \n",
      "2 -0.514370     -0.008471  \n",
      "3 -1.001750      1.489122  \n",
      "4  1.435154      0.490727  \n",
      "\n",
      "[5 rows x 24 columns]\n",
      "       page_likes  page_checkin  daily_crowd  page_category        F1  \\\n",
      "0       -0.123635     -0.227150     0.422501      -0.320890  0.281473   \n",
      "1       -0.132635     -0.227150    -0.392828      -0.121241 -0.880443   \n",
      "2       -0.145234     -0.227150    -0.391601      -0.520538 -0.677389   \n",
      "3       -0.016988     -0.227150    -0.336217      -1.169395 -0.698071   \n",
      "4       -0.092896     -0.227150     0.417504      -0.021417 -0.041908   \n",
      "5       -0.036328     -0.227150    -0.085919      -0.420714  1.123768   \n",
      "6       -0.156326     -0.227150    -0.297212      -0.320890  0.572892   \n",
      "7       -0.147619     -0.227150     0.111239      -0.021417 -0.380330   \n",
      "8       -0.162386     -0.197283    -0.392272       0.377880 -0.801478   \n",
      "9       -0.000391      1.439834     0.043195      -0.770098  0.215669   \n",
      "10      -0.138273     -0.227150    -0.379956      -0.320890 -0.777036   \n",
      "11       0.079886     -0.227150    -0.383584      -1.119483 -0.530740   \n",
      "12      -0.130607     -0.227150    -0.354202       0.677352 -0.887963   \n",
      "13      -0.160447     -0.218406    -0.368399       0.577528 -0.547661   \n",
      "14      -0.156326     -0.227150    -0.297212      -0.320890  0.572892   \n",
      "15      -0.151056     -0.227150    -0.356806      -0.021417 -0.609705   \n",
      "16       0.331619     -0.183922     1.030104      -0.770098  3.088497   \n",
      "17      -0.147619     -0.227150     0.111239      -0.021417 -0.380330   \n",
      "18      -0.162980     -0.227150    -0.392837       2.773661 -0.730033   \n",
      "19      -0.159671     -0.212118    -0.392864      -0.370802 -0.867282   \n",
      "20      -0.042186     -0.227150     0.367479      -0.021417  1.772411   \n",
      "21      -0.055360     -0.227101    -0.353425       1.126561 -0.408532   \n",
      "22      -0.081630     -0.227101    -0.267326      -0.770098 -0.047548   \n",
      "23      -0.153203     -0.227150    -0.336677      -0.221065 -0.716872   \n",
      "24       0.441461     -0.219339     0.378957      -0.770098  3.541606   \n",
      "25      -0.118706     -0.227150    -0.369237       3.771903 -0.688670   \n",
      "26      -0.132635     -0.227150    -0.392828      -0.121241 -0.880443   \n",
      "27      -0.112321     -0.227150    -0.367595      -0.820011 -0.818399   \n",
      "28      -0.155060     -0.227150    -0.359234      -1.119483 -0.683030   \n",
      "29       0.024891      0.303524     0.133293      -0.770098  1.885218   \n",
      "...           ...           ...          ...            ...       ...   \n",
      "31970   -0.132085     -0.227150     0.329550      -0.221065  1.324941   \n",
      "31971   -0.160560     -0.227150    -0.392290       0.577528 -0.008066   \n",
      "31972   -0.092913     -0.227150    -0.100504      -0.320890  0.097221   \n",
      "31973    0.235365     -0.227101    -0.045695      -0.520538  2.129634   \n",
      "31974   -0.092896     -0.227150     0.417504      -0.021417 -0.041908   \n",
      "31975   -0.160447     -0.218406    -0.368399       0.577528 -0.547661   \n",
      "31976    0.743575     -0.154546     1.532608      -0.770098  1.514834   \n",
      "31977   -0.156762     -0.227150    -0.350238       2.124803 -0.416052   \n",
      "31978   -0.157228      0.841174    -0.371197       0.377880 -0.807118   \n",
      "31979   -0.161867     -0.227150    -0.392113      -0.320890 -0.886083   \n",
      "31980   -0.078187     -0.227150    -0.378755       0.477704 -0.775156   \n",
      "31981   -0.152562     -0.227150    -0.364903      -1.119483 -0.840960   \n",
      "31982   -0.016297     -0.224104     0.548639      -0.021417  1.319301   \n",
      "31983   -0.146025     -0.227150    -0.218228      -0.770098 -0.620986   \n",
      "31984   -0.136530     -0.227150    -0.279942      -0.320890  1.104967   \n",
      "31985   -0.058167     -0.227150    -0.187884      -0.021417 -0.226160   \n",
      "31986   -0.160447     -0.218406    -0.368399       0.577528 -0.547661   \n",
      "31987   -0.162817     -0.227150    -0.391786       0.777177 -0.840960   \n",
      "31988   -0.159609     -0.227150    -0.391416      -0.021417 -0.865402   \n",
      "31989   -0.082065     -0.226953    -0.017451      -0.770098  1.716007   \n",
      "31990   -0.016297     -0.224104     0.548639      -0.021417  1.319301   \n",
      "31991    0.267488      5.784897     1.616721      -0.770098  1.990505   \n",
      "31992   -0.162924     -0.227150    -0.389173      -0.320890 -0.701831   \n",
      "31993   -0.160063     -0.227150    -0.372928       0.178231 -0.846600   \n",
      "31994   -0.036328     -0.227150    -0.085919      -0.420714  1.123768   \n",
      "31995    0.089301     -0.093781     0.352744      -0.770098  1.251617   \n",
      "31996   -0.081630     -0.227101    -0.267326      -0.770098 -0.047548   \n",
      "31997   -0.057918     -0.227150    -0.198973      -1.019659  0.093461   \n",
      "31998   -0.087555     -0.227150    -0.369343       0.078407 -0.442374   \n",
      "31999   -0.156644     -0.227150    -0.376689      -0.570450 -0.124633   \n",
      "\n",
      "             F2        F3        F4        F5        F6   ...          c4  \\\n",
      "0      0.361314  0.290350  0.423033 -0.035865  0.130563   ...   -0.299321   \n",
      "1     -0.638125 -0.510835 -0.805110 -0.035865 -0.828732   ...   -0.408900   \n",
      "2     -0.614223 -0.496268 -0.704525 -0.035865 -0.645901   ...   -0.361937   \n",
      "3     -0.574555 -0.467134 -0.680507 -0.035865 -0.623330   ...   -0.401073   \n",
      "4      0.805920  1.062402  0.343487 -0.035865 -0.061296   ...    0.060727   \n",
      "5     -0.330071 -0.467134  0.607067 -0.035865  1.207230   ...   -0.393246   \n",
      "6     -0.323751 -0.263196 -0.189155 -0.035865 -0.007124   ...   -0.385419   \n",
      "7     -0.023033  0.042711 -0.221380 -0.035865 -0.223812   ...   -0.244531   \n",
      "8     -0.592615 -0.481701 -0.722467 -0.035865 -0.799388   ...   -0.361937   \n",
      "9      0.039177 -0.044691  0.180062 -0.035865  0.444309   ...   -0.361937   \n",
      "10    -0.488412 -0.336031 -0.674529 -0.035865 -0.779074   ...   -0.377592   \n",
      "11    -0.353435 -0.292330 -0.422567 -0.035865 -0.499186   ...   -0.322802   \n",
      "12    -0.635938 -0.510835 -0.808256 -0.035865 -0.835503   ...   -0.408900   \n",
      "13    -0.441635 -0.350598 -0.548379 -0.035865 -0.607530   ...    0.326848   \n",
      "14    -0.323751 -0.263196 -0.189155 -0.035865 -0.007124   ...   -0.338456   \n",
      "15    -0.487479 -0.379732 -0.595703 -0.035865 -0.571415   ...   -0.244531   \n",
      "16     1.007887  0.639959  1.570028 -0.035865  3.559194   ...    0.005937   \n",
      "17    -0.023033  0.042711 -0.221380 -0.035865 -0.223812   ...    0.295540   \n",
      "18    -0.605665 -0.510835 -0.706249 -0.035865 -0.643644   ...   -0.408900   \n",
      "19    -0.625778 -0.496268 -0.790821 -0.035865 -0.821960   ...   -0.401073   \n",
      "20     1.221921  0.967717  1.624718 -0.035865  1.728635   ...   -0.330629   \n",
      "21     0.423007  0.720077 -0.322478 -0.035865 -0.329898   ...    0.397292   \n",
      "22    -0.285974 -0.306897 -0.165995 -0.035865  0.159906   ...   -0.361937   \n",
      "23    -0.554619 -0.452567 -0.666160 -0.035865 -0.672987   ...   -0.385419   \n",
      "24     1.719538  0.960433  3.779563 -0.035865  3.762339   ...    0.326848   \n",
      "25    -0.504744 -0.423433 -0.630880 -0.035865 -0.625587   ...   -0.322802   \n",
      "26    -0.638125 -0.510835 -0.805110 -0.035865 -0.828732   ...   -0.408900   \n",
      "27    -0.461512 -0.336031 -0.689276 -0.035865 -0.779074   ...   -0.354110   \n",
      "28    -0.334484 -0.234062 -0.533570 -0.035865 -0.641387   ...   -0.346283   \n",
      "29     0.269234  0.057278  0.836318 -0.035865  2.175553   ...   -0.275839   \n",
      "...         ...       ...       ...       ...       ...   ...         ...   \n",
      "31970  0.446224  0.232082  0.969107 -0.035865  0.654225   ...    0.193788   \n",
      "31971 -0.406213 -0.336031 -0.340064 -0.035865 -0.275727   ...   -0.236704   \n",
      "31972  0.191320  0.173814  0.084215 -0.035865  0.297593   ...   -0.056680   \n",
      "31973  1.633384  1.550397  1.589926 -0.035865  2.193610   ...    0.467737   \n",
      "31974  0.805920  1.062402  0.343487 -0.035865 -0.061296   ...    0.358157   \n",
      "31975 -0.441635 -0.350598 -0.548379 -0.035865 -0.607530   ...   -0.252358   \n",
      "31976  1.572480  1.222639  1.678116 -0.035865  1.439718   ...   -0.025372   \n",
      "31977 -0.527045 -0.452567 -0.514838 -0.035865 -0.266698   ...   -0.377592   \n",
      "31978 -0.574730 -0.467134 -0.720045 -0.035865 -0.747474   ...   -0.393246   \n",
      "31979 -0.632689 -0.510835 -0.804029 -0.035865 -0.833246   ...   -0.401073   \n",
      "31980 -0.576269 -0.481701 -0.690581 -0.035865 -0.718131   ...   -0.408900   \n",
      "31981 -0.567024 -0.438000 -0.752539 -0.035865 -0.785845   ...   -0.369765   \n",
      "31982  2.816758  3.101784  1.973658 -0.035865  0.683568   ...    0.553835   \n",
      "31983 -0.358078 -0.263196 -0.505766 -0.035865 -0.544329   ...   -0.244531   \n",
      "31984 -0.162114 -0.190361  0.075597 -0.035865  1.349431   ...   -0.041026   \n",
      "31985 -0.129364 -0.095675 -0.209045 -0.035865 -0.169640   ...   -0.322802   \n",
      "31986 -0.441635 -0.350598 -0.548379 -0.035865 -0.607530   ...   -0.189741   \n",
      "31987 -0.603025 -0.481701 -0.763376 -0.035865 -0.794874   ...   -0.393246   \n",
      "31988 -0.592964 -0.467134 -0.770698 -0.035865 -0.815189   ...   -0.408900   \n",
      "31989  0.633839  0.720077  0.625284 -0.035865  2.270354   ...    0.679069   \n",
      "31990  2.816758  3.101784  1.973658 -0.035865  0.683568   ...   -0.119297   \n",
      "31991  1.038639  0.836613  1.435440 -0.035865  2.320011   ...   -0.174087   \n",
      "31992 -0.620180 -0.510835 -0.725230 -0.035865 -0.821960   ...   -0.408900   \n",
      "31993 -0.592506 -0.467134 -0.760023 -0.035865 -0.794874   ...   -0.338456   \n",
      "31994 -0.330071 -0.467134  0.607067 -0.035865  1.207230   ...   -0.354110   \n",
      "31995  1.216711  0.945866  1.391303 -0.035865  1.615776   ...   -0.158432   \n",
      "31996 -0.285974 -0.306897 -0.165995 -0.035865  0.159906   ...   -0.393246   \n",
      "31997 -0.279253 -0.292330 -0.197994 -0.035865  0.098963   ...   -0.080161   \n",
      "31998 -0.553358 -0.496268 -0.469836 -0.035865 -0.517243   ...   -0.377592   \n",
      "31999 -0.574852 -0.481701 -0.494493 -0.035865  0.001905   ...   -0.377592   \n",
      "\n",
      "             c5  base_time  post_length  share_count  promotion  h_target  \\\n",
      "0     -0.006469   0.209917    -0.423366     0.076657  -0.011181  0.121774   \n",
      "1      0.001531  -0.698290    -0.186804    -0.146686  -0.011181  0.121774   \n",
      "2     -0.014468  -0.124686     0.008498    -0.145431  -0.011181  0.121774   \n",
      "3      0.001531   0.592320    -0.445372    -0.001137  -0.011181  0.121774   \n",
      "4      0.241520  -1.319694    -0.398609     0.054072  -0.011181  0.121774   \n",
      "5      0.009531  -1.271894    -0.335343    -0.121591  -0.011181  0.121774   \n",
      "6      0.013530  -0.889491    -0.142792    -0.145431  -0.011181  0.121774   \n",
      "7     -0.006469  -0.507088    -0.280328    -0.131629  -0.011181  0.121774   \n",
      "8      0.025530  -1.415295    -0.192305    -0.146686  -0.011181  0.121774   \n",
      "9      0.025530  -0.698290     0.055260    -0.144176  -0.011181  0.121774   \n",
      "10     0.017530  -1.176293    -0.343595    -0.047562  -0.011181  0.121774   \n",
      "11    -0.042467  -0.172486     0.536637    -0.146686  -0.011181  0.121774   \n",
      "12     0.001531   1.165924     0.157037    -0.146686  -0.011181  0.121774   \n",
      "13    -0.350453   0.162116    -0.109783    -0.141667  -0.011181  0.121774   \n",
      "14     0.001531   1.357125    -0.057519    -0.132884  -0.011181  0.121774   \n",
      "15    -0.006469   1.118123    -0.280328    -0.145431  -0.011181  0.121774   \n",
      "16    -0.198460   0.592320    -0.305085    -0.036269  -0.011181  0.121774   \n",
      "17    -0.058466  -0.268087    -0.365601    -0.134138  -0.011181  0.121774   \n",
      "18     0.001531  -0.793891     0.179043    -0.145431  -0.011181  0.121774   \n",
      "19     0.005531  -0.793891    -0.107032    -0.145431  -0.011181  0.121774   \n",
      "20     0.041529  -1.654297    -0.371102    -0.080185  -0.011181  0.121774   \n",
      "21     0.413512  -0.650490     1.771713     0.139394  -0.011181  0.121774   \n",
      "22    -0.002469   1.452726    -0.010757    -0.140412  -0.011181  0.121774   \n",
      "23    -0.010469   0.114316     0.022252    -0.142922  -0.011181  0.121774   \n",
      "24    -0.270457   0.162116    -0.041015     0.054072  -0.011181  0.121774   \n",
      "25    -0.006469   1.404925    -0.305085    -0.093987  -0.011181  0.121774   \n",
      "26     0.001531   1.357125    -0.362850    -0.146686  -0.011181  0.121774   \n",
      "27    -0.002469   1.500526    -0.450873    -0.146686  -0.011181  0.121774   \n",
      "28     0.001531  -0.315887    -0.450873    -0.145431  -0.011181  0.121774   \n",
      "29     0.069528  -1.606496     0.302826     0.277415  -0.011181  0.121774   \n",
      "...         ...        ...          ...          ...        ...       ...   \n",
      "31970 -0.130463   0.018715    -0.239067     0.445549  -0.011181  0.121774   \n",
      "31971  0.001531   1.691728    -0.071273     0.074148  -0.011181  0.121774   \n",
      "31972  0.181523  -1.319694    -0.390357    -0.071402  -0.011181  0.121774   \n",
      "31973  0.449510  -0.985092     0.616408     0.377793  -0.011181  0.121774   \n",
      "31974 -0.350453   0.496719    -0.181302    -0.080185  -0.011181  0.121774   \n",
      "31975  0.081527  -1.080693     0.126779    -0.125355  -0.011181  0.121774   \n",
      "31976  0.197522  -0.793891     0.913487    -0.047562  -0.011181  0.121774   \n",
      "31977  0.017530  -0.698290    -0.035514    -0.141667  -0.011181  0.121774   \n",
      "31978  0.001531  -0.459288     1.106038    -0.146686  -0.011181  0.121774   \n",
      "31979  0.001531   0.926922     0.008498    -0.146686  -0.011181  0.121774   \n",
      "31980  0.001531   0.496719     0.245060    -0.129120  -0.011181  0.121774   \n",
      "31981  0.001531  -0.315887    -0.302334    -0.144176  -0.011181  0.121774   \n",
      "31982  0.493508  -1.510896    -0.228064    -0.040033  -0.011181  0.121774   \n",
      "31983  0.085527  -0.841691     0.063513    -0.146686  -0.011181  0.121774   \n",
      "31984 -0.106464   0.640120    -0.351847    -0.129120  -0.011181  0.121774   \n",
      "31985 -0.022468   0.114316    -0.450873    -0.146686  -0.011181  0.121774   \n",
      "31986 -0.086465  -0.172486    -0.145543    -0.121591  -0.011181  0.121774   \n",
      "31987  0.001531   1.596127     2.093548    -0.141667  -0.011181  0.121774   \n",
      "31988  0.001531  -0.985092    -0.038264    -0.145431  -0.011181  0.121774   \n",
      "31989 -0.386451   0.162116     1.207815     0.527107  -0.011181  0.121774   \n",
      "31990  0.149524  -1.463095    -0.090528    -0.019958  -0.011181  0.121774   \n",
      "31991  0.121525  -0.985092    -0.046516    -0.055090  -0.011181  0.121774   \n",
      "31992  0.001531  -0.793891    -0.426117    -0.126610  -0.011181  0.121774   \n",
      "31993 -0.030468   0.401118     3.603697    -0.099006  -0.011181  0.121774   \n",
      "31994  0.029530  -1.319694    -0.450873     0.409162  -0.011181  0.121774   \n",
      "31995 -0.082465   0.687920     1.917502    -0.036269  -0.011181  0.121774   \n",
      "31996 -0.010469   0.687920    -0.065772    -0.144176  -0.011181  0.121774   \n",
      "31997 -0.022468   0.879122     0.036005    -0.146686  -0.011181  0.121774   \n",
      "31998 -0.006469  -0.076885    -0.159296    -0.124101  -0.011181  0.121774   \n",
      "31999  0.001531   1.739528     0.093771    -0.110299  -0.011181  0.121774   \n",
      "\n",
      "       post_day  basetime_day  target  \n",
      "0     -0.514370     -1.006866       2  \n",
      "1     -0.026989     -1.006866       1  \n",
      "2     -0.514370     -0.008471       1  \n",
      "3     -1.001750      1.489122       1  \n",
      "4      1.435154      0.490727       3  \n",
      "5      0.947773      1.489122       1  \n",
      "6     -1.489131     -0.507668       1  \n",
      "7     -0.026989     -1.006866       1  \n",
      "8     -0.026989     -0.008471       1  \n",
      "9      1.435154      0.490727       1  \n",
      "10     1.435154      0.490727       3  \n",
      "11     1.435154      0.490727       1  \n",
      "12     1.435154     -0.507668       1  \n",
      "13     1.435154     -1.506064       2  \n",
      "14     1.435154     -0.507668       1  \n",
      "15    -1.489131     -1.006866       1  \n",
      "16    -1.489131     -0.008471       2  \n",
      "17    -1.489131     -0.008471       3  \n",
      "18     0.947773      1.489122       1  \n",
      "19    -0.514370     -0.008471       1  \n",
      "20    -0.026989     -0.008471       3  \n",
      "21    -0.026989     -1.006866       2  \n",
      "22    -1.001750      0.490727       1  \n",
      "23    -0.514370     -1.006866       1  \n",
      "24    -1.489131     -0.507668       2  \n",
      "25    -1.489131     -1.006866       2  \n",
      "26     0.947773     -1.506064       1  \n",
      "27    -1.001750      0.490727       1  \n",
      "28    -0.514370     -0.008471       2  \n",
      "29    -0.026989     -0.008471       3  \n",
      "...         ...           ...     ...  \n",
      "31970 -1.489131     -0.008471       3  \n",
      "31971 -0.026989      1.489122       1  \n",
      "31972 -0.026989     -1.006866       3  \n",
      "31973 -0.514370     -0.008471       3  \n",
      "31974 -1.001750      1.489122       3  \n",
      "31975  1.435154      0.490727       2  \n",
      "31976 -1.489131     -0.507668       2  \n",
      "31977  0.460392      0.490727       3  \n",
      "31978  0.947773      1.489122       2  \n",
      "31979 -0.514370     -1.006866       1  \n",
      "31980  0.947773      0.490727       1  \n",
      "31981 -0.514370     -0.008471       1  \n",
      "31982 -0.026989     -0.008471       3  \n",
      "31983  1.435154      0.490727       3  \n",
      "31984 -0.026989      0.989925       1  \n",
      "31985 -1.001750      1.489122       1  \n",
      "31986 -0.514370     -0.008471       1  \n",
      "31987  0.460392     -0.008471       1  \n",
      "31988  1.435154      0.490727       1  \n",
      "31989 -0.514370     -0.008471       2  \n",
      "31990  0.947773      1.489122       3  \n",
      "31991  1.435154      1.489122       2  \n",
      "31992  1.435154      0.490727       1  \n",
      "31993  1.435154     -1.506064       2  \n",
      "31994 -0.026989     -0.008471       3  \n",
      "31995  0.947773      0.490727       2  \n",
      "31996 -1.489131     -0.008471       1  \n",
      "31997  1.435154     -1.506064       1  \n",
      "31998  1.435154      0.490727       1  \n",
      "31999  0.947773     -1.506064       1  \n",
      "\n",
      "[32000 rows x 25 columns]\n"
     ]
    }
   ],
   "source": [
    "val_percent=0.1\n",
    "val_size=int(val_percent*len(df.index))\n",
    "train_data=preprocessData(df)\n",
    "val_data=train_data.tail(val_size)\n",
    "train_data=train_data.head(len(df.index)-val_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehotencoding(y):\n",
    "    y=y.astype(int)\n",
    "#     print(y.shape)\n",
    "    max_y=max(y)\n",
    "#     max_y=3\n",
    "    encode_mat=np.zeros((y.shape[0],max_y))\n",
    "    rows=np.arange(y.shape[0])\n",
    "    #array([0, 1, 2, 3, 4])\n",
    "    encode_mat[rows,y-1]=1\n",
    "    return encode_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class nn:\n",
    "    #nn for classifier works for only one layer\n",
    "    def __init__(self, no_of_inputs, no_of_outputs, HUs,no_of_hidden_layers=1, activation=[\"sigmoid\",\"sigmoid\"],regu=0, dropout=0, weights_seed=0):\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.no_of_inputs=no_of_inputs\n",
    "        self.no_of_hidden_layers=no_of_hidden_layers\n",
    "        self.no_of_outputs=no_of_outputs\n",
    "        self.HUs=HUs #hidden units array\n",
    "        self.act=activation\n",
    "        self.dropout=dropout\n",
    "        self.weights_seed=weights_seed\n",
    "        self.regu=regu\n",
    "        #sanity checks\n",
    "        if(len(self.HUs) != self.no_of_hidden_layers):\n",
    "            print(\"error mismatch hidden units and layers\")\n",
    "        if((self.no_of_hidden_layers+1)!=len(activation)):\n",
    "            print(\"error mismatch activations and layers\")\n",
    "        self.initialize_weights()\n",
    "    \n",
    "    def initialize_weights(self):\n",
    "        \n",
    "        np.random.seed(self.weights_seed)\n",
    "        self.weights=[]\n",
    "        weights0=0.01 * np.random.randn(self.no_of_inputs+1,self.HUs[0])\n",
    "        self.weights.append(weights0);\n",
    "        # to add multiple layers\n",
    "        for i in range(1,self.no_of_hidden_layers):\n",
    "            weightsh=0.01 * np.random.randn(self.HUs[i-1]+1,self.HUs[i])\n",
    "            self.weights.append(np.copy(weightsh));\n",
    "              \n",
    "            \n",
    "        weightsl=0.01 * np.random.randn(self.HUs[-1]+1,self.no_of_outputs)\n",
    "        self.weights.append(weightsl);\n",
    "    \n",
    "    def get_weights(self):\n",
    "        return self.weights\n",
    "    \n",
    "    def sigmoid_forward(self,z):\n",
    "        return 1/(1+np.exp(-z))\n",
    "        \n",
    "    def sigmoid_backward(self,z):\n",
    "        return z*(1-z)\n",
    "    \n",
    "    def activation_forward(self,z,act_index):\n",
    "        if(self.act[act_index]==\"sigmoid\"):\n",
    "            return self.sigmoid_forward(z)\n",
    "        elif (self.act[act_index]==\"relu\"):\n",
    "            pass\n",
    "        else:\n",
    "            print(\"error invalid activation\")\n",
    "    \n",
    "    def activation_backward(self,z,act_index):\n",
    "        if(self.act[act_index]==\"sigmoid\"):\n",
    "            return self.sigmoid_backward(z)\n",
    "        elif (self.act[act_index]==\"relu\"):\n",
    "            pass\n",
    "        else:\n",
    "            print(\"error invalid activation\")\n",
    "    \n",
    "    def feed_forward(self, input_arr):\n",
    "#         print(input_arr.shape)\n",
    "        self.inps = np.ones((input_arr.shape[0],self.no_of_inputs+1))\n",
    "        self.inps[:,1:self.no_of_inputs+1]=input_arr\n",
    "        self.layers=[]\n",
    "        self.layers.append(self.inps)\n",
    "        layer0nodes=np.matmul(self.inps,self.weights[0])\n",
    "        layer0nodes=self.activation_forward(layer0nodes,0)\n",
    "        layer0nodes_bias=np.ones((layer0nodes.shape[0],layer0nodes.shape[1]+1))\n",
    "        layer0nodes_bias[:,1:layer0nodes.shape[1]+1]=layer0nodes\n",
    "        #for all the internal layer nodes\n",
    "        self.layers.append(layer0nodes_bias)\n",
    "        for i in range(1,self.no_of_hidden_layers):\n",
    "            layerinodes=np.matmul(self.layers[-1],self.weights[i])\n",
    "            layerinodes=self.activation_forward(layerinodes,i)\n",
    "            layerinodes_bias=np.ones((layerinodes.shape[0],layerinodes.shape[1]+1))\n",
    "            layerinodes_bias[:,1:layerinodes.shape[1]+1]=layerinodes\n",
    "            self.layers.append(np.copy(layerinodes_bias))\n",
    "            \n",
    "        #for the last layer\n",
    "        layer1nodes=np.matmul(self.layers[-1],self.weights[-1])\n",
    "        layer1nodes=self.activation_forward(layer1nodes,-1)\n",
    "        self.layers.append(layer1nodes)\n",
    "#         print(len(self.layers))\n",
    "        return self.layers[-1]\n",
    "        \n",
    "    def get_layers(self):\n",
    "        return self.layers\n",
    "    \n",
    "    #can use with sigmoid\n",
    "    def compute_cross_entropy_err(self,y_hat,y):\n",
    "        #cross entropy, expects one hot encoded y label\n",
    "        loss=0\n",
    "        #ylog(y_hat)+(1-y)log(1-y_hat)\n",
    "        loss_matrix=np.multiply(y,np.log(y_hat))+np.multiply(1-y,np.log(1-y_hat))\n",
    "        #computing sum of squares of weights\n",
    "        sum_square=0\n",
    "        for i in range(0,len(self.weights)):\n",
    "            sum_square=sum_square+np.sum((np.array(self.weights[i]))**2)\n",
    "        \n",
    "        loss=-1*np.sum(loss_matrix)*(1/y.shape[0])+self.regu*(1/y.shape[0])*sum_square\n",
    "        return loss\n",
    "    \n",
    "    def compute_gradients(self,y_hat,y):\n",
    "        #dE/d(sigma)\n",
    "        self.gradients=[]\n",
    "        dE_dsigmaL=(-1)*(np.multiply(y,1/y_hat)-np.multiply(1-y,1/(1-y_hat)))\n",
    "#         dE_dsigmaL=np.multiply(dE_dsigmaL_init,y)-np.multiply(dE_dsigmaL_init,1-y)\n",
    "        \n",
    "        dE_dsigmaL_dsumL=np.multiply(dE_dsigmaL,self.activation_backward(self.layers[-1],-1))\n",
    "        #average\n",
    "        #gradient for the last layer\n",
    "        #update weight with this gradient\n",
    "        dE_dsigmaL_dsumL_dw=np.matmul(self.layers[-2].T,dE_dsigmaL_dsumL)*(1/self.layers[-2].shape[0])\n",
    "        self.gradients.insert(0,dE_dsigmaL_dsumL_dw)\n",
    "        \n",
    "        for i in range(self.no_of_hidden_layers,0,-1):\n",
    "            #weights without bias\n",
    "            weights_wo_bias=self.weights[i][1:,:]\n",
    "            dE_dsigmaL_dsumL_dsigmal=np.matmul(dE_dsigmaL_dsumL,weights_wo_bias.T)\n",
    "            layer_wo_bias=self.layers[i][:,1:]\n",
    "            dE_dsigmaL_dsumL=np.multiply(dE_dsigmaL_dsumL_dsigmal,self.activation_backward(layer_wo_bias,i))\n",
    "            dE_dsigmal_dsuml_dw=np.matmul(self.layers[i-1].T,dE_dsigmaL_dsumL)*(1/self.layers[i-1].shape[0])\n",
    "            self.gradients.insert(0,np.copy(dE_dsigmal_dsuml_dw))\n",
    "        \n",
    "        return self.gradients\n",
    "    \n",
    "    def update_weights(self, gradients,learning_rate=0.001):\n",
    "        self.lr=learning_rate\n",
    "        for i in range(0,len(self.weights)):\n",
    "            self.weights[i]=np.copy(self.weights[i])-gradients[i]*learning_rate-self.regu*learning_rate*np.copy(self.weights[i])\n",
    "    \n",
    "    #dont use predict for now\n",
    "    def predict(self,input_arr):\n",
    "        inps = np.ones((input_arr.shape[0],self.no_of_inputs+1))\n",
    "        inps[:,1:self.no_of_inputs+1]=input_arr\n",
    "        self.layers=[]\n",
    "#         self.layers.append(self.inps)\n",
    "        layer0nodes=np.matmul(self.inps,self.weights[0])\n",
    "        layer0nodes=self.sigmoid_forward(layer0nodes)\n",
    "        layer0nodes_bias=np.ones((layer0nodes.shape[0],layer0nodes.shape[1]+1))\n",
    "        layer0nodes_bias[:,1:layer0nodes.shape[1]+1]=layer0nodes\n",
    "        #output layer nodes\n",
    "#         self.layers.append(layer0nodes_bias)\n",
    "        layer1nodes=np.matmul(layer0nodes_bias,self.weights[1])\n",
    "        layer1nodes=self.sigmoid_forward(layer1nodes)\n",
    "#         self.layers.append(layer1nodes)\n",
    "#         print(len(self.layers))\n",
    "        return layer1nodes\n",
    "        \n",
    "        \n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape\n",
    "\n",
    "network1=nn(no_of_inputs=24,no_of_outputs=3,HUs=[100],regu=0.01,no_of_hidden_layers=1, activation=[\"sigmoid\",\"sigmoid\"])\n",
    "train_data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data=train_data.values\n",
    "input_x=input_data[:,:-1]\n",
    "y_label=input_data[:,-1]\n",
    "y_encod=onehotencoding(y_label)\n",
    "val_input=val_data.values\n",
    "val_x=val_input[:,:-1]\n",
    "y_val_label=val_input[:,-1]\n",
    "y_val_encod=onehotencoding(y_val_label)\n",
    "# y_encod=y_encod[[0],:]\n",
    "# input_x=input_x[[0],:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_encod;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(input_x,y_encod,no_of_epochs=10000,batchSize=100):\n",
    "    print(input_x.shape)\n",
    "    print(y_encod.shape)\n",
    "    no_of_batches=int(input_x.shape[0]/batchSize)\n",
    "    lr=0.1\n",
    "    for i in range(0,no_of_epochs):\n",
    "        loss=0\n",
    "        for batch_no in range(0,no_of_batches):\n",
    "            idx = np.random.randint(input_x.shape[0], size=batchSize)\n",
    "            input_x_train=input_x[idx,:]\n",
    "            y_encod_train=y_encod[idx,:]\n",
    "            y_pred=network1.feed_forward(input_x_train)\n",
    "\n",
    "            loss=loss+network1.compute_cross_entropy_err(y_pred,y_encod_train)\n",
    "        #         print(y_pred)\n",
    "            grads=network1.compute_gradients(y_pred,y_encod_train)\n",
    "        #     if(i%1000==0):\n",
    "        #         print(grads)\n",
    "        #         print(network1.get_weights())\n",
    "            network1.update_weights(grads,learning_rate= lr)\n",
    "        if i%50==0:\n",
    "            loss=loss*1/no_of_batches\n",
    "            print(\"train loss: \"+ str(loss))\n",
    "            \n",
    "            #checking val loss\n",
    "            y_pred_val=network1.feed_forward(val_x)\n",
    "            val_loss=network1.compute_cross_entropy_err(y_pred_val,y_val_encod)\n",
    "            print(\"val loss: \"+ str(val_loss))\n",
    "        if (i+1)%1000==0:\n",
    "            lr=lr*0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28800, 24)\n",
      "(28800, 3)\n",
      "train loss: 1.6098729738252913\n",
      "val loss: 1.3931739627354598\n",
      "train loss: 1.224577144480735\n",
      "val loss: 1.200728788337614\n",
      "train loss: 1.2392791103222254\n",
      "val loss: 1.2066175945366504\n",
      "train loss: 1.2323440959505971\n",
      "val loss: 1.203128493476538\n",
      "train loss: 1.2110027746900283\n",
      "val loss: 1.1988450155135963\n",
      "train loss: 1.2252143035688459\n",
      "val loss: 1.2019370681271253\n",
      "train loss: 1.2278746542412486\n",
      "val loss: 1.1993951144701536\n",
      "train loss: 1.2317666155890388\n",
      "val loss: 1.2032374551750704\n",
      "train loss: 1.2259736481714314\n",
      "val loss: 1.2027410378165593\n",
      "train loss: 1.2309489411130532\n",
      "val loss: 1.2044561506586933\n",
      "train loss: 1.2347383527352151\n",
      "val loss: 1.2063256547745718\n",
      "train loss: 1.2224639329244846\n",
      "val loss: 1.2100005099292217\n",
      "train loss: 1.2255501981619252\n",
      "val loss: 1.2039362431341616\n",
      "train loss: 1.214564721204346\n",
      "val loss: 1.197742784367452\n",
      "train loss: 1.2291339054712704\n",
      "val loss: 1.2079337875380678\n",
      "train loss: 1.2335432599633291\n",
      "val loss: 1.200968352767608\n",
      "train loss: 1.2269188705041485\n",
      "val loss: 1.2055652576549203\n",
      "train loss: 1.2228699890075032\n",
      "val loss: 1.1986232281473796\n",
      "train loss: 1.2117375415425387\n",
      "val loss: 1.1985899962457849\n",
      "train loss: 1.2198622311253986\n",
      "val loss: 1.2025842526494128\n",
      "train loss: 1.2299347531575302\n",
      "val loss: 1.201750012910689\n",
      "train loss: 1.2295941269129111\n",
      "val loss: 1.199617556725723\n",
      "train loss: 1.2188993015931548\n",
      "val loss: 1.1995159089842562\n",
      "train loss: 1.2170014459293315\n",
      "val loss: 1.2011062726414667\n",
      "train loss: 1.2278054954391786\n",
      "val loss: 1.203372041540361\n",
      "train loss: 1.2228641484082343\n",
      "val loss: 1.2015865873233642\n",
      "train loss: 1.2255287607437841\n",
      "val loss: 1.2021395200971317\n",
      "train loss: 1.2257926771099399\n",
      "val loss: 1.2024195534526791\n",
      "train loss: 1.2098165863741306\n",
      "val loss: 1.2003113793555276\n",
      "train loss: 1.2133768816209043\n",
      "val loss: 1.2035656223002655\n",
      "train loss: 1.2299464178892934\n",
      "val loss: 1.2011298981595224\n",
      "train loss: 1.2247005244878595\n",
      "val loss: 1.1997488714114721\n",
      "train loss: 1.2273335144590256\n",
      "val loss: 1.2012464979693396\n",
      "train loss: 1.2362372196280824\n",
      "val loss: 1.2006337494166492\n",
      "train loss: 1.2307101023972016\n",
      "val loss: 1.205657967560916\n",
      "train loss: 1.2203011173310694\n",
      "val loss: 1.2003346210501606\n",
      "train loss: 1.2328097436302512\n",
      "val loss: 1.2034924369408033\n",
      "train loss: 1.2118486224055514\n",
      "val loss: 1.1991755744859567\n",
      "train loss: 1.2208849612420165\n",
      "val loss: 1.2013111193361272\n",
      "train loss: 1.2287374138671936\n",
      "val loss: 1.2022923878273337\n",
      "train loss: 1.2275029114986844\n",
      "val loss: 1.2002129510240698\n",
      "train loss: 1.2216113968055087\n",
      "val loss: 1.199761115425017\n",
      "train loss: 1.2270980974246126\n",
      "val loss: 1.2035617569157468\n",
      "train loss: 1.2236026868925838\n",
      "val loss: 1.2008114086216204\n",
      "train loss: 1.224204309963859\n",
      "val loss: 1.1992001109194275\n",
      "train loss: 1.2287601434745659\n",
      "val loss: 1.2051774224095329\n",
      "train loss: 1.2250496741134176\n",
      "val loss: 1.2036618638864227\n",
      "train loss: 1.221633921261791\n",
      "val loss: 1.202191711806027\n",
      "train loss: 1.2167683103864986\n",
      "val loss: 1.2022153652355065\n",
      "train loss: 1.2101268096951174\n",
      "val loss: 1.2017326418221084\n",
      "train loss: 1.2285596671323944\n",
      "val loss: 1.2019520993800832\n",
      "train loss: 1.2351197473197049\n",
      "val loss: 1.2005116229273018\n",
      "train loss: 1.2265863720187926\n",
      "val loss: 1.2032608335845103\n",
      "train loss: 1.2225947065858642\n",
      "val loss: 1.2008967252512812\n",
      "train loss: 1.2120587153797076\n",
      "val loss: 1.1988583441889278\n",
      "train loss: 1.2244726807499142\n",
      "val loss: 1.2023160260349892\n",
      "train loss: 1.2243978575096472\n",
      "val loss: 1.2011825945390828\n",
      "train loss: 1.224896185073664\n",
      "val loss: 1.2024170144184725\n",
      "train loss: 1.21488384131973\n",
      "val loss: 1.1995308705189245\n",
      "train loss: 1.2229758047414545\n",
      "val loss: 1.2035303112587759\n",
      "train loss: 1.2255824204602286\n",
      "val loss: 1.2006316997923978\n",
      "train loss: 1.217859842827175\n",
      "val loss: 1.2017857275694837\n",
      "train loss: 1.2294987434363696\n",
      "val loss: 1.2022522014684502\n",
      "train loss: 1.2205281792297569\n",
      "val loss: 1.2018177319606966\n",
      "train loss: 1.225805126086828\n",
      "val loss: 1.2014864126833575\n",
      "train loss: 1.2133092665083156\n",
      "val loss: 1.2009670535012293\n",
      "train loss: 1.2030867787631394\n",
      "val loss: 1.1990071196540477\n",
      "train loss: 1.2281143902704783\n",
      "val loss: 1.2011763264575257\n",
      "train loss: 1.2213022478306967\n",
      "val loss: 1.2016339093414483\n",
      "train loss: 1.2150787522558057\n",
      "val loss: 1.2009593838752968\n",
      "train loss: 1.2205589284496656\n",
      "val loss: 1.200165736310005\n",
      "train loss: 1.2197056715064325\n",
      "val loss: 1.2004112846893393\n",
      "train loss: 1.211864441975589\n",
      "val loss: 1.2006542577776953\n",
      "train loss: 1.2210576370728947\n",
      "val loss: 1.2003191973114313\n",
      "train loss: 1.2272764992426992\n",
      "val loss: 1.1997226074122607\n",
      "train loss: 1.2282660396153324\n",
      "val loss: 1.2003756502702785\n",
      "train loss: 1.2136498177556683\n",
      "val loss: 1.2027131437136733\n",
      "train loss: 1.2261521130990547\n",
      "val loss: 1.201091770807725\n",
      "train loss: 1.2208235336613695\n",
      "val loss: 1.2013969264605553\n",
      "train loss: 1.2332392494334306\n",
      "val loss: 1.2007138439612939\n",
      "train loss: 1.208574046628299\n",
      "val loss: 1.2009658434902912\n",
      "train loss: 1.2229843241416747\n",
      "val loss: 1.2005079270642285\n",
      "train loss: 1.2166524747365726\n",
      "val loss: 1.2010464674145116\n",
      "train loss: 1.223551697617608\n",
      "val loss: 1.199620813157863\n",
      "train loss: 1.214254187712225\n",
      "val loss: 1.199925871690636\n",
      "train loss: 1.217312168549208\n",
      "val loss: 1.2000869623763923\n",
      "train loss: 1.2175575643877274\n",
      "val loss: 1.2000157281873534\n",
      "train loss: 1.2178195210013432\n",
      "val loss: 1.200177814933855\n",
      "train loss: 1.219306738226727\n",
      "val loss: 1.2009981926658884\n",
      "train loss: 1.2218439613039336\n",
      "val loss: 1.200265292844171\n",
      "train loss: 1.214177716536038\n",
      "val loss: 1.2004174030772032\n",
      "train loss: 1.2213513471119102\n",
      "val loss: 1.2009711869578468\n",
      "train loss: 1.2235422522074038\n",
      "val loss: 1.2007606452818191\n",
      "train loss: 1.2327348871214034\n",
      "val loss: 1.2009241427072592\n",
      "train loss: 1.2122983446018294\n",
      "val loss: 1.200421286848165\n",
      "train loss: 1.2200842357083093\n",
      "val loss: 1.2009810381608084\n",
      "train loss: 1.227360726462635\n",
      "val loss: 1.1999563295429634\n",
      "train loss: 1.23611304144601\n",
      "val loss: 1.2010609911865628\n",
      "train loss: 1.2261634100182153\n",
      "val loss: 1.200318832953112\n",
      "train loss: 1.2145855552943838\n",
      "val loss: 1.2009320710737998\n",
      "train loss: 1.2176354712214805\n",
      "val loss: 1.2005973350451984\n",
      "train loss: 1.2223934094951603\n",
      "val loss: 1.201129692505214\n",
      "train loss: 1.2134354282863624\n",
      "val loss: 1.2007693139369575\n",
      "train loss: 1.216869456654237\n",
      "val loss: 1.1997087209784083\n",
      "train loss: 1.2177248123036166\n",
      "val loss: 1.2006391103694254\n",
      "train loss: 1.2137068710524845\n",
      "val loss: 1.2006753537403092\n",
      "train loss: 1.2230676007141488\n",
      "val loss: 1.2012998556897325\n",
      "train loss: 1.2223320202379666\n",
      "val loss: 1.200213318114441\n",
      "train loss: 1.218861289799959\n",
      "val loss: 1.2007714524489934\n",
      "train loss: 1.2124198729838216\n",
      "val loss: 1.1996907162332437\n",
      "train loss: 1.21852187166467\n",
      "val loss: 1.2001631352613742\n",
      "train loss: 1.2242613287803172\n",
      "val loss: 1.199643279292011\n",
      "train loss: 1.2279455511422992\n",
      "val loss: 1.2001654401701534\n",
      "train loss: 1.226094107337261\n",
      "val loss: 1.2000927167585793\n",
      "train loss: 1.2234932795947975\n",
      "val loss: 1.2007525186833725\n",
      "train loss: 1.2300575191177279\n",
      "val loss: 1.2007969321092045\n",
      "train loss: 1.2148646555513756\n",
      "val loss: 1.2003971112396976\n",
      "train loss: 1.2184046913349027\n",
      "val loss: 1.1998080464294831\n",
      "train loss: 1.2164386755876404\n",
      "val loss: 1.199730740944868\n",
      "train loss: 1.2222913825566686\n",
      "val loss: 1.2009027865935646\n",
      "train loss: 1.2202191320638904\n",
      "val loss: 1.200165789360611\n",
      "train loss: 1.2212031349020296\n",
      "val loss: 1.2001549987695357\n",
      "train loss: 1.2287580831391782\n",
      "val loss: 1.200485175912624\n",
      "train loss: 1.217593176581881\n",
      "val loss: 1.2005393715589536\n",
      "train loss: 1.2255747819543308\n",
      "val loss: 1.2002159475976606\n",
      "train loss: 1.2230600808052887\n",
      "val loss: 1.2003798114174333\n",
      "train loss: 1.2330157224151685\n",
      "val loss: 1.2006279855915\n",
      "train loss: 1.2106389558399575\n",
      "val loss: 1.200354667229511\n",
      "train loss: 1.223967784841912\n",
      "val loss: 1.2002973162113073\n",
      "train loss: 1.2256001324703467\n",
      "val loss: 1.2005524843842303\n",
      "train loss: 1.2169958844386704\n",
      "val loss: 1.2002370778192817\n",
      "train loss: 1.2251685793764935\n",
      "val loss: 1.2004944076922544\n",
      "train loss: 1.2146133553735488\n",
      "val loss: 1.2001468995434768\n",
      "train loss: 1.2352866777205744\n",
      "val loss: 1.200310773351684\n",
      "train loss: 1.21642633514541\n",
      "val loss: 1.2004698075310931\n",
      "train loss: 1.232182872798522\n",
      "val loss: 1.2002165232576556\n",
      "train loss: 1.207438776444759\n",
      "val loss: 1.1999569384791373\n",
      "train loss: 1.224999160474192\n",
      "val loss: 1.2004236988408663\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 1.2183038357480034\n",
      "val loss: 1.200119589254444\n",
      "train loss: 1.2284084707834486\n",
      "val loss: 1.2008124920615575\n",
      "train loss: 1.223694523924365\n",
      "val loss: 1.200440453705752\n",
      "train loss: 1.2291481038530465\n",
      "val loss: 1.200526541957938\n",
      "train loss: 1.2216873787619162\n",
      "val loss: 1.2001118327148688\n",
      "train loss: 1.2170890639326537\n",
      "val loss: 1.2001825551383025\n",
      "train loss: 1.2322008246408487\n",
      "val loss: 1.2006260817036325\n",
      "train loss: 1.216503158160452\n",
      "val loss: 1.2003827097230544\n",
      "train loss: 1.2158206065404498\n",
      "val loss: 1.1998354227283299\n",
      "train loss: 1.2226268432368288\n",
      "val loss: 1.2003395661101444\n",
      "train loss: 1.2094112215851123\n",
      "val loss: 1.2004012349386293\n",
      "train loss: 1.221903477328102\n",
      "val loss: 1.2004739810841947\n",
      "train loss: 1.2282770316927012\n",
      "val loss: 1.2005380476746204\n",
      "train loss: 1.2205034714305723\n",
      "val loss: 1.2003615085128516\n",
      "train loss: 1.2097484908052067\n",
      "val loss: 1.2004941970057474\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-8d58168d1c82>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_encod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-7c641d88f243>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(input_x, y_encod, no_of_epochs, batchSize)\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mnetwork1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_cross_entropy_err\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_encod_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m#         print(y_pred)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0mgrads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnetwork1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_encod_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0;31m#     if(i%1000==0):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m#         print(grads)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-f03b0f7f5073>\u001b[0m in \u001b[0;36mcompute_gradients\u001b[0;34m(self, y_hat, y)\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;31m#dE/d(sigma)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0mdE_dsigmaL\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0my_hat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0my_hat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;31m#         dE_dsigmaL=np.multiply(dE_dsigmaL_init,y)-np.multiply(dE_dsigmaL_init,1-y)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(input_x,y_encod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-42aacb8a6d31>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnetwork1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m     \"\"\"\n\u001b[0;32m--> 492\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "weights=network1.get_weights()\n",
    "np.asarray(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28800, 3)\n",
      "0.7351388888888889\n"
     ]
    }
   ],
   "source": [
    "def trainAccuracy(input_x,y_encod):\n",
    "    y_pred=network1.feed_forward(input_x)\n",
    "    y_pred_encod=(y_pred == y_pred.max(axis=1)[:,None]).astype(int)\n",
    "    print(y_pred_encod.shape)\n",
    "#     print(np.argmax(y_pred_encod,axis=1).shape)\n",
    "    correct_pred_encod=np.multiply(y_pred_encod,y_encod)\n",
    "    accuracy=np.sum(correct_pred_encod)/input_x.shape[0]\n",
    "    print(accuracy)\n",
    "    \n",
    "trainAccuracy(input_x,y_encod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessData_test(df):\n",
    "    df['post_day'] = df['post_day'].factorize(sort=True)[0]\n",
    "    df['basetime_day'] = df['basetime_day'].factorize(sort=True)[0]\n",
    "#     df=df.head(5)\n",
    "    df_norm=(df-train_mean)/train_std\n",
    "    df_norm.fillna(0,inplace=True)\n",
    "    return df_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('./../data/test.csv')\n",
    "test_data=preprocessData_test(df_test)\n",
    "input_data=test_data.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:42: RuntimeWarning: overflow encountered in exp\n"
     ]
    }
   ],
   "source": [
    "def testOutput(input_x_test):\n",
    "    y_pred=network1.feed_forward(input_x_test)\n",
    "    y_pred_arr=np.argmax(y_pred,axis=1)\n",
    "    y_pred_arr=y_pred_arr+1\n",
    "    file=open(\"submission.csv\",\"w\")\n",
    "    file.write(\"Id,predicted_class\\n\")\n",
    "    print(len(y_pred_arr))\n",
    "    for i in range(0,len(y_pred_arr)):\n",
    "        file.write(str(i+1)+\",\"+str(y_pred_arr[i])+\"\\n\")\n",
    "    file.close()\n",
    "\n",
    "testOutput(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 2)\n",
      "(5, 2)\n",
      "[[0.49765536 0.50368654 0.49381384]\n",
      " [0.49764957 0.50367189 0.49383668]\n",
      " [0.49764852 0.50367562 0.4938206 ]\n",
      " [0.49765452 0.5036914  0.49379497]\n",
      " [0.49765335 0.50368932 0.4937968 ]]\n",
      "[array([[ 0.01624345, -0.00611756, -0.00528172],\n",
      "       [-0.01072969,  0.00865408, -0.02301539],\n",
      "       [ 0.01744812, -0.00761207,  0.00319039]]), array([[-0.0024937 ,  0.01462108, -0.02060141],\n",
      "       [-0.00322417, -0.00384054,  0.01133769],\n",
      "       [-0.01099891, -0.00172428, -0.00877858],\n",
      "       [ 0.00042214,  0.00582815, -0.01100619]])]\n",
      "[array([[ 1.        , -0.60865379,  0.23565675],\n",
      "       [ 1.        ,  1.18539118,  1.39296744],\n",
      "       [ 1.        ,  0.98290729,  0.32057733],\n",
      "       [ 1.        , -0.92479789, -1.00327539],\n",
      "       [ 1.        , -0.63484679, -0.94592614]]), array([[1.        , 0.50672107, 0.49670536, 0.50236961],\n",
      "       [1.        , 0.50695686, 0.49838439, 0.4929705 ],\n",
      "       [1.        , 0.50282263, 0.49998708, 0.49328017],\n",
      "       [1.        , 0.50216523, 0.49837905, 0.50320046],\n",
      "       [1.        , 0.50163763, 0.49889722, 0.50157791]]), array([[0.49765536, 0.50368654, 0.49381384],\n",
      "       [0.49764957, 0.50367189, 0.49383668],\n",
      "       [0.49764852, 0.50367562, 0.4938206 ],\n",
      "       [0.49765452, 0.5036914 , 0.49379497],\n",
      "       [0.49765335, 0.50368932, 0.4937968 ]])]\n"
     ]
    }
   ],
   "source": [
    "print(input_x.shape)\n",
    "input_x_part=input_x\n",
    "print(input_x_part.shape)\n",
    "y_pred=network1.feed_forward(input_x_part)\n",
    "print(y_pred)\n",
    "weights=network1.get_weights()\n",
    "print(weights)\n",
    "layers=network1.get_layers()\n",
    "print(layers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.99066523 -1.98536177 -1.97555777]\n",
      " [-1.99064225 -2.01479624 -2.02496096]\n",
      " [-1.99063813 -2.01481138 -2.02502692]\n",
      " [-2.00942613 -2.01487543 -1.97548412]\n",
      " [-1.99065726 -2.01486698 -2.0251245 ]]\n"
     ]
    }
   ],
   "source": [
    "dE_dsigmaL=(-1)*(np.multiply(y_encod,1/y_pred)+np.multiply(1-y_encod,1/(1-y_pred)))\n",
    "print(dE_dsigmaL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.         -1.98536177 -0.        ]\n",
      " [-0.         -0.         -2.02496096]\n",
      " [-0.         -0.         -2.02502692]\n",
      " [-2.00942613 -0.         -0.        ]\n",
      " [-0.         -0.         -2.0251245 ]]\n",
      "[[-0.         -0.49631346 -0.        ]\n",
      " [-0.         -0.         -0.50616332]\n",
      " [-0.         -0.         -0.5061794 ]\n",
      " [-0.50234548 -0.         -0.        ]\n",
      " [-0.         -0.         -0.5062032 ]]\n",
      "[[0.2499945  0.24998641 0.24996173]\n",
      " [0.24999448 0.24998652 0.24996201]\n",
      " [0.24999447 0.24998649 0.24996181]\n",
      " [0.2499945  0.24998637 0.2499615 ]\n",
      " [0.24999449 0.24998639 0.24996152]]\n",
      "(4, 3)\n"
     ]
    }
   ],
   "source": [
    "dE_dsigmaL=np.multiply(dE_dsigmaL,y_encod)\n",
    "print(dE_dsigmaL)\n",
    "dE_dsigmaL_dsumL=np.multiply(dE_dsigmaL,network1.sigmoid_backward(layers[-1]))\n",
    "print(dE_dsigmaL_dsumL)\n",
    "print(network1.sigmoid_backward(layers[-1]))\n",
    "print(weights[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.50672107 0.49670536 0.50236961]\n",
      " [1.         0.50695686 0.49838439 0.4929705 ]\n",
      " [1.         0.50282263 0.49998708 0.49328017]\n",
      " [1.         0.50216523 0.49837905 0.50320046]\n",
      " [1.         0.50163763 0.49889722 0.50157791]]\n",
      "[[1.         1.         1.         1.         1.        ]\n",
      " [0.50672107 0.50695686 0.50282263 0.50216523 0.50163763]\n",
      " [0.49670536 0.49838439 0.49998708 0.49837905 0.49889722]\n",
      " [0.50236961 0.4929705  0.49328017 0.50320046 0.50157791]]\n",
      "[[-0.         -0.49631346 -0.        ]\n",
      " [-0.         -0.         -0.50616332]\n",
      " [-0.         -0.         -0.5061794 ]\n",
      " [-0.50234548 -0.         -0.        ]\n",
      " [-0.         -0.         -0.5062032 ]]\n",
      "[[-0.1004691  -0.09926269 -0.30370918]\n",
      " [-0.05045209 -0.0502985  -0.1530104 ]\n",
      " [-0.05007169 -0.04930431 -0.15157809]\n",
      " [-0.0505561  -0.04986656 -0.15062244]]\n",
      "(5, 3)\n"
     ]
    }
   ],
   "source": [
    "print(layers[-2])\n",
    "print(layers[-2].T)\n",
    "print(dE_dsigmaL_dsumL)\n",
    "dE_dsigmaL_dsumL_dw=np.matmul(layers[-2].T,dE_dsigmaL_dsumL)*(1/layers[-2].shape[0])\n",
    "print(dE_dsigmaL_dsumL_dw)\n",
    "#update weight with this gradient\n",
    "print(dE_dsigmaL_dsumL.shape)\n",
    "#weights without bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.         -0.49631346 -0.        ]\n",
      " [-0.         -0.         -0.49383668]\n",
      " [-0.         -0.         -0.4938206 ]\n",
      " [-0.49765452 -0.         -0.        ]\n",
      " [-0.         -0.         -0.4937968 ]]\n",
      "[[-0.00322417 -0.01099891  0.00042214]\n",
      " [-0.00384054 -0.00172428  0.00582815]\n",
      " [ 0.01133769 -0.00877858 -0.01100619]]\n",
      "[[ 0.00190611  0.00085578 -0.00289259]\n",
      " [-0.00559897  0.00433519  0.00543526]\n",
      " [-0.00559879  0.00433505  0.00543508]\n",
      " [ 0.00160452  0.00547366 -0.00021008]\n",
      " [-0.00559852  0.00433484  0.00543482]]\n",
      "(5, 3)\n",
      "(5, 4)\n"
     ]
    }
   ],
   "source": [
    "print(dE_dsigmaL_dsumL)\n",
    "weights_wo_bias=weights[1][1:,:]\n",
    "print(weights_wo_bias.T)\n",
    "dE_dsigmaL_dsumL_dsigmal=np.matmul(dE_dsigmaL_dsumL,weights_wo_bias.T)\n",
    "print(dE_dsigmaL_dsumL_dsigmal)\n",
    "print(dE_dsigmaL_dsumL_dsigmal.shape)\n",
    "print(network1.sigmoid_backward(layers[-2]).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2.01468662 -2.0021407  -1.99843484]\n",
      " [-2.0147238  -1.99790979 -2.00170064]]\n",
      "[[-0.50364489 -0.5005346  -0.4996084 ]\n",
      " [-0.50365405 -0.4994769  -0.5004248 ]]\n",
      "[[1.         0.50537419 0.49874734]\n",
      " [1.         0.50550913 0.48727871]]\n",
      "(3, 3)\n",
      "[[-0.50364947 -0.50000575 -0.5000166 ]\n",
      " [-0.25456542 -0.2527237  -0.25272925]\n",
      " [-0.24830572 -0.24651238 -0.24651236]]\n",
      "(2, 3)\n",
      "[[ 0.00423022 -0.00211824]\n",
      " [ 0.00426253 -0.00213153]]\n",
      "(2, 2)\n",
      "(2, 3)\n",
      "[[1.         0.50537419 0.49874734]\n",
      " [1.         0.50550913 0.48727871]]\n",
      "[[ 0.00105743 -0.00052956]\n",
      " [ 0.0010655  -0.00053254]]\n",
      "(2, 3)\n",
      "(2, 2)\n",
      "[[ 0.00106147 -0.00053105]\n",
      " [ 0.00030971 -0.00015447]\n",
      " [ 0.0008667  -0.0004333 ]]\n",
      "[[ 0.01624345 -0.00611756]\n",
      " [-0.00528172 -0.01072969]\n",
      " [ 0.00865408 -0.02301539]]\n"
     ]
    }
   ],
   "source": [
    "dE_dsigmaL=(-1)*(np.multiply(y_encod,1/y_pred)+np.multiply(1-y_encod,1/(1-y_pred)))\n",
    "print(dE_dsigmaL)\n",
    "\n",
    "dE_dsigmaL_dsumL=np.multiply(dE_dsigmaL,network1.sigmoid_backward(layers[-1]))\n",
    "print(dE_dsigmaL_dsumL)\n",
    "print(layers[1])\n",
    "print(weights[1].shape)\n",
    "#average\n",
    "dE_dsigmaL_dsumL_dw=np.matmul(layers[-2].T,dE_dsigmaL_dsumL)*(1/layers[-2].shape[0])\n",
    "print(dE_dsigmaL_dsumL_dw)\n",
    "#update weight with this gradient\n",
    "print(dE_dsigmaL_dsumL.shape)\n",
    "#weights without bias\n",
    "weights_wo_bias=weights[1][1:,:]\n",
    "dE_dsigmaL_dsumL_dsigmal=np.matmul(dE_dsigmaL_dsumL,weights_wo_bias.T)\n",
    "print(dE_dsigmaL_dsumL_dsigmal)\n",
    "print(dE_dsigmaL_dsumL_dsigmal.shape)\n",
    "print(network1.sigmoid_backward(layers[-2]).shape)\n",
    "#layer without bias\n",
    "print(layers[-2])\n",
    "layer_wo_bias=layers[-2][:,1:]\n",
    "dE_dsigmal_dsuml=np.multiply(dE_dsigmaL_dsumL_dsigmal,network1.sigmoid_backward(layer_wo_bias))\n",
    "print(dE_dsigmal_dsuml)\n",
    "print(layers[0].shape)\n",
    "print(dE_dsigmal_dsuml.shape)\n",
    "dE_dsigmal_dsuml_dw=np.matmul(layers[0].T,dE_dsigmal_dsuml)*(1/layers[0].shape[0])\n",
    "print(dE_dsigmal_dsuml_dw)\n",
    "print(weights[0])\n",
    "# dE_dsigmaL_dsumL_dw_mean=np.mean(dE_dsigmaL_dsumL_dw,axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.73105858 0.73105858 0.73105858 0.73105858]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "range(0, 10)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "range(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 3., 3., 1., 3.])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "9\n",
      "8\n",
      "7\n",
      "6\n",
      "5\n",
      "4\n",
      "3\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "for i in range(10,1,-1):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def a(b=[2,3]):\n",
    "    c=b[0]\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'builtin_function_or_method' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-7daed7379c46>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'builtin_function_or_method' object is not subscriptable"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

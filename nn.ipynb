{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   page_likes  page_checkin  daily_crowd  page_category   F1          F2  \\\n",
      "0      400487             0        57833             13  414   26.635492   \n",
      "1     2667410           141       111855             18  793  185.317073   \n",
      "2     2411555             0        61797              9  560  103.860465   \n",
      "3        1013             0            1             85   16    2.224299   \n",
      "4      367390             0         2678            100  110   11.828175   \n",
      "\n",
      "      F3          F4  F5   F6   ...     c4  c5  base_time  post_length  \\\n",
      "0   15.0   44.941598   0  300   ...     43  43         24          100   \n",
      "1  157.0  164.739179   0  419   ...    216 -29         68          118   \n",
      "2   78.0  107.011971   0  325   ...      0   0          0          119   \n",
      "3    1.0    3.489124   0    7   ...      1   1          1            0   \n",
      "4    6.0   15.430200   0   96   ...      9   9         19           53   \n",
      "\n",
      "   share_count  promotion  h_target   post_day  basetime_day  target  \n",
      "0           20          0        24  wednesday      thursday       2  \n",
      "1           71          0        24   saturday       tuesday       3  \n",
      "2          207          0        24     friday        friday       3  \n",
      "3            1          0        24    tuesday       tuesday       1  \n",
      "4            4          0        24     friday      saturday       3  \n",
      "\n",
      "[5 rows x 25 columns]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "32000"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('./data/train.csv')\n",
    "train_mean=0\n",
    "train_std=0\n",
    "print(df.head())\n",
    "len(df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessData(df):\n",
    "    df['post_day'] = df['post_day'].factorize(sort=True)[0]\n",
    "    df['basetime_day'] = df['basetime_day'].factorize(sort=True)[0]\n",
    "#     df=df.head(100)\n",
    "#     print(df)\n",
    "    df_norm=df.drop(labels='target', axis=1)\n",
    "    print(df_norm.mean())\n",
    "    print(df_norm.std())\n",
    "    df_norm=(df_norm-df_norm.mean())/df_norm.std()\n",
    "    global train_mean\n",
    "    global train_std\n",
    "    train_mean=df_norm.mean()\n",
    "    train_std=df_norm.std()\n",
    "#     df_norm=(df_norm-df_norm.min())/(df_norm.max()-df_norm.min())\n",
    "    df_norm.fillna(0,inplace=True)\n",
    "    print(df_norm.head(5))\n",
    "    df_norm=pd.concat([df_norm, df['target']], axis=1)\n",
    "#     print(df_norm)\n",
    "#     df_norm=df_norm[['page_likes','daily_crowd','target']]\n",
    "    print(df_norm)\n",
    "    return df_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_likes       1.343977e+06\n",
      "page_checkin     4.624117e+03\n",
      "daily_crowd      4.449958e+04\n",
      "page_category    2.442909e+01\n",
      "F1               4.762900e+02\n",
      "F2               5.528131e+01\n",
      "F3               3.506795e+01\n",
      "F4               6.726324e+01\n",
      "F5               8.284375e-02\n",
      "F6               3.731562e+02\n",
      "F7               2.122784e+01\n",
      "F8               7.013547e+00\n",
      "c1               5.541925e+01\n",
      "c2               2.125191e+01\n",
      "c3               2.030247e+01\n",
      "c4               5.224150e+01\n",
      "c5              -3.827500e-01\n",
      "base_time        3.560847e+01\n",
      "post_length      1.639106e+02\n",
      "share_count      1.179058e+02\n",
      "promotion        1.250000e-04\n",
      "h_target         2.377538e+01\n",
      "post_day         3.055375e+00\n",
      "basetime_day     3.016969e+00\n",
      "dtype: float64\n",
      "page_likes       8.226959e+06\n",
      "page_checkin     2.035713e+04\n",
      "daily_crowd      1.132647e+05\n",
      "page_category    2.003522e+01\n",
      "F1               5.318801e+02\n",
      "F2               8.608946e+01\n",
      "F3               6.864825e+01\n",
      "F4               8.215990e+01\n",
      "F5               2.309889e+00\n",
      "F6               4.430340e+02\n",
      "F7               5.676682e+01\n",
      "F8               2.052160e+01\n",
      "c1               1.367967e+02\n",
      "c2               7.314642e+01\n",
      "c3               7.551940e+01\n",
      "c4               1.277610e+02\n",
      "c5               2.500114e+02\n",
      "base_time        2.092035e+01\n",
      "post_length      3.635404e+02\n",
      "share_count      7.969809e+02\n",
      "promotion        1.117982e-02\n",
      "h_target         1.844610e+00\n",
      "post_day         2.051783e+00\n",
      "basetime_day     2.003215e+00\n",
      "dtype: float64\n",
      "   page_likes  page_checkin  daily_crowd  page_category        F1        F2  \\\n",
      "0   -0.114683     -0.227150     0.117719      -0.570450 -0.117113 -0.332745   \n",
      "1    0.160865     -0.220223     0.594673      -0.320890  0.595454  1.510473   \n",
      "2    0.129766     -0.227150     0.152717      -0.770098  0.157385  0.564287   \n",
      "3   -0.163239     -0.227150    -0.392872       3.023221 -0.865402 -0.616301   \n",
      "4   -0.118706     -0.227150    -0.369237       3.771903 -0.688670 -0.504744   \n",
      "\n",
      "         F3        F4        F5        F6      ...             c3        c4  \\\n",
      "0 -0.292330 -0.271685 -0.035865 -0.165126      ...      -0.268838 -0.072334   \n",
      "1  1.776186  1.186417 -0.035865  0.103477      ...       0.221103  1.281756   \n",
      "2  0.625392  0.483797 -0.035865 -0.108696      ...      -0.268838 -0.408900   \n",
      "3 -0.496268 -0.776220 -0.035865 -0.826474      ...      -0.268838 -0.401073   \n",
      "4 -0.423433 -0.630880 -0.035865 -0.625587      ...      -0.268838 -0.338456   \n",
      "\n",
      "         c5  base_time  post_length  share_count  promotion  h_target  \\\n",
      "0  0.173523  -0.554889    -0.175801    -0.122846  -0.011181  0.121774   \n",
      "1 -0.114464   1.548326    -0.126288    -0.058854  -0.011181  0.121774   \n",
      "2  0.001531  -1.702097    -0.123537     0.111790  -0.011181  0.121774   \n",
      "3  0.005531  -1.654297    -0.450873    -0.146686  -0.011181  0.121774   \n",
      "4  0.037529  -0.793891    -0.305085    -0.142922  -0.011181  0.121774   \n",
      "\n",
      "   post_day  basetime_day  \n",
      "0  1.435154      0.490727  \n",
      "1 -0.514370      0.989925  \n",
      "2 -1.489131     -1.506064  \n",
      "3  0.947773      0.989925  \n",
      "4 -1.489131     -0.507668  \n",
      "\n",
      "[5 rows x 24 columns]\n",
      "       page_likes  page_checkin  daily_crowd  page_category        F1  \\\n",
      "0       -0.114683     -0.227150     0.117719      -0.570450 -0.117113   \n",
      "1        0.160865     -0.220223     0.594673      -0.320890  0.595454   \n",
      "2        0.129766     -0.227150     0.152717      -0.770098  0.157385   \n",
      "3       -0.163239     -0.227150    -0.392872       3.023221 -0.865402   \n",
      "4       -0.118706     -0.227150    -0.369237       3.771903 -0.688670   \n",
      "5       -0.154536     -0.227150    -0.361371      -0.520538  1.479112   \n",
      "6        0.468763     -0.227101     0.698721      -0.770098  0.516489   \n",
      "7       -0.153248     -0.207845    -0.284648      -0.320890 -0.758235   \n",
      "8       -0.119316     -0.226511     0.050893      -0.021417 -0.109592   \n",
      "9       -0.082065     -0.226953    -0.017451      -0.770098  1.716007   \n",
      "10       0.359318     -0.227150     2.198950      -0.320890  1.349383   \n",
      "11      -0.137950     -0.227150    -0.388961       1.525858 -0.587144   \n",
      "12      -0.151325     -0.206813    -0.383584       0.377880 -0.526980   \n",
      "13      -0.145234     -0.227150    -0.391601      -0.520538 -0.677389   \n",
      "14       0.358144     -0.226609     0.767427      -0.770098  1.567477   \n",
      "15       0.386845     -0.227150     0.588307      -0.770098  0.696980   \n",
      "16      -0.162959     -0.226659    -0.392872      -0.370802 -0.837200   \n",
      "17      -0.153371     -0.227150    -0.359437      -0.520538 -0.816519   \n",
      "18      -0.160251     -0.127774    -0.376680       0.377880 -0.551421   \n",
      "19      -0.162959     -0.226659    -0.392872      -0.370802 -0.837200   \n",
      "20      -0.160684     -0.227150    -0.372487      -0.021417 -0.822159   \n",
      "21      -0.159382     -0.150567    -0.386083       0.377880 -0.867282   \n",
      "22      -0.144490     -0.164076    -0.371215       1.775419 -0.865402   \n",
      "23      -0.155302     -0.197676    -0.388979       2.524100 -0.884203   \n",
      "24      -0.137839     -0.226167    -0.391248       1.076649 -0.887963   \n",
      "25      -0.160294     -0.207845    -0.338301      -0.021417 -0.656708   \n",
      "26      -0.137950     -0.227150    -0.388961       1.525858 -0.587144   \n",
      "27      -0.162320     -0.227150    -0.387681       0.976825 -0.886083   \n",
      "28      -0.152438     -0.207845    -0.381783      -0.520538 -0.865402   \n",
      "29      -0.100987     -0.227150     0.255926       0.877001  1.826934   \n",
      "...           ...           ...          ...            ...       ...   \n",
      "31970   -0.123301     -0.227150    -0.293371      -1.019659 -0.662348   \n",
      "31971   -0.163030     -0.203030    -0.390436       1.475946 -0.861641   \n",
      "31972   -0.162513     -0.227150    -0.388025       2.773661 -0.810878   \n",
      "31973   -0.160236     -0.227150    -0.361221      -1.119483 -0.782676   \n",
      "31974   -0.113150     -0.227150    -0.222475      -0.021417 -0.139674   \n",
      "31975   -0.091016     -0.222287    -0.379099      -0.420714  0.452564   \n",
      "31976   -0.153248     -0.207845    -0.284648      -0.320890 -0.758235   \n",
      "31977   -0.086418     -0.218160     1.098492      -0.320890  1.001560   \n",
      "31978    1.356287     -0.227101    -0.072464      -0.520538  1.793092   \n",
      "31979   -0.160629     -0.227150    -0.390233      -0.420714 -0.124633   \n",
      "31980   -0.161974     -0.227101    -0.363481      -0.820011 -0.848481   \n",
      "31981    0.089301     -0.093781     0.352744      -0.770098  1.251617   \n",
      "31982    0.756174     -0.227150     0.695198      -0.620362 -0.810878   \n",
      "31983   -0.152138     -0.227150    -0.349911      -0.021417 -0.478096   \n",
      "31984   -0.144516     -0.227150    -0.090121      -0.320890  1.281699   \n",
      "31985   -0.152562     -0.227150    -0.364903      -1.119483 -0.840960   \n",
      "31986   -0.154447     -0.227150    -0.379779      -0.320890 -0.748834   \n",
      "31987   -0.131256     -0.227150    -0.325314      -0.021417 -0.649188   \n",
      "31988   -0.144490     -0.164076    -0.371215       1.775419 -0.865402   \n",
      "31989    0.098736     -0.221550     0.024133      -0.770098 -0.096431   \n",
      "31990   -0.160251     -0.127774    -0.376680       0.377880 -0.551421   \n",
      "31991   -0.126832     -0.213788    -0.145205      -0.021417  0.764665   \n",
      "31992   -0.163294     -0.227150    -0.392881      -0.420714 -0.872922   \n",
      "31993   -0.056448     -0.226609     0.044501      -0.770098  1.167011   \n",
      "31994   -0.145234     -0.227150    -0.391601      -0.520538 -0.677389   \n",
      "31995   -0.162704      0.990458    -0.390109       1.475946 -0.884203   \n",
      "31996   -0.160536     -0.227150    -0.352313      -0.820011 -0.649188   \n",
      "31997   -0.019758     -0.227150    -0.372513      -0.370802 -0.818399   \n",
      "31998   -0.123635     -0.227150     0.422501      -0.320890  0.281473   \n",
      "31999   -0.147423     -0.227150     0.080223       2.074891 -0.179157   \n",
      "\n",
      "             F2        F3        F4        F5        F6   ...          c4  \\\n",
      "0     -0.332745 -0.292330 -0.271685 -0.035865 -0.165126   ...   -0.072334   \n",
      "1      1.510473  1.776186  1.186417 -0.035865  0.103477   ...    1.281756   \n",
      "2      0.564287  0.625392  0.483797 -0.035865 -0.108696   ...   -0.408900   \n",
      "3     -0.616301 -0.496268 -0.776220 -0.035865 -0.826474   ...   -0.401073   \n",
      "4     -0.504744 -0.423433 -0.630880 -0.035865 -0.625587   ...   -0.338456   \n",
      "5     -0.312866 -0.423433  0.781318 -0.035865  0.417223   ...   -0.338456   \n",
      "6      1.633529  1.157088  1.639571 -0.035865  0.351765   ...   -0.119297   \n",
      "7     -0.497374 -0.365165 -0.686447 -0.035865 -0.682016   ...   -0.377592   \n",
      "8      0.652459  0.858464  0.312281 -0.035865 -0.040982   ...    0.702550   \n",
      "9      0.633839  0.720077  0.625284 -0.035865  2.270354   ...   -0.025372   \n",
      "10     3.344539  2.926980  2.751155 -0.035865  1.227544   ...    6.150220   \n",
      "11    -0.603910 -0.496268 -0.702337 -0.035865 -0.677502   ...   -0.408900   \n",
      "12    -0.561675 -0.481701 -0.623400 -0.035865 -0.440499   ...   -0.393246   \n",
      "13    -0.614223 -0.496268 -0.704525 -0.035865 -0.645901   ...   -0.408900   \n",
      "14     0.992603  0.436021  1.697633 -0.035865  1.936294   ...    0.217269   \n",
      "15     0.097007 -0.088392  0.438812 -0.035865  0.148620   ...   -0.213222   \n",
      "16    -0.601212 -0.481701 -0.761527 -0.035865 -0.801646   ...   -0.346283   \n",
      "17    -0.550082 -0.423433 -0.731498 -0.035865 -0.765531   ...   -0.401073   \n",
      "18    -0.507297 -0.510835 -0.415509 -0.035865 -0.472100   ...   -0.408900   \n",
      "19    -0.601212 -0.481701 -0.761527 -0.035865 -0.801646   ...   -0.330629   \n",
      "20    -0.541773 -0.438000 -0.705004 -0.035865 -0.761017   ...   -0.377592   \n",
      "21    -0.626551 -0.510835 -0.791026 -0.035865 -0.808417   ...   -0.377592   \n",
      "22    -0.626065 -0.510835 -0.790674 -0.035865 -0.812931   ...   -0.408900   \n",
      "23    -0.632634 -0.496268 -0.804727 -0.035865 -0.830989   ...   -0.401073   \n",
      "24    -0.635604 -0.510835 -0.808256 -0.035865 -0.835503   ...   -0.408900   \n",
      "25    -0.416463 -0.292330 -0.602022 -0.035865 -0.593987   ...   -0.322802   \n",
      "26    -0.603910 -0.496268 -0.702337 -0.035865 -0.677502   ...   -0.361937   \n",
      "27    -0.636763 -0.510835 -0.808361 -0.035865 -0.833246   ...   -0.401073   \n",
      "28    -0.615273 -0.481701 -0.789885 -0.035865 -0.817446   ...   -0.408900   \n",
      "29     0.793993 -0.066541  1.940152 -0.035865  1.730892   ...   -0.361937   \n",
      "...         ...       ...       ...       ...       ...   ...         ...   \n",
      "31970 -0.507338 -0.408866 -0.624035 -0.035865 -0.682016   ...   -0.338456   \n",
      "31971 -0.624129 -0.510835 -0.784582 -0.035865 -0.812931   ...   -0.408900   \n",
      "31972 -0.608235 -0.496268 -0.757478 -0.035865 -0.774560   ...   -0.385419   \n",
      "31973 -0.381317 -0.190361 -0.672866 -0.035865 -0.722645   ...   -0.330629   \n",
      "31974 -0.205677 -0.161227 -0.266745 -0.035865 -0.162868   ...   -0.401073   \n",
      "31975 -0.196231 -0.204928  0.046945 -0.035865  0.008676   ...    0.483391   \n",
      "31976 -0.497374 -0.365165 -0.686447 -0.035865 -0.682016   ...   -0.338456   \n",
      "31977  0.681881  0.093696  1.099792 -0.035865  0.471395   ...   -0.228877   \n",
      "31978  1.381941  1.149804  1.585900 -0.035865  0.498480   ...    5.077906   \n",
      "31979 -0.500922 -0.496268 -0.152930 -0.035865 -0.508214   ...   -0.385419   \n",
      "31980 -0.581098 -0.452567 -0.755198 -0.035865 -0.808417   ...   -0.401073   \n",
      "31981  1.216711  0.945866  1.391303 -0.035865  1.615776   ...    0.921709   \n",
      "31982 -0.602356 -0.496268 -0.749111 -0.035865 -0.756502   ...   -0.408900   \n",
      "31983 -0.360494 -0.277763 -0.444784 -0.035865 -0.368270   ...   -0.252358   \n",
      "31984  0.105717  0.086412  0.349783 -0.035865  0.893484   ...   -0.236704   \n",
      "31985 -0.567024 -0.438000 -0.752539 -0.035865 -0.785845   ...   -0.314975   \n",
      "31986 -0.497475 -0.379732 -0.662336 -0.035865 -0.722645   ...   -0.338456   \n",
      "31987 -0.351188 -0.175794 -0.577904 -0.035865 -0.578186   ...   -0.244531   \n",
      "31988 -0.626065 -0.510835 -0.790674 -0.035865 -0.812931   ...   -0.385419   \n",
      "31989  0.300287  0.421454  0.054600 -0.035865  0.019962   ...   -0.025372   \n",
      "31990 -0.507297 -0.510835 -0.415509 -0.035865 -0.472100   ...   -0.408900   \n",
      "31991 -0.026348  0.042711  0.137736 -0.035865  0.521052   ...    0.068554   \n",
      "31992 -0.620212 -0.510835 -0.778653 -0.035865 -0.815189   ...   -0.408900   \n",
      "31993  0.547382  0.100979  1.308499 -0.035865  1.412632   ...    0.311194   \n",
      "31994 -0.614223 -0.496268 -0.704525 -0.035865 -0.645901   ...   -0.401073   \n",
      "31995 -0.632510 -0.510835 -0.801100 -0.035865 -0.830989   ...   -0.408900   \n",
      "31996 -0.519381 -0.423433 -0.617855 -0.035865 -0.770045   ...   -0.322802   \n",
      "31997 -0.594163 -0.467134 -0.762462 -0.035865 -0.763274   ...   -0.377592   \n",
      "31998  0.361314  0.290350  0.423033 -0.035865  0.130563   ...    0.389465   \n",
      "31999  0.332031  0.348618  0.140468 -0.035865 -0.043239   ...   -0.142778   \n",
      "\n",
      "             c5  base_time  post_length  share_count  promotion  h_target  \\\n",
      "0      0.173523  -0.554889    -0.175801    -0.122846  -0.011181  0.121774   \n",
      "1     -0.114464   1.548326    -0.126288    -0.058854  -0.011181  0.121774   \n",
      "2      0.001531  -1.702097    -0.123537     0.111790  -0.011181  0.121774   \n",
      "3      0.005531  -1.654297    -0.450873    -0.146686  -0.011181  0.121774   \n",
      "4      0.037529  -0.793891    -0.305085    -0.142922  -0.011181  0.121774   \n",
      "5     -0.006469   1.739528    -0.450873    -0.146686  -0.011181  0.121774   \n",
      "6     -0.030468  -0.315887    -0.087777    -0.124101  -0.011181  0.121774   \n",
      "7     -0.010469   0.162116     0.253313    -0.146686  -0.011181  0.121774   \n",
      "8     -0.070466   1.404925    -0.016259    -0.127865  -0.011181  0.121774   \n",
      "9      0.197522  -1.415295    -0.093279     0.015175  -0.011181  0.121774   \n",
      "10    -2.658348   0.401118    -0.192305     0.160724  -0.011181  0.121774   \n",
      "11     0.001531  -1.176293    -0.184053    -0.146686  -0.011181  0.121774   \n",
      "12    -0.002469   1.548326     0.013999    -0.139157  -0.011181  0.121774   \n",
      "13     0.001531  -0.220286     0.291823    -0.146686  -0.011181  0.121774   \n",
      "14    -0.126463  -0.268087     0.027753     0.039015  -0.011181  0.121774   \n",
      "15     0.101526  -1.080693    -0.255572    -0.107789  -0.011181  0.121774   \n",
      "16     0.029530   1.452726    -0.195056    -0.146686  -0.011181  0.121774   \n",
      "17    -0.002469  -0.268087     1.892745    -0.121591  -0.011181  0.121774   \n",
      "18     0.001531   0.544519     0.360591    -0.146686  -0.011181  0.121774   \n",
      "19     0.041529  -1.128493    -0.203308    -0.144176  -0.011181  0.121774   \n",
      "20    -0.006469   1.357125    -0.327091    -0.145431  -0.011181  0.121774   \n",
      "21     0.017530  -1.558696     0.434861    -0.140412  -0.011181  0.121774   \n",
      "22     0.001531  -0.124686    -0.376604    -0.141667  -0.011181  0.121774   \n",
      "23    -0.002469  -0.172486    -0.376604    -0.125355  -0.011181  0.121774   \n",
      "24     0.001531  -1.032892    -0.450873    -0.114063  -0.011181  0.121774   \n",
      "25    -0.002469   1.070323    -0.450873    -0.086459  -0.011181  0.121774   \n",
      "26     0.025530  -1.319694     0.011249    -0.146686  -0.011181  0.121774   \n",
      "27     0.005531  -1.271894    -0.151044    -0.145431  -0.011181  0.121774   \n",
      "28     0.001531   0.879122    -0.206059    -0.144176  -0.011181  0.121774   \n",
      "29     0.001531   1.165924    -0.107032    -0.121591  -0.011181  0.121774   \n",
      "...         ...        ...          ...          ...        ...       ...   \n",
      "31970 -0.026468  -0.029085    -0.263824    -0.142922  -0.011181  0.121774   \n",
      "31971  0.001531  -1.510896    -0.164798    -0.146686  -0.011181  0.121774   \n",
      "31972 -0.002469   1.357125     1.991771    -0.145431  -0.011181  0.121774   \n",
      "31973  0.041529  -0.841691    -0.129038    -0.146686  -0.011181  0.121774   \n",
      "31974 -0.006469   1.404925    -0.450873    -0.145431  -0.011181  0.121774   \n",
      "31975 -0.394451   0.496719    -0.404111    -0.145431  -0.011181  0.121774   \n",
      "31976 -0.026468   0.305517     0.143284    -0.141667  -0.011181  0.121774   \n",
      "31977  0.005531  -0.459288    -0.310586    -0.146686  -0.011181  0.121774   \n",
      "31978 -0.354453   1.022523    -0.450873     6.771924  -0.011181  0.121774   \n",
      "31979 -0.006469   0.448918    -0.302334    -0.146686  -0.011181  0.121774   \n",
      "31980 -0.002469  -0.076885    -0.010757    -0.146686  -0.011181  0.121774   \n",
      "31981 -0.078465   1.500526     0.324832    -0.080185  -0.011181  0.121774   \n",
      "31982  0.001531   0.305517    -0.211560    -0.146686  -0.011181  0.121774   \n",
      "31983  0.081527  -0.793891     0.027753    -0.145431  -0.011181  0.121774   \n",
      "31984 -0.010469   1.404925    -0.393108    -0.127865  -0.011181  0.121774   \n",
      "31985 -0.026468   0.735721    -0.269325    -0.144176  -0.011181  0.121774   \n",
      "31986 -0.010469   1.357125    -0.450873    -0.146686  -0.011181  0.121774   \n",
      "31987  0.001531   1.404925    -0.230815    -0.146686  -0.011181  0.121774   \n",
      "31988 -0.006469   1.118123    -0.450873    -0.003646  -0.011181  0.121774   \n",
      "31989 -0.022468   0.974722    -0.090528    -0.072656  -0.011181  0.121774   \n",
      "31990  0.001531   0.162116    -0.450873    -0.146686  -0.011181  0.121774   \n",
      "31991 -0.214459   0.592320    -0.420615     0.547183  -0.011181  0.121774   \n",
      "31992 -0.002469   1.213724    -0.415114    -0.146686  -0.011181  0.121774   \n",
      "31993  0.369514  -1.606496    -0.346346    -0.056345  -0.011181  0.121774   \n",
      "31994  0.005531  -1.032892    -0.032763    -0.142922  -0.011181  0.121774   \n",
      "31995  0.001531  -0.268087    -0.112534    -0.146686  -0.011181  0.121774   \n",
      "31996 -0.026468  -0.315887    -0.299583    -0.145431  -0.011181  0.121774   \n",
      "31997 -0.002469   1.309325    -0.148293    -0.146686  -0.011181  0.121774   \n",
      "31998 -0.118464   0.831321    -0.266575    -0.122846  -0.011181  0.121774   \n",
      "31999 -0.010469   1.500526     0.013999    -0.086459  -0.011181  0.121774   \n",
      "\n",
      "       post_day  basetime_day  target  \n",
      "0      1.435154      0.490727       2  \n",
      "1     -0.514370      0.989925       3  \n",
      "2     -1.489131     -1.506064       3  \n",
      "3      0.947773      0.989925       1  \n",
      "4     -1.489131     -0.507668       3  \n",
      "5     -1.001750      0.490727       1  \n",
      "6     -1.001750      0.989925       3  \n",
      "7      1.435154     -1.506064       3  \n",
      "8     -1.489131     -1.006866       3  \n",
      "9     -0.514370     -0.008471       3  \n",
      "10     0.460392     -0.507668       3  \n",
      "11    -1.001750      0.989925       1  \n",
      "12     0.947773     -1.506064       1  \n",
      "13    -1.001750      1.489122       1  \n",
      "14     1.435154     -1.506064       3  \n",
      "15    -1.489131     -0.507668       3  \n",
      "16     0.460392     -0.008471       3  \n",
      "17    -0.514370     -1.006866       2  \n",
      "18    -0.514370     -1.006866       3  \n",
      "19    -1.001750      0.989925       2  \n",
      "20    -0.514370     -1.006866       1  \n",
      "21    -1.489131     -1.506064       1  \n",
      "22    -1.001750      0.989925       1  \n",
      "23    -0.514370     -1.006866       1  \n",
      "24     0.947773      1.489122       1  \n",
      "25    -0.026989      0.989925       1  \n",
      "26     0.460392     -1.506064       1  \n",
      "27     1.435154      0.490727       2  \n",
      "28     0.947773      0.490727       1  \n",
      "29    -0.026989      1.489122       1  \n",
      "...         ...           ...     ...  \n",
      "31970 -0.026989      0.989925       1  \n",
      "31971  0.947773      0.989925       1  \n",
      "31972  0.460392     -0.008471       1  \n",
      "31973 -1.001750      0.989925       2  \n",
      "31974 -0.026989      1.489122       1  \n",
      "31975 -1.001750      1.489122       2  \n",
      "31976 -0.514370     -1.006866       1  \n",
      "31977 -0.514370     -0.008471       2  \n",
      "31978  0.460392     -0.008471       3  \n",
      "31979  0.947773      0.490727       1  \n",
      "31980  1.435154     -1.506064       1  \n",
      "31981 -1.001750      0.490727       1  \n",
      "31982  0.460392     -0.507668       1  \n",
      "31983 -1.489131     -1.506064       2  \n",
      "31984  0.460392     -0.008471       1  \n",
      "31985 -1.001750      1.489122       1  \n",
      "31986 -1.489131     -1.006866       1  \n",
      "31987 -1.489131     -1.006866       1  \n",
      "31988 -0.514370     -1.006866       1  \n",
      "31989 -1.001750      0.490727       1  \n",
      "31990 -1.001750      1.489122       1  \n",
      "31991 -0.026989      0.989925       3  \n",
      "31992 -0.514370      0.989925       1  \n",
      "31993 -0.514370     -0.507668       3  \n",
      "31994  0.947773      1.489122       1  \n",
      "31995  1.435154      0.490727       1  \n",
      "31996 -1.489131     -0.008471       1  \n",
      "31997  1.435154     -0.507668       1  \n",
      "31998 -0.514370      0.989925       1  \n",
      "31999  0.947773     -1.506064       2  \n",
      "\n",
      "[32000 rows x 25 columns]\n"
     ]
    }
   ],
   "source": [
    "train_data=preprocessData(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehotencoding(y):\n",
    "    y=y.astype(int)\n",
    "#     print(y.shape)\n",
    "    max_y=max(y)\n",
    "#     max_y=3\n",
    "    encode_mat=np.zeros((y.shape[0],max_y))\n",
    "    rows=np.arange(y.shape[0])\n",
    "    #array([0, 1, 2, 3, 4])\n",
    "    encode_mat[rows,y-1]=1\n",
    "    return encode_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class nn:\n",
    "    #nn for classifier works for only one layer\n",
    "    def __init__(self, no_of_inputs, no_of_outputs, HUs,no_of_hidden_layers=1, activation=[\"sigmoid\",\"sigmoid\"], dropout=0, weights_seed=0):\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.no_of_inputs=no_of_inputs\n",
    "        self.no_of_hidden_layers=no_of_hidden_layers\n",
    "        self.no_of_outputs=no_of_outputs\n",
    "        self.HUs=HUs #hidden units array\n",
    "        self.act=activation\n",
    "        self.dropout=dropout\n",
    "        self.weights_seed=weights_seed\n",
    "        #sanity checks\n",
    "        if(len(self.HUs) != self.no_of_hidden_layers):\n",
    "            print(\"error mismatch hidden units and layers\")\n",
    "        if((self.no_of_hidden_layers+1)!=len(activation)):\n",
    "            print(\"error mismatch activations and layers\")\n",
    "        self.initialize_weights()\n",
    "    \n",
    "    def initialize_weights(self):\n",
    "        \n",
    "        np.random.seed(self.weights_seed)\n",
    "        self.weights=[]\n",
    "        weights0=0.01 * np.random.randn(self.no_of_inputs+1,self.HUs[0])\n",
    "        self.weights.append(weights0);\n",
    "        # to add multiple layers\n",
    "        for i in range(1,self.no_of_hidden_layers):\n",
    "            weightsh=0.01 * np.random.randn(self.HUs[i-1]+1,self.HUs[i])\n",
    "            self.weights.append(np.copy(weightsh));\n",
    "              \n",
    "            \n",
    "        weightsl=0.01 * np.random.randn(self.HUs[-1]+1,self.no_of_outputs)\n",
    "        self.weights.append(weightsl);\n",
    "    \n",
    "    def get_weights(self):\n",
    "        return self.weights\n",
    "    \n",
    "    def sigmoid_forward(self,z):\n",
    "        return 1/(1+np.exp(-z))\n",
    "        \n",
    "    def sigmoid_backward(self,z):\n",
    "        return z*(1-z)\n",
    "    \n",
    "    def activation_forward(self,z,act_index):\n",
    "        if(self.act[act_index]==\"sigmoid\"):\n",
    "            return self.sigmoid_forward(z)\n",
    "        elif (self.act[act_index]==\"relu\"):\n",
    "            pass\n",
    "        else:\n",
    "            print(\"error invalid activation\")\n",
    "    \n",
    "    def activation_backward(self,z,act_index):\n",
    "        if(self.act[act_index]==\"sigmoid\"):\n",
    "            return self.sigmoid_backward(z)\n",
    "        elif (self.act[act_index]==\"relu\"):\n",
    "            pass\n",
    "        else:\n",
    "            print(\"error invalid activation\")\n",
    "    \n",
    "    def feed_forward(self, input_arr):\n",
    "#         print(input_arr.shape)\n",
    "        self.inps = np.ones((input_arr.shape[0],self.no_of_inputs+1))\n",
    "        self.inps[:,1:self.no_of_inputs+1]=input_arr\n",
    "        self.layers=[]\n",
    "        self.layers.append(self.inps)\n",
    "        layer0nodes=np.matmul(self.inps,self.weights[0])\n",
    "        layer0nodes=self.activation_forward(layer0nodes,0)\n",
    "        layer0nodes_bias=np.ones((layer0nodes.shape[0],layer0nodes.shape[1]+1))\n",
    "        layer0nodes_bias[:,1:layer0nodes.shape[1]+1]=layer0nodes\n",
    "        #for all the internal layer nodes\n",
    "        self.layers.append(layer0nodes_bias)\n",
    "        for i in range(1,self.no_of_hidden_layers):\n",
    "            layerinodes=np.matmul(self.layers[-1],self.weights[i])\n",
    "            layerinodes=self.activation_forward(layerinodes,i)\n",
    "            layerinodes_bias=np.ones((layerinodes.shape[0],layerinodes.shape[1]+1))\n",
    "            layerinodes_bias[:,1:layerinodes.shape[1]+1]=layerinodes\n",
    "            self.layers.append(np.copy(layerinodes_bias))\n",
    "            \n",
    "        #for the last layer\n",
    "        layer1nodes=np.matmul(self.layers[-1],self.weights[-1])\n",
    "        layer1nodes=self.activation_forward(layer1nodes,-1)\n",
    "        self.layers.append(layer1nodes)\n",
    "#         print(len(self.layers))\n",
    "        return self.layers[-1]\n",
    "        \n",
    "    def get_layers(self):\n",
    "        return self.layers\n",
    "    \n",
    "    #can use with sigmoid\n",
    "    def compute_cross_entropy_err(self,y_hat,y):\n",
    "        #cross entropy, expects one hot encoded y label\n",
    "        loss=0\n",
    "        #ylog(y_hat)+(1-y)log(1-y_hat)\n",
    "        loss_matrix=np.multiply(y,np.log(y_hat))+np.multiply(1-y,np.log(1-y_hat))\n",
    "        loss=-1*np.sum(loss_matrix)*(1/y.shape[0])\n",
    "        return loss\n",
    "    \n",
    "    def compute_gradients(self,y_hat,y):\n",
    "        #dE/d(sigma)\n",
    "        self.gradients=[]\n",
    "        dE_dsigmaL=(-1)*(np.multiply(y,1/y_hat)-np.multiply(1-y,1/(1-y_hat)))\n",
    "#         dE_dsigmaL=np.multiply(dE_dsigmaL_init,y)-np.multiply(dE_dsigmaL_init,1-y)\n",
    "        \n",
    "        dE_dsigmaL_dsumL=np.multiply(dE_dsigmaL,self.activation_backward(self.layers[-1],-1))\n",
    "        #average\n",
    "        #gradient for the last layer\n",
    "        #update weight with this gradient\n",
    "        dE_dsigmaL_dsumL_dw=np.matmul(self.layers[-2].T,dE_dsigmaL_dsumL)*(1/self.layers[-2].shape[0])\n",
    "        self.gradients.insert(0,dE_dsigmaL_dsumL_dw)\n",
    "        \n",
    "        for i in range(self.no_of_hidden_layers,0,-1):\n",
    "            #weights without bias\n",
    "            weights_wo_bias=self.weights[i][1:,:]\n",
    "            dE_dsigmaL_dsumL_dsigmal=np.matmul(dE_dsigmaL_dsumL,weights_wo_bias.T)\n",
    "            layer_wo_bias=self.layers[i][:,1:]\n",
    "            dE_dsigmaL_dsumL=np.multiply(dE_dsigmaL_dsumL_dsigmal,self.activation_backward(layer_wo_bias,i))\n",
    "            dE_dsigmal_dsuml_dw=np.matmul(self.layers[i-1].T,dE_dsigmaL_dsumL)*(1/self.layers[i-1].shape[0])\n",
    "            self.gradients.insert(0,np.copy(dE_dsigmal_dsuml_dw))\n",
    "        \n",
    "        return self.gradients\n",
    "    \n",
    "    def update_weights(self, gradients,learning_rate=0.001):\n",
    "        self.lr=learning_rate\n",
    "        for i in range(0,len(self.weights)):\n",
    "            self.weights[i]=self.weights[i]-gradients[i]*learning_rate\n",
    "    \n",
    "    #dont use predict for now\n",
    "    def predict(self,input_arr):\n",
    "        inps = np.ones((input_arr.shape[0],self.no_of_inputs+1))\n",
    "        inps[:,1:self.no_of_inputs+1]=input_arr\n",
    "        self.layers=[]\n",
    "#         self.layers.append(self.inps)\n",
    "        layer0nodes=np.matmul(self.inps,self.weights[0])\n",
    "        layer0nodes=self.sigmoid_forward(layer0nodes)\n",
    "        layer0nodes_bias=np.ones((layer0nodes.shape[0],layer0nodes.shape[1]+1))\n",
    "        layer0nodes_bias[:,1:layer0nodes.shape[1]+1]=layer0nodes\n",
    "        #output layer nodes\n",
    "#         self.layers.append(layer0nodes_bias)\n",
    "        layer1nodes=np.matmul(layer0nodes_bias,self.weights[1])\n",
    "        layer1nodes=self.sigmoid_forward(layer1nodes)\n",
    "#         self.layers.append(layer1nodes)\n",
    "#         print(len(self.layers))\n",
    "        return layer1nodes\n",
    "        \n",
    "        \n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape\n",
    "\n",
    "network1=nn(no_of_inputs=24,no_of_outputs=3,HUs=[100,100],no_of_hidden_layers=2, activation=[\"sigmoid\",\"sigmoid\",\"sigmoid\"])\n",
    "train_data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data=train_data.values\n",
    "input_x=input_data[:,:-1]\n",
    "y_label=input_data[:,-1]\n",
    "y_encod=onehotencoding(y_label)\n",
    "# y_encod=y_encod[[0],:]\n",
    "# input_x=input_x[[0],:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_encod;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(input_x,y_encod,no_of_epochs=10000,batchSize=100):\n",
    "    print(input_x.shape)\n",
    "    print(y_encod.shape)\n",
    "    no_of_batches=int(input_x.shape[0]/batchSize)\n",
    "    for i in range(0,no_of_epochs):\n",
    "        loss=0\n",
    "        for batch_no in range(0,no_of_batches):\n",
    "            input_x_train=input_x[batchSize*(batch_no):batchSize*(batch_no+1),:]\n",
    "            y_encod_train=y_encod[batchSize*(batch_no):batchSize*(batch_no+1),:]\n",
    "            y_pred=network1.feed_forward(input_x_train)\n",
    "\n",
    "            loss=loss+network1.compute_cross_entropy_err(y_pred,y_encod_train)\n",
    "        #         print(y_pred)\n",
    "            grads=network1.compute_gradients(y_pred,y_encod_train)\n",
    "        #     if(i%1000==0):\n",
    "        #         print(grads)\n",
    "        #         print(network1.get_weights())\n",
    "            network1.update_weights(grads,learning_rate= 0.001)\n",
    "        if i%50==0:\n",
    "            loss=loss*1/no_of_batches\n",
    "            print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32000, 24)\n",
      "(32000, 3)\n",
      "1.8357780633596934\n",
      "1.7371461331593139\n",
      "1.7371023149264624\n",
      "1.7370443477917186\n",
      "1.7369614838562442\n",
      "1.7368355742530164\n",
      "1.7366324257888668\n",
      "1.7362796294433491\n",
      "1.7356005538124255\n",
      "1.7340765833646112\n",
      "1.7297331039354695\n",
      "1.711593165034893\n",
      "1.5990007134433504\n",
      "1.3866025866977612\n",
      "1.2719221658096072\n",
      "1.235753228847094\n",
      "1.2245305799454098\n",
      "1.2178301129476432\n",
      "1.2124257262243603\n",
      "1.2074226119733262\n",
      "1.2026088839523534\n",
      "1.1977369637154673\n",
      "1.1926582210704244\n",
      "1.187297566463203\n",
      "1.1816263072154016\n",
      "1.175663466649754\n",
      "1.1694803043817001\n",
      "1.1631959193291241\n",
      "1.1569556577455395\n",
      "1.1508958330009254\n",
      "1.1451145875266098\n",
      "1.139666195680904\n",
      "1.1345749043393343\n",
      "1.129849489212677\n",
      "1.1254887421215352\n",
      "1.121481914200766\n",
      "1.1178096574006615\n",
      "1.1144466923514726\n",
      "1.1113648609389801\n",
      "1.1085352799039119\n",
      "1.1059299412606758\n",
      "1.103522800049292\n",
      "1.1012897721162846\n",
      "1.0992088824730202\n",
      "1.0972607085141322\n",
      "1.0954284873702071\n",
      "1.0936979630126282\n",
      "1.0920571423820171\n",
      "1.0904960257144523\n",
      "1.0890063338097578\n",
      "1.087581241864029\n",
      "1.0862151262346287\n",
      "1.0849033307053337\n",
      "1.0836419585690202\n",
      "1.0824276951211378\n",
      "1.0812576625971695\n",
      "1.0801293069840083\n",
      "1.0790403139903477\n",
      "1.0779885500137933\n",
      "1.0769720232347386\n",
      "1.0759888599007372\n",
      "1.0750372912632664\n",
      "1.0741156473014017\n",
      "1.0732223541525374\n",
      "1.0723559329547359\n",
      "1.0715149985183112\n",
      "1.0706982568505163\n",
      "1.0699045010453072\n",
      "1.0691326054223693\n",
      "1.0683815180636185\n",
      "1.067650252061623\n",
      "1.066937875878685\n",
      "1.066243503241862\n",
      "1.0655662829977586\n",
      "1.0649053893419596\n",
      "1.0642600128222637\n",
      "1.0636293524721065\n",
      "1.063012609338826\n",
      "1.06240898152712\n",
      "1.0618176607132823\n",
      "1.0612378299864047\n",
      "1.0606686630480209\n",
      "1.0601093257384542\n",
      "1.0595589832396113\n",
      "1.0590168186564675\n",
      "1.0584820606075342\n",
      "1.0579539904202\n",
      "1.057431902895597\n",
      "1.0569150646579595\n",
      "1.0564027196304029\n",
      "1.0558941192418443\n",
      "1.0553885451542357\n",
      "1.0548853209191564\n",
      "1.054383818873568\n",
      "1.0538834661301226\n",
      "1.053383750659942\n",
      "1.0528842271898105\n",
      "1.0523845222455441\n",
      "1.0518843376585523\n",
      "1.0513834520111147\n",
      "1.050881719740286\n",
      "1.0503790678892817\n",
      "1.0498754907367223\n",
      "1.0493710427110665\n",
      "1.048865830094087\n",
      "1.0483600020374726\n",
      "1.0478537413779128\n",
      "1.047347255660849\n",
      "1.0468407686922416\n",
      "1.0463345128471295\n",
      "1.0458287222832625\n",
      "1.045323627142033\n",
      "1.0448194487679137\n",
      "1.0443163959393533\n",
      "1.0438146620754813\n",
      "1.0433144233609029\n",
      "1.0428158377140515\n",
      "1.0423190445127375\n",
      "1.0418241649839999\n",
      "1.0413313031627838\n",
      "1.0408405473219728\n",
      "1.0403519717718885\n",
      "1.0398656389199206\n",
      "1.0393816014734354\n",
      "1.0388999046663787\n",
      "1.0384205883964241\n",
      "1.0379436891774791\n",
      "1.0374692418409242\n",
      "1.0369972809542707\n",
      "1.0365278419619954\n",
      "1.0360609620834005\n",
      "1.0355966810214783\n",
      "1.0351350415423155\n",
      "1.0346760899775829\n",
      "1.034219876686498\n",
      "1.0337664564927145\n",
      "1.0333158890903815\n",
      "1.0328682393950768\n",
      "1.032423577801444\n",
      "1.031981980300849\n",
      "1.031543528409312\n",
      "1.0311083088585054\n",
      "1.0306764130104056\n",
      "1.030247935969275\n",
      "1.0298229753824293\n",
      "1.0294016299429931\n",
      "1.0289839976316641\n",
      "1.028570173758347\n",
      "1.028160248885443\n",
      "1.0277543067297488\n",
      "1.027352422147029\n",
      "1.0269546593009768\n",
      "1.0265610701066294\n",
      "1.026171693018725\n",
      "1.0257865522106404\n",
      "1.0254056571626025\n",
      "1.0250290026518014\n",
      "1.024656569114383\n",
      "1.0242883233317797\n",
      "1.0239242193819853\n",
      "1.0235641997904894\n",
      "1.023208196815002\n",
      "1.0228561338021926\n",
      "1.0225079265622807\n",
      "1.0221634847173005\n",
      "1.021822712989686\n",
      "1.0214855124084228\n",
      "1.0211517814191788\n",
      "1.020821416892358\n",
      "1.0204943150283121\n",
      "1.0201703721628355\n",
      "1.0198494854781912\n",
      "1.019531553626151\n",
      "1.0192164772698722\n",
      "1.018904159551233\n",
      "1.0185945064897015\n",
      "1.018287427317984\n",
      "1.0179828347588273\n",
      "1.0176806452463478\n",
      "1.0173807790944405\n",
      "1.0170831606142616\n",
      "1.0167877181828522\n",
      "1.016494384265716\n",
      "1.016203095397307\n",
      "1.0159137921236998\n",
      "1.0156264189109407\n",
      "1.015340924021284\n",
      "1.0150572593597587\n",
      "1.0147753802956436\n",
      "1.0144952454675022\n",
      "1.0142168165843812\n",
      "1.013940058234986\n",
      "1.013664937707598\n",
      "1.0133914248137053\n",
      "1.0131194917098365\n",
      "1.012849112721012\n",
      "1.012580264173726\n",
      "1.0123129242441429\n",
      "1.0120470728233228\n",
      "1.0117826913988943\n"
     ]
    }
   ],
   "source": [
    "train(input_x,y_encod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32000, 3)\n",
      "0.7638125\n"
     ]
    }
   ],
   "source": [
    "def trainAccuracy(input_x,y_encod):\n",
    "    y_pred=network1.feed_forward(input_x)\n",
    "    y_pred_encod=(y_pred == y_pred.max(axis=1)[:,None]).astype(int)\n",
    "    print(y_pred_encod.shape)\n",
    "#     print(np.argmax(y_pred_encod,axis=1).shape)\n",
    "    correct_pred_encod=np.multiply(y_pred_encod,y_encod)\n",
    "    accuracy=np.sum(correct_pred_encod)/input_x.shape[0]\n",
    "    print(accuracy)\n",
    "    \n",
    "trainAccuracy(input_x,y_encod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessData_test(df):\n",
    "    df['post_day'] = df['post_day'].factorize(sort=True)[0]\n",
    "    df['basetime_day'] = df['basetime_day'].factorize(sort=True)[0]\n",
    "#     df=df.head(5)\n",
    "    df_norm=(df-train_mean)/train_std\n",
    "    df_norm.fillna(0,inplace=True)\n",
    "    return df_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('./data/test.csv')\n",
    "test_data=preprocessData_test(df_test)\n",
    "input_data=test_data.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:41: RuntimeWarning: overflow encountered in exp\n"
     ]
    }
   ],
   "source": [
    "def testOutput(input_x_test):\n",
    "    y_pred=network1.feed_forward(input_x_test)\n",
    "    y_pred_arr=np.argmax(y_pred,axis=1)\n",
    "    y_pred_arr=y_pred_arr+1\n",
    "    file=open(\"submission.csv\",\"w\")\n",
    "    file.write(\"Id,predicted_class\\n\")\n",
    "    print(len(y_pred_arr))\n",
    "    for i in range(0,len(y_pred_arr)):\n",
    "        file.write(str(i+1)+\",\"+str(y_pred_arr[i])+\"\\n\")\n",
    "    file.close()\n",
    "\n",
    "testOutput(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 2)\n",
      "(5, 2)\n",
      "[[0.49765536 0.50368654 0.49381384]\n",
      " [0.49764957 0.50367189 0.49383668]\n",
      " [0.49764852 0.50367562 0.4938206 ]\n",
      " [0.49765452 0.5036914  0.49379497]\n",
      " [0.49765335 0.50368932 0.4937968 ]]\n",
      "[array([[ 0.01624345, -0.00611756, -0.00528172],\n",
      "       [-0.01072969,  0.00865408, -0.02301539],\n",
      "       [ 0.01744812, -0.00761207,  0.00319039]]), array([[-0.0024937 ,  0.01462108, -0.02060141],\n",
      "       [-0.00322417, -0.00384054,  0.01133769],\n",
      "       [-0.01099891, -0.00172428, -0.00877858],\n",
      "       [ 0.00042214,  0.00582815, -0.01100619]])]\n",
      "[array([[ 1.        , -0.60865379,  0.23565675],\n",
      "       [ 1.        ,  1.18539118,  1.39296744],\n",
      "       [ 1.        ,  0.98290729,  0.32057733],\n",
      "       [ 1.        , -0.92479789, -1.00327539],\n",
      "       [ 1.        , -0.63484679, -0.94592614]]), array([[1.        , 0.50672107, 0.49670536, 0.50236961],\n",
      "       [1.        , 0.50695686, 0.49838439, 0.4929705 ],\n",
      "       [1.        , 0.50282263, 0.49998708, 0.49328017],\n",
      "       [1.        , 0.50216523, 0.49837905, 0.50320046],\n",
      "       [1.        , 0.50163763, 0.49889722, 0.50157791]]), array([[0.49765536, 0.50368654, 0.49381384],\n",
      "       [0.49764957, 0.50367189, 0.49383668],\n",
      "       [0.49764852, 0.50367562, 0.4938206 ],\n",
      "       [0.49765452, 0.5036914 , 0.49379497],\n",
      "       [0.49765335, 0.50368932, 0.4937968 ]])]\n"
     ]
    }
   ],
   "source": [
    "print(input_x.shape)\n",
    "input_x_part=input_x\n",
    "print(input_x_part.shape)\n",
    "y_pred=network1.feed_forward(input_x_part)\n",
    "print(y_pred)\n",
    "weights=network1.get_weights()\n",
    "print(weights)\n",
    "layers=network1.get_layers()\n",
    "print(layers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.99066523 -1.98536177 -1.97555777]\n",
      " [-1.99064225 -2.01479624 -2.02496096]\n",
      " [-1.99063813 -2.01481138 -2.02502692]\n",
      " [-2.00942613 -2.01487543 -1.97548412]\n",
      " [-1.99065726 -2.01486698 -2.0251245 ]]\n"
     ]
    }
   ],
   "source": [
    "dE_dsigmaL=(-1)*(np.multiply(y_encod,1/y_pred)+np.multiply(1-y_encod,1/(1-y_pred)))\n",
    "print(dE_dsigmaL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.         -1.98536177 -0.        ]\n",
      " [-0.         -0.         -2.02496096]\n",
      " [-0.         -0.         -2.02502692]\n",
      " [-2.00942613 -0.         -0.        ]\n",
      " [-0.         -0.         -2.0251245 ]]\n",
      "[[-0.         -0.49631346 -0.        ]\n",
      " [-0.         -0.         -0.50616332]\n",
      " [-0.         -0.         -0.5061794 ]\n",
      " [-0.50234548 -0.         -0.        ]\n",
      " [-0.         -0.         -0.5062032 ]]\n",
      "[[0.2499945  0.24998641 0.24996173]\n",
      " [0.24999448 0.24998652 0.24996201]\n",
      " [0.24999447 0.24998649 0.24996181]\n",
      " [0.2499945  0.24998637 0.2499615 ]\n",
      " [0.24999449 0.24998639 0.24996152]]\n",
      "(4, 3)\n"
     ]
    }
   ],
   "source": [
    "dE_dsigmaL=np.multiply(dE_dsigmaL,y_encod)\n",
    "print(dE_dsigmaL)\n",
    "dE_dsigmaL_dsumL=np.multiply(dE_dsigmaL,network1.sigmoid_backward(layers[-1]))\n",
    "print(dE_dsigmaL_dsumL)\n",
    "print(network1.sigmoid_backward(layers[-1]))\n",
    "print(weights[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.50672107 0.49670536 0.50236961]\n",
      " [1.         0.50695686 0.49838439 0.4929705 ]\n",
      " [1.         0.50282263 0.49998708 0.49328017]\n",
      " [1.         0.50216523 0.49837905 0.50320046]\n",
      " [1.         0.50163763 0.49889722 0.50157791]]\n",
      "[[1.         1.         1.         1.         1.        ]\n",
      " [0.50672107 0.50695686 0.50282263 0.50216523 0.50163763]\n",
      " [0.49670536 0.49838439 0.49998708 0.49837905 0.49889722]\n",
      " [0.50236961 0.4929705  0.49328017 0.50320046 0.50157791]]\n",
      "[[-0.         -0.49631346 -0.        ]\n",
      " [-0.         -0.         -0.50616332]\n",
      " [-0.         -0.         -0.5061794 ]\n",
      " [-0.50234548 -0.         -0.        ]\n",
      " [-0.         -0.         -0.5062032 ]]\n",
      "[[-0.1004691  -0.09926269 -0.30370918]\n",
      " [-0.05045209 -0.0502985  -0.1530104 ]\n",
      " [-0.05007169 -0.04930431 -0.15157809]\n",
      " [-0.0505561  -0.04986656 -0.15062244]]\n",
      "(5, 3)\n"
     ]
    }
   ],
   "source": [
    "print(layers[-2])\n",
    "print(layers[-2].T)\n",
    "print(dE_dsigmaL_dsumL)\n",
    "dE_dsigmaL_dsumL_dw=np.matmul(layers[-2].T,dE_dsigmaL_dsumL)*(1/layers[-2].shape[0])\n",
    "print(dE_dsigmaL_dsumL_dw)\n",
    "#update weight with this gradient\n",
    "print(dE_dsigmaL_dsumL.shape)\n",
    "#weights without bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.         -0.49631346 -0.        ]\n",
      " [-0.         -0.         -0.49383668]\n",
      " [-0.         -0.         -0.4938206 ]\n",
      " [-0.49765452 -0.         -0.        ]\n",
      " [-0.         -0.         -0.4937968 ]]\n",
      "[[-0.00322417 -0.01099891  0.00042214]\n",
      " [-0.00384054 -0.00172428  0.00582815]\n",
      " [ 0.01133769 -0.00877858 -0.01100619]]\n",
      "[[ 0.00190611  0.00085578 -0.00289259]\n",
      " [-0.00559897  0.00433519  0.00543526]\n",
      " [-0.00559879  0.00433505  0.00543508]\n",
      " [ 0.00160452  0.00547366 -0.00021008]\n",
      " [-0.00559852  0.00433484  0.00543482]]\n",
      "(5, 3)\n",
      "(5, 4)\n"
     ]
    }
   ],
   "source": [
    "print(dE_dsigmaL_dsumL)\n",
    "weights_wo_bias=weights[1][1:,:]\n",
    "print(weights_wo_bias.T)\n",
    "dE_dsigmaL_dsumL_dsigmal=np.matmul(dE_dsigmaL_dsumL,weights_wo_bias.T)\n",
    "print(dE_dsigmaL_dsumL_dsigmal)\n",
    "print(dE_dsigmaL_dsumL_dsigmal.shape)\n",
    "print(network1.sigmoid_backward(layers[-2]).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2.01468662 -2.0021407  -1.99843484]\n",
      " [-2.0147238  -1.99790979 -2.00170064]]\n",
      "[[-0.50364489 -0.5005346  -0.4996084 ]\n",
      " [-0.50365405 -0.4994769  -0.5004248 ]]\n",
      "[[1.         0.50537419 0.49874734]\n",
      " [1.         0.50550913 0.48727871]]\n",
      "(3, 3)\n",
      "[[-0.50364947 -0.50000575 -0.5000166 ]\n",
      " [-0.25456542 -0.2527237  -0.25272925]\n",
      " [-0.24830572 -0.24651238 -0.24651236]]\n",
      "(2, 3)\n",
      "[[ 0.00423022 -0.00211824]\n",
      " [ 0.00426253 -0.00213153]]\n",
      "(2, 2)\n",
      "(2, 3)\n",
      "[[1.         0.50537419 0.49874734]\n",
      " [1.         0.50550913 0.48727871]]\n",
      "[[ 0.00105743 -0.00052956]\n",
      " [ 0.0010655  -0.00053254]]\n",
      "(2, 3)\n",
      "(2, 2)\n",
      "[[ 0.00106147 -0.00053105]\n",
      " [ 0.00030971 -0.00015447]\n",
      " [ 0.0008667  -0.0004333 ]]\n",
      "[[ 0.01624345 -0.00611756]\n",
      " [-0.00528172 -0.01072969]\n",
      " [ 0.00865408 -0.02301539]]\n"
     ]
    }
   ],
   "source": [
    "dE_dsigmaL=(-1)*(np.multiply(y_encod,1/y_pred)+np.multiply(1-y_encod,1/(1-y_pred)))\n",
    "print(dE_dsigmaL)\n",
    "\n",
    "dE_dsigmaL_dsumL=np.multiply(dE_dsigmaL,network1.sigmoid_backward(layers[-1]))\n",
    "print(dE_dsigmaL_dsumL)\n",
    "print(layers[1])\n",
    "print(weights[1].shape)\n",
    "#average\n",
    "dE_dsigmaL_dsumL_dw=np.matmul(layers[-2].T,dE_dsigmaL_dsumL)*(1/layers[-2].shape[0])\n",
    "print(dE_dsigmaL_dsumL_dw)\n",
    "#update weight with this gradient\n",
    "print(dE_dsigmaL_dsumL.shape)\n",
    "#weights without bias\n",
    "weights_wo_bias=weights[1][1:,:]\n",
    "dE_dsigmaL_dsumL_dsigmal=np.matmul(dE_dsigmaL_dsumL,weights_wo_bias.T)\n",
    "print(dE_dsigmaL_dsumL_dsigmal)\n",
    "print(dE_dsigmaL_dsumL_dsigmal.shape)\n",
    "print(network1.sigmoid_backward(layers[-2]).shape)\n",
    "#layer without bias\n",
    "print(layers[-2])\n",
    "layer_wo_bias=layers[-2][:,1:]\n",
    "dE_dsigmal_dsuml=np.multiply(dE_dsigmaL_dsumL_dsigmal,network1.sigmoid_backward(layer_wo_bias))\n",
    "print(dE_dsigmal_dsuml)\n",
    "print(layers[0].shape)\n",
    "print(dE_dsigmal_dsuml.shape)\n",
    "dE_dsigmal_dsuml_dw=np.matmul(layers[0].T,dE_dsigmal_dsuml)*(1/layers[0].shape[0])\n",
    "print(dE_dsigmal_dsuml_dw)\n",
    "print(weights[0])\n",
    "# dE_dsigmaL_dsumL_dw_mean=np.mean(dE_dsigmaL_dsumL_dw,axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.73105858 0.73105858 0.73105858 0.73105858]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "range(0, 10)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "range(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 3., 3., 1., 3.])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "9\n",
      "8\n",
      "7\n",
      "6\n",
      "5\n",
      "4\n",
      "3\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "for i in range(10,1,-1):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def a(b=[2,3]):\n",
    "    c=b[0]\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "a()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

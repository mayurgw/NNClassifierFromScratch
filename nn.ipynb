{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   page_likes  page_checkin  daily_crowd  page_category   F1          F2  \\\n",
      "0      400487             0        57833             13  414   26.635492   \n",
      "1     2667410           141       111855             18  793  185.317073   \n",
      "2     2411555             0        61797              9  560  103.860465   \n",
      "3        1013             0            1             85   16    2.224299   \n",
      "4      367390             0         2678            100  110   11.828175   \n",
      "\n",
      "      F3          F4  F5   F6   ...     c4  c5  base_time  post_length  \\\n",
      "0   15.0   44.941598   0  300   ...     43  43         24          100   \n",
      "1  157.0  164.739179   0  419   ...    216 -29         68          118   \n",
      "2   78.0  107.011971   0  325   ...      0   0          0          119   \n",
      "3    1.0    3.489124   0    7   ...      1   1          1            0   \n",
      "4    6.0   15.430200   0   96   ...      9   9         19           53   \n",
      "\n",
      "   share_count  promotion  h_target   post_day  basetime_day  target  \n",
      "0           20          0        24  wednesday      thursday       2  \n",
      "1           71          0        24   saturday       tuesday       3  \n",
      "2          207          0        24     friday        friday       3  \n",
      "3            1          0        24    tuesday       tuesday       1  \n",
      "4            4          0        24     friday      saturday       3  \n",
      "\n",
      "[5 rows x 25 columns]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "32000"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('./../data/train.csv')\n",
    "train_mean=0\n",
    "train_std=0\n",
    "print(df.head())\n",
    "len(df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessData(df):\n",
    "    df['post_day'] = df['post_day'].factorize(sort=True)[0]\n",
    "    df['basetime_day'] = df['basetime_day'].factorize(sort=True)[0]\n",
    "    df = df.sample(frac=1).reset_index(drop=True)\n",
    "    df=df.head(2000)\n",
    "#     print(df)\n",
    "    df_norm=df.drop(labels='target', axis=1)\n",
    "    print(df_norm.mean())\n",
    "    print(df_norm.std())\n",
    "    df_norm=(df_norm-df_norm.mean())/df_norm.std()\n",
    "    global train_mean\n",
    "    global train_std\n",
    "    train_mean=df_norm.mean()\n",
    "    train_std=df_norm.std()\n",
    "#     df_norm=(df_norm-df_norm.min())/(df_norm.max()-df_norm.min())\n",
    "    df_norm.fillna(0,inplace=True)\n",
    "    print(df_norm.head(5))\n",
    "    df_norm=pd.concat([df_norm, df['target']], axis=1)\n",
    "#     print(df_norm)\n",
    "#     df_norm=df_norm[['page_likes','daily_crowd','target']]\n",
    "    print(df_norm)\n",
    "    return df_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_likes       1.247298e+06\n",
      "page_checkin     4.251725e+03\n",
      "daily_crowd      4.555683e+04\n",
      "page_category    2.455600e+01\n",
      "F1               4.850150e+02\n",
      "F2               5.821294e+01\n",
      "F3               3.745175e+01\n",
      "F4               6.952764e+01\n",
      "F5               9.750000e-02\n",
      "F6               3.822115e+02\n",
      "F7               2.264343e+01\n",
      "F8               7.439000e+00\n",
      "c1               5.694600e+01\n",
      "c2               2.104650e+01\n",
      "c3               2.169600e+01\n",
      "c4               5.372400e+01\n",
      "c5              -3.424500e+00\n",
      "base_time        3.547550e+01\n",
      "post_length      1.553295e+02\n",
      "share_count      1.209355e+02\n",
      "promotion        5.000000e-04\n",
      "h_target         2.371600e+01\n",
      "post_day         3.040500e+00\n",
      "basetime_day     3.044500e+00\n",
      "dtype: float64\n",
      "page_likes       2.744024e+06\n",
      "page_checkin     1.865781e+04\n",
      "daily_crowd      1.039190e+05\n",
      "page_category    2.005437e+01\n",
      "F1               5.272393e+02\n",
      "F2               9.161446e+01\n",
      "F3               7.486694e+01\n",
      "F4               8.332650e+01\n",
      "F5               2.002248e+00\n",
      "F6               4.420441e+02\n",
      "F7               3.703450e+01\n",
      "F8               1.851575e+01\n",
      "c1               1.344561e+02\n",
      "c2               6.517950e+01\n",
      "c3               7.482506e+01\n",
      "c4               1.245912e+02\n",
      "c5               1.516926e+02\n",
      "base_time        2.074776e+01\n",
      "post_length      2.447240e+02\n",
      "share_count      4.943574e+02\n",
      "promotion        2.236068e-02\n",
      "h_target         2.026178e+00\n",
      "post_day         2.053039e+00\n",
      "basetime_day     2.010358e+00\n",
      "dtype: float64\n",
      "   page_likes  page_checkin  daily_crowd  page_category        F1        F2  \\\n",
      "0   -0.371458     -0.227879    -0.438157      -0.426640 -0.343326 -0.494271   \n",
      "1   -0.397967     -0.159061    -0.414774       1.767395 -0.889568 -0.620308   \n",
      "2   -0.091247     -0.227879     0.390354      -0.027725  1.771463  1.116231   \n",
      "3   -0.394419     -0.227879    -0.194910       0.570649  0.290162  0.071118   \n",
      "4   -0.210809     -0.227665    -0.029194      -0.775691  1.714563  0.563615   \n",
      "\n",
      "         F3        F4        F5        F6      ...             c3        c4  \\\n",
      "0 -0.473530 -0.373598 -0.048695 -0.195029      ...      -0.289956 -0.399097   \n",
      "1 -0.500244 -0.806779 -0.048695 -0.835237      ...      -0.276592 -0.431202   \n",
      "2  0.855494  1.574796 -0.048695  1.712020      ...      -0.116218  0.981418   \n",
      "3 -0.072819  0.199606 -0.048695  0.282299      ...       0.658924  0.178793   \n",
      "4  0.628425  0.589354 -0.048695  2.254953      ...       0.725746  0.291160   \n",
      "\n",
      "         c5  base_time  post_length  share_count  promotion  h_target  \\\n",
      "0  0.022575   0.941041     0.815901    -0.238563  -0.022361  0.140165   \n",
      "1  0.015983   1.471219    -0.467177    -0.135399  -0.022361  0.140165   \n",
      "2 -0.036755   1.760407     0.775039    -0.216312  -0.022361  0.140165   \n",
      "3 -0.372962   0.362666    -0.544816    -0.088874  -0.022361  0.140165   \n",
      "4 -0.346592  -0.263908    -0.377280     1.092458  -0.022361  0.140165   \n",
      "\n",
      "   post_day  basetime_day  \n",
      "0 -0.993893      1.470136  \n",
      "1 -0.993893      0.475288  \n",
      "2  0.954439     -1.514407  \n",
      "3  1.441522     -1.514407  \n",
      "4  0.954439      1.470136  \n",
      "\n",
      "[5 rows x 24 columns]\n",
      "      page_likes  page_checkin  daily_crowd  page_category        F1  \\\n",
      "0      -0.371458     -0.227879    -0.438157      -0.426640 -0.343326   \n",
      "1      -0.397967     -0.159061    -0.414774       1.767395 -0.889568   \n",
      "2      -0.091247     -0.227879     0.390354      -0.027725  1.771463   \n",
      "3      -0.394419     -0.227879    -0.194910       0.570649  0.290162   \n",
      "4      -0.210809     -0.227665    -0.029194      -0.775691  1.714563   \n",
      "5      -0.454250     -0.227879    -0.438186      -0.326911 -0.906638   \n",
      "6      -0.449543     -0.227879    -0.437753      -0.376776 -0.887671   \n",
      "7      -0.306576     -0.227879    -0.132631       0.869835  0.508280   \n",
      "8      -0.305046     -0.227879    -0.174413      -0.027725  0.001868   \n",
      "9       0.639539     -0.227879     0.019344      -1.074878  0.586043   \n",
      "10     -0.433454     -0.227879    -0.334114      -0.326911  0.561386   \n",
      "11      1.336172     -0.224663     1.089995      -0.775691  1.887160   \n",
      "12     -0.299394     -0.110127    -0.333133       0.321326  1.621626   \n",
      "13     -0.086663      9.189840     0.018728      -0.775691 -0.199179   \n",
      "14     -0.453119     -0.186020    -0.438311       4.061159 -0.910431   \n",
      "15     -0.449543     -0.227879    -0.437753      -0.376776 -0.887671   \n",
      "16     -0.446291     -0.227879    -0.288223       3.363057 -0.453333   \n",
      "17     -0.445810     -0.218339    -0.411704       0.570649 -0.569030   \n",
      "18     -0.453047      0.276575    -0.435934       0.121869 -0.899051   \n",
      "19     -0.448306     -0.227879    -0.438080      -0.576233 -0.912328   \n",
      "20     -0.449316     -0.227879    -0.431893       2.166310 -0.887671   \n",
      "21     -0.451619     -0.217267    -0.437387       3.811837 -0.912328   \n",
      "22      0.067864      1.163013     0.538007      -0.775691  0.946411   \n",
      "23     -0.376971     -0.225253    -0.438147      -0.426640 -0.248492   \n",
      "24     -0.324793     -0.227879    -0.169053      -0.825556 -0.091069   \n",
      "25     -0.424756     -0.227879    -0.410029       0.570649 -0.525407   \n",
      "26      0.092780     -0.227879    -0.113596      -0.526369 -0.601273   \n",
      "27     -0.414774     -0.227879    -0.303273      -0.027725 -0.142279   \n",
      "28     -0.013627     -0.224556     0.587806      -0.027725  1.314365   \n",
      "29     -0.452554     -0.227879    -0.438378      -0.027725 -0.891464   \n",
      "...          ...           ...          ...            ...       ...   \n",
      "1970   -0.267541     -0.227879     0.268769       0.869835  1.826467   \n",
      "1971   -0.347806     -0.227879    -0.197113      -0.027725 -0.527303   \n",
      "1972   -0.434450     -0.172728    -0.412146       0.520784 -0.847841   \n",
      "1973   -0.346060     -0.219947    -0.433692       1.518073 -0.608860   \n",
      "1974    0.172593     -0.166136    -0.341235       1.069293 -0.449540   \n",
      "1975   -0.425118     -0.227879    -0.352042      -0.576233 -0.371776   \n",
      "1976   -0.416637     -0.227879    -0.438099      -0.526369 -0.709384   \n",
      "1977   -0.452041     -0.227879    -0.438196      -0.277047 -0.895258   \n",
      "1978   -0.454130     -0.227879    -0.438224      -0.127453 -0.885774   \n",
      "1979   -0.454130     -0.227879    -0.438378       3.313192 -0.891464   \n",
      "1980   -0.442617     -0.144322    -0.430978       0.371191 -0.891464   \n",
      "1981    0.291123     -0.227879     0.751818      -0.775691  1.756290   \n",
      "1982   -0.422170     -0.227879    -0.407893      -1.124742 -0.864911   \n",
      "1983    1.721422      2.617900     1.818188      -0.775691  1.574209   \n",
      "1984   -0.362223     -0.227879    -0.420220       0.969564 -0.789044   \n",
      "1985   -0.302603     -0.227879     0.112618      -0.326911  1.524896   \n",
      "1986   -0.366167     -0.210567    -0.431642      -1.025013 -0.688520   \n",
      "1987   -0.073683     -0.227879    -0.103820      -0.426640  1.117111   \n",
      "1988   -0.243333     -0.227879    -0.119717      -0.326911  0.081528   \n",
      "1989   -0.013627     -0.224556     0.587806      -0.027725  1.314365   \n",
      "1990   -0.134005     -0.227290     0.038330      -0.775691  1.160735   \n",
      "1991   -0.354622     -0.227879     0.528086      -0.326911  0.339476   \n",
      "1992   -0.441164     -0.227879    -0.438350      -0.526369 -0.889568   \n",
      "1993    0.119977     -0.227879    -0.113202      -0.526369  1.439546   \n",
      "1994    0.639473     -0.227879     0.304883      -0.775691  0.656220   \n",
      "1995   -0.453236     -0.227879    -0.434346      -0.326911 -0.724557   \n",
      "1996   -0.024004     -0.227879    -0.416188      -0.376776 -0.842151   \n",
      "1997    0.119977     -0.227879    -0.113202      -0.526369  1.439546   \n",
      "1998    2.970870     -0.227879    -0.331670      -0.526369  1.143665   \n",
      "1999   -0.337952     -0.215820    -0.437897      -0.077589 -0.844047   \n",
      "\n",
      "            F2        F3        F4        F5        F6   ...          c4  \\\n",
      "0    -0.494271 -0.473530 -0.373598 -0.048695 -0.195029   ...   -0.399097   \n",
      "1    -0.620308 -0.500244 -0.806779 -0.048695 -0.835237   ...   -0.431202   \n",
      "2     1.116231  0.855494  1.574796 -0.048695  1.712020   ...    0.981418   \n",
      "3     0.071118 -0.072819  0.199606 -0.048695  0.282299   ...    0.178793   \n",
      "4     0.563615  0.628425  0.589354 -0.048695  2.254953   ...    0.291160   \n",
      "5    -0.618854 -0.486887 -0.812474 -0.048695 -0.853335   ...   -0.431202   \n",
      "6    -0.623336 -0.500244 -0.809295 -0.048695 -0.848810   ...   -0.431202   \n",
      "7     0.959072  0.835459  0.981901 -0.048695  0.544716   ...    0.090504   \n",
      "8     0.208209  0.314535  0.143288 -0.048695  0.175975   ...    0.082478   \n",
      "9    -0.376394 -0.406745 -0.075900 -0.048695  0.562814   ...   -0.399097   \n",
      "10   -0.336226 -0.273175 -0.213682 -0.048695 -0.027625   ...   -0.423176   \n",
      "11    1.114091  1.062528  1.305052 -0.048695  1.958602   ...    1.254311   \n",
      "12    0.777630  0.214357  1.722462 -0.048695  2.053615   ...   -0.350940   \n",
      "13   -0.062271 -0.166318  0.017320 -0.048695 -0.025363   ...    2.546537   \n",
      "14   -0.634793 -0.500244 -0.828667 -0.048695 -0.853335   ...   -0.431202   \n",
      "15   -0.623336 -0.500244 -0.809295 -0.048695 -0.848810   ...   -0.431202   \n",
      "16   -0.112961 -0.006034 -0.320582 -0.048695 -0.308140   ...    1.198127   \n",
      "17   -0.447001 -0.353317 -0.567876 -0.048695 -0.629375   ...   -0.358966   \n",
      "18   -0.616868 -0.486887 -0.804545 -0.048695 -0.839761   ...   -0.342914   \n",
      "19   -0.633168 -0.500244 -0.826750 -0.048695 -0.860121   ...   -0.431202   \n",
      "20   -0.622504 -0.500244 -0.808831 -0.048695 -0.826188   ...   -0.431202   \n",
      "21   -0.623345 -0.486887 -0.819675 -0.048695 -0.855597   ...   -0.415150   \n",
      "22    0.977523  0.848816  1.121230 -0.048695  0.703071   ...   -0.294756   \n",
      "23   -0.076277 -0.179675 -0.161624 -0.048695 -0.231225   ...   -0.366992   \n",
      "24   -0.010802  0.060751 -0.167364 -0.048695 -0.262896   ...   -0.198441   \n",
      "25   -0.424545 -0.353317 -0.519308 -0.048695 -0.471020   ...   -0.294756   \n",
      "26   -0.574370 -0.460173 -0.647080 -0.048695 -0.570557   ...   -0.254625   \n",
      "27    0.534661  0.488176  0.368299 -0.048695  0.004046   ...    0.443659   \n",
      "28    2.614888  2.812300  1.918851 -0.048695  0.664614   ...    5.885458   \n",
      "29   -0.608361 -0.500244 -0.782495 -0.048695 -0.832975   ...   -0.431202   \n",
      "...        ...       ...       ...       ...       ...   ...         ...   \n",
      "1970  0.714110 -0.092855  1.885814 -0.048695  1.714283   ...   -0.302782   \n",
      "1971 -0.186203 -0.219747 -0.280120 -0.048695 -0.541148   ...    0.804840   \n",
      "1972 -0.616577 -0.486887 -0.785386 -0.048695 -0.794517   ...   -0.431202   \n",
      "1973 -0.572154 -0.460173 -0.687458 -0.048695 -0.559246   ...   -0.358966   \n",
      "1974 -0.459988 -0.406745 -0.468815 -0.048695 -0.342073   ...   -0.431202   \n",
      "1975 -0.412960 -0.339960 -0.385637 -0.048695 -0.253847   ...   -0.270677   \n",
      "1976 -0.558961 -0.446816 -0.702719 -0.048695 -0.627113   ...   -0.391071   \n",
      "1977 -0.621384 -0.500244 -0.809570 -0.048695 -0.835237   ...   -0.431202   \n",
      "1978 -0.614137 -0.486887 -0.799684 -0.048695 -0.837499   ...   -0.431202   \n",
      "1979 -0.604496 -0.473530 -0.789007 -0.048695 -0.832975   ...   -0.391071   \n",
      "1980 -0.620765 -0.500244 -0.807126 -0.048695 -0.830712   ...   -0.423176   \n",
      "1981  0.708079  0.474819  1.130636 -0.048695  1.569501   ...    1.326547   \n",
      "1982 -0.564828 -0.433459 -0.769178 -0.048695 -0.808090   ...   -0.286730   \n",
      "1983  0.826618  0.140893  1.920420 -0.048695  1.381737   ...   -0.278704   \n",
      "1984 -0.581350 -0.473530 -0.741499 -0.048695 -0.728913   ...   -0.423176   \n",
      "1985  0.364269  0.221035  0.582481 -0.048695  0.619369   ...    0.034320   \n",
      "1986 -0.588141 -0.473530 -0.726606 -0.048695 -0.633899   ...   -0.431202   \n",
      "1987 -0.342165 -0.460173  0.571393 -0.048695  1.189448   ...   -0.366992   \n",
      "1988  0.147782  0.127536  0.055861 -0.048695  0.277774   ...   -0.029890   \n",
      "1989  2.614888  2.812300  1.918851 -0.048695  0.664614   ...    1.029575   \n",
      "1990  0.482371  0.060751  1.263005 -0.048695  1.395310   ...   -0.045942   \n",
      "1991  0.790010  0.554961  0.738684 -0.048695  0.273250   ...    0.122609   \n",
      "1992 -0.615699 -0.486887 -0.794783 -0.048695 -0.844286   ...   -0.431202   \n",
      "1993  1.166244  0.755317  1.680568 -0.048695  1.241479   ...    0.066425   \n",
      "1994  0.143034 -0.019391  0.367601 -0.048695  0.354690   ...   -0.166336   \n",
      "1995 -0.614778 -0.500244 -0.742251 -0.048695 -0.844286   ...   -0.366992   \n",
      "1996 -0.590330 -0.460173 -0.778962 -0.048695 -0.785468   ...   -0.415150   \n",
      "1997  1.166244  0.755317  1.680568 -0.048695  1.241479   ...   -0.134231   \n",
      "1998  3.505431  3.500187  2.431884 -0.048695  0.933817   ...    3.726396   \n",
      "1999 -0.564417 -0.446816 -0.721126 -0.048695 -0.778681   ...   -0.423176   \n",
      "\n",
      "            c5  base_time  post_length  share_count  promotion  h_target  \\\n",
      "0     0.022575   0.941041     0.815901    -0.238563  -0.022361  0.140165   \n",
      "1     0.015983   1.471219    -0.467177    -0.135399  -0.022361  0.140165   \n",
      "2    -0.036755   1.760407     0.775039    -0.216312  -0.022361  0.140165   \n",
      "3    -0.372962   0.362666    -0.544816    -0.088874  -0.022361  0.140165   \n",
      "4    -0.346592  -0.263908    -0.377280     1.092458  -0.022361  0.140165   \n",
      "5     0.022575  -1.709847     0.779125    -0.242609  -0.022361  0.140165   \n",
      "6     0.022575  -0.553096     1.269473    -0.240586  -0.022361  0.140165   \n",
      "7     0.451074  -0.986878    -0.459005    -0.183947  -0.022361  0.140165   \n",
      "8    -0.214747   0.266270    -0.132106    -0.220358  -0.022361  0.140165   \n",
      "9     0.015983   1.085635    -0.213831    -0.242609  -0.022361  0.140165   \n",
      "10    0.015983   0.507260    -0.487609    -0.236540  -0.022361  0.140165   \n",
      "11   -0.030163   1.133833     0.039516     0.240847  -0.022361  0.140165   \n",
      "12    0.002798   0.892843    -0.409970    -0.194061  -0.022361  0.140165   \n",
      "13    2.468312  -0.601294    -0.320073     2.435615  -0.022361  0.140165   \n",
      "14    0.022575  -0.215710    -0.479436    -0.240586  -0.022361  0.140165   \n",
      "15    0.022575  -1.709847    -0.471264    -0.242609  -0.022361  0.140165   \n",
      "16    1.360808  -0.649492    -0.205658    -0.208221  -0.022361  0.140165   \n",
      "17    0.081906  -1.227867     0.766866    -0.236540  -0.022361  0.140165   \n",
      "18    0.095090  -0.697690     0.873108    -0.242609  -0.022361  0.140165   \n",
      "19    0.022575  -0.071116    -0.250607    -0.238563  -0.022361  0.140165   \n",
      "20    0.022575  -0.408502     0.162103    -0.240586  -0.022361  0.140165   \n",
      "21    0.035760  -0.553096     0.644279    -0.238563  -0.022361  0.140165   \n",
      "22    0.002798   1.230229    -0.148451    -0.137422  -0.022361  0.140165   \n",
      "23    0.075314  -1.131471    -0.495781    -0.240586  -0.022361  0.140165   \n",
      "24    0.029168   1.230229    -0.397711    -0.242609  -0.022361  0.140165   \n",
      "25    0.002798   1.085635    -0.634713    -0.129331  -0.022361  0.140165   \n",
      "26   -0.056532  -0.071116    -0.262866    -0.139445  -0.022361  0.140165   \n",
      "27   -0.583915   0.410864     1.580027    -0.242609  -0.022361  0.140165   \n",
      "28   -4.486545   0.555457     0.407277     0.224664  -0.022361  0.140165   \n",
      "29    0.022575   1.567615    -0.242434    -0.242609  -0.022361  0.140165   \n",
      "...        ...        ...          ...          ...        ...       ...   \n",
      "1970 -0.016978   1.133833    -0.381366     0.123523  -0.022361  0.140165   \n",
      "1971 -0.214747   1.133833    -0.409970    -0.198107  -0.022361  0.140165   \n",
      "1972  0.022575   1.760407    -0.585678    -0.242609  -0.022361  0.140165   \n",
      "1973  0.002798  -0.456700    -0.606109    -0.238563  -0.022361  0.140165   \n",
      "1974  0.022575  -1.613451     0.023171    -0.242609  -0.022361  0.140165   \n",
      "1975  0.022575   1.471219    -0.634713    -0.240586  -0.022361  0.140165   \n",
      "1976  0.055537  -1.468857    -0.401798    -0.206198  -0.022361  0.140165   \n",
      "1977  0.022575   1.712209     9.143651    -0.242609  -0.022361  0.140165   \n",
      "1978  0.022575  -1.613451    -0.536643    -0.242609  -0.022361  0.140165   \n",
      "1979  0.042352  -0.456700    -0.258779    -0.242609  -0.022361  0.140165   \n",
      "1980  0.029168  -0.601294     0.811815    -0.236540  -0.022361  0.140165   \n",
      "1981 -0.287262   1.374823    -0.508040     0.795102  -0.022361  0.140165   \n",
      "1982  0.141236  -0.601294    -0.557075    -0.220358  -0.022361  0.140165   \n",
      "1983 -0.089494   0.410864     1.698528    -0.192038  -0.022361  0.140165   \n",
      "1984  0.015983   0.025280    -0.087157    -0.242609  -0.022361  0.140165   \n",
      "1985  0.404928  -0.842284    -0.381366    -0.236540  -0.022361  0.140165   \n",
      "1986  0.022575  -1.565253     0.039516    -0.242609  -0.022361  0.140165   \n",
      "1987  0.015983   0.941041    -0.634713     0.252984  -0.022361  0.140165   \n",
      "1988 -0.162009  -0.263908    -0.132106    -0.236540  -0.022361  0.140165   \n",
      "1989 -0.122455  -0.360304     0.252000    -0.139445  -0.022361  0.140165   \n",
      "1990 -0.280670   0.218072    -0.336418    -0.171810  -0.022361  0.140165   \n",
      "1991  0.477443  -0.649492    -0.577506    -0.236540  -0.022361  0.140165   \n",
      "1992  0.022575   1.326625     0.014999    -0.240586  -0.022361  0.140165   \n",
      "1993 -0.043348  -0.263908    -0.401798     0.089135  -0.022361  0.140165   \n",
      "1994 -0.194970   0.314468    -0.332332    -0.232495  -0.022361  0.140165   \n",
      "1995  0.022575   1.760407    -0.528471    -0.242609  -0.022361  0.140165   \n",
      "1996  0.009391   0.459062    -0.087157    -0.240586  -0.022361  0.140165   \n",
      "1997  0.022575   1.664011    -0.185227    -0.096965  -0.022361  0.140165   \n",
      "1998 -2.053993  -0.119314    -0.132106     6.543979  -0.022361  0.140165   \n",
      "1999  0.015983  -0.263908    -0.279211    -0.242609  -0.022361  0.140165   \n",
      "\n",
      "      post_day  basetime_day  target  \n",
      "0    -0.993893      1.470136       1  \n",
      "1    -0.993893      0.475288       1  \n",
      "2     0.954439     -1.514407       1  \n",
      "3     1.441522     -1.514407       3  \n",
      "4     0.954439      1.470136       3  \n",
      "5    -0.506810     -0.519559       1  \n",
      "6     0.467356     -1.514407       1  \n",
      "7     0.954439      1.470136       3  \n",
      "8     1.441522     -1.514407       3  \n",
      "9     1.441522     -0.519559       2  \n",
      "10   -0.019727      0.972712       1  \n",
      "11   -0.993893      0.475288       1  \n",
      "12    1.441522     -1.514407       1  \n",
      "13    0.467356     -1.514407       3  \n",
      "14    0.954439      1.470136       1  \n",
      "15   -1.480976     -1.514407       2  \n",
      "16   -1.480976     -0.519559       3  \n",
      "17   -1.480976     -0.519559       3  \n",
      "18   -1.480976     -0.519559       1  \n",
      "19    0.467356     -0.519559       2  \n",
      "20   -0.993893      0.972712       1  \n",
      "21    1.441522      0.475288       1  \n",
      "22    0.467356     -0.022135       1  \n",
      "23   -0.506810     -0.022135       3  \n",
      "24   -0.506810      0.972712       1  \n",
      "25   -1.480976     -1.016983       2  \n",
      "26   -1.480976     -0.519559       2  \n",
      "27   -0.019727      0.972712       2  \n",
      "28   -0.993893      1.470136       3  \n",
      "29   -1.480976     -1.016983       1  \n",
      "...        ...           ...     ...  \n",
      "1970  0.467356     -0.519559       1  \n",
      "1971  0.954439     -1.514407       1  \n",
      "1972 -0.019727      1.470136       1  \n",
      "1973  0.467356     -1.514407       1  \n",
      "1974  1.441522      1.470136       1  \n",
      "1975  0.467356     -0.022135       1  \n",
      "1976  0.954439      0.972712       2  \n",
      "1977 -0.019727      1.470136       1  \n",
      "1978  1.441522      1.470136       1  \n",
      "1979 -0.506810     -0.022135       1  \n",
      "1980 -0.993893      0.972712       1  \n",
      "1981 -0.993893      0.475288       2  \n",
      "1982 -0.019727     -1.016983       3  \n",
      "1983 -1.480976     -0.022135       2  \n",
      "1984  0.954439      0.475288       1  \n",
      "1985  0.954439      1.470136       3  \n",
      "1986 -0.993893     -1.016983       1  \n",
      "1987 -0.506810      0.972712       1  \n",
      "1988  0.467356     -1.514407       2  \n",
      "1989 -0.993893      0.972712       3  \n",
      "1990 -0.019727      0.972712       1  \n",
      "1991 -0.019727     -1.016983       3  \n",
      "1992  0.467356     -0.022135       1  \n",
      "1993  1.441522     -1.514407       2  \n",
      "1994 -0.993893      1.470136       1  \n",
      "1995 -0.506810      0.972712       1  \n",
      "1996  0.954439      0.475288       1  \n",
      "1997 -1.480976     -1.016983       2  \n",
      "1998  0.954439      1.470136       3  \n",
      "1999 -1.480976     -0.519559       1  \n",
      "\n",
      "[2000 rows x 25 columns]\n"
     ]
    }
   ],
   "source": [
    "val_percent=0.1\n",
    "\n",
    "train_data=preprocessData(df)\n",
    "val_size=int(val_percent*len(train_data.index))\n",
    "val_data=train_data.tail(val_size)\n",
    "train_data=train_data.head(len(train_data.index)-val_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehotencoding(y):\n",
    "    y=y.astype(int)\n",
    "#     print(y.shape)\n",
    "    max_y=max(y)\n",
    "#     max_y=3\n",
    "    encode_mat=np.zeros((y.shape[0],max_y))\n",
    "    rows=np.arange(y.shape[0])\n",
    "    #array([0, 1, 2, 3, 4])\n",
    "    encode_mat[rows,y-1]=1\n",
    "    return encode_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class nn:\n",
    "    #nn for classifier works for only one layer\n",
    "    def __init__(self, no_of_inputs, no_of_outputs, HUs,no_of_hidden_layers=1, activation=[\"sigmoid\",\"sigmoid\"],regu=0, dropout=0, weights_seed=0):\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.no_of_inputs=no_of_inputs\n",
    "        self.no_of_hidden_layers=no_of_hidden_layers\n",
    "        self.no_of_outputs=no_of_outputs\n",
    "        self.HUs=HUs #hidden units array\n",
    "        self.act=activation\n",
    "        self.dropout=dropout\n",
    "        self.weights_seed=weights_seed\n",
    "        self.regu=regu\n",
    "        #sanity checks\n",
    "        if(len(self.HUs) != self.no_of_hidden_layers):\n",
    "            print(\"error mismatch hidden units and layers\")\n",
    "        if((self.no_of_hidden_layers+1)!=len(activation)):\n",
    "            print(\"error mismatch activations and layers\")\n",
    "        self.initialize_weights()\n",
    "    \n",
    "    def initialize_weights(self):\n",
    "        \n",
    "        np.random.seed(self.weights_seed)\n",
    "        self.weights=[]\n",
    "        weights0=0.01 * np.random.randn(self.no_of_inputs+1,self.HUs[0])\n",
    "        self.weights.append(weights0);\n",
    "        # to add multiple layers\n",
    "        for i in range(1,self.no_of_hidden_layers):\n",
    "            weightsh=0.01 * np.random.randn(self.HUs[i-1]+1,self.HUs[i])\n",
    "            self.weights.append(np.copy(weightsh));\n",
    "              \n",
    "            \n",
    "        weightsl=0.01 * np.random.randn(self.HUs[-1]+1,self.no_of_outputs)\n",
    "        self.weights.append(weightsl);\n",
    "    \n",
    "    def get_weights(self):\n",
    "        return self.weights\n",
    "    \n",
    "    def sigmoid_forward(self,z):\n",
    "#         print(\"sig forward\")\n",
    "        return 1/(1+np.exp(-z))\n",
    "        \n",
    "    def sigmoid_backward(self,z):\n",
    "#         print(\"sig bckward\")\n",
    "        return z*(1-z)\n",
    "    \n",
    "    def tanh_forward(self,z):\n",
    "#         print(\"tanh forwad\")\n",
    "        return np.tanh(z) \n",
    "    \n",
    "    def tanh_backward(self,z):\n",
    "#         print(\"tanh backward\")\n",
    "        return 1-z**2\n",
    "    \n",
    "    #not working relu\n",
    "    def relu_forward(self,z):\n",
    "#         print(\"relu forwad\")\n",
    "        return z * (z > 0) \n",
    "    \n",
    "    def relu_backward(self,z):\n",
    "#         print(\"relu backward\")\n",
    "        \n",
    "        return 1 * (z > 0)\n",
    "    \n",
    "    def softplus_forward(self,z):\n",
    "        return np.log(1+np.exp(z)) \n",
    "    \n",
    "    def softplus_backward(self,z): \n",
    "        return 1-1/np.exp(z)\n",
    "    \n",
    "    def activation_forward(self,z,act_index):\n",
    "        if(self.act[act_index]==\"sigmoid\"):\n",
    "            return self.sigmoid_forward(z)\n",
    "        elif (self.act[act_index]==\"softplus\"):\n",
    "            return self.softplus_forward(z)\n",
    "        elif (self.act[act_index]==\"tanh\"):\n",
    "            return self.tanh_forward(z)\n",
    "        elif (self.act[act_index]==\"relu\"):\n",
    "            return self.relu_forward(z)\n",
    "        else:\n",
    "            print(\"error invalid activation\")\n",
    "    \n",
    "    def activation_backward(self,z,act_index):\n",
    "        if(self.act[act_index]==\"sigmoid\"):\n",
    "            return self.sigmoid_backward(z)\n",
    "        elif (self.act[act_index]==\"softplus\"):\n",
    "            return self.softplus_backward(z)\n",
    "        elif (self.act[act_index]==\"tanh\"):\n",
    "            return self.tanh_backward(z)\n",
    "        elif (self.act[act_index]==\"relu\"):\n",
    "            return self.relu_backward(z)\n",
    "        else:\n",
    "            print(\"error invalid activation\")\n",
    "    \n",
    "    def feed_forward(self, input_arr,dropout=0):\n",
    "#         print(input_arr.shape)\n",
    "        p=1-dropout\n",
    "        \n",
    "        \n",
    "        self.inps = np.ones((input_arr.shape[0],self.no_of_inputs+1))\n",
    "        self.inps[:,1:self.no_of_inputs+1]=input_arr\n",
    "        self.layers=[]\n",
    "        self.layers.append(self.inps)\n",
    "        u1 = np.random.binomial(1, p, size=self.layers[-1].shape) / p\n",
    "        i1 = np.copy(self.layers[-1])*u1\n",
    "        self.layers[-1] = np.copy(self.layers[-1])*u1\n",
    "        layer0nodes=np.matmul(i1,self.weights[0])\n",
    "        layer0nodes=self.activation_forward(layer0nodes,0)\n",
    "        layer0nodes_bias=np.ones((layer0nodes.shape[0],layer0nodes.shape[1]+1))\n",
    "        layer0nodes_bias[:,1:layer0nodes.shape[1]+1]=layer0nodes\n",
    "        #for all the internal layer nodes\n",
    "        self.layers.append(layer0nodes_bias)\n",
    "        u1 = np.random.binomial(1, p, size=self.layers[-1].shape) / p\n",
    "        i1 = np.copy(self.layers[-1])*u1\n",
    "        self.layers[-1] = np.copy(self.layers[-1])*u1\n",
    "        for i in range(1,self.no_of_hidden_layers):\n",
    "            layerinodes=np.matmul(i1,self.weights[i])\n",
    "            layerinodes=self.activation_forward(layerinodes,i)\n",
    "            layerinodes_bias=np.ones((layerinodes.shape[0],layerinodes.shape[1]+1))\n",
    "            layerinodes_bias[:,1:layerinodes.shape[1]+1]=layerinodes\n",
    "            self.layers.append(np.copy(layerinodes_bias))\n",
    "            u1 = np.random.binomial(1, p, size=self.layers[-1].shape) / p\n",
    "            i1 = np.copy(self.layers[-1])*u1\n",
    "            self.layers[-1] = np.copy(self.layers[-1])*u1\n",
    "            \n",
    "        #for the last layer\n",
    "        layer1nodes=np.matmul(self.layers[-1],self.weights[-1])\n",
    "        layer1nodes=self.activation_forward(layer1nodes,-1)\n",
    "        self.layers.append(layer1nodes)\n",
    "#         print(len(self.layers))\n",
    "        return self.layers[-1]\n",
    "        \n",
    "    def get_layers(self):\n",
    "        return self.layers\n",
    "    \n",
    "    #can use with sigmoid\n",
    "    def compute_cross_entropy_err(self,y_hat,y):\n",
    "        #cross entropy, expects one hot encoded y label\n",
    "        loss=0\n",
    "        #ylog(y_hat)+(1-y)log(1-y_hat)\n",
    "        loss_matrix=np.multiply(y,np.log(y_hat))+np.multiply(1-y,np.log(1-y_hat))\n",
    "        loss_matrix=np.nan_to_num(loss_matrix)\n",
    "        #computing sum of squares of weights\n",
    "        sum_square=0\n",
    "        for i in range(0,len(self.weights)):\n",
    "            sum_square=sum_square+np.sum((np.array(self.weights[i]))**2)\n",
    "        \n",
    "        loss=-1*np.sum(loss_matrix)*(1/y.shape[0])+self.regu*(1/y.shape[0])*sum_square\n",
    "        return loss\n",
    "    \n",
    "    def compute_gradients(self,y_hat,y):\n",
    "        #dE/d(sigma)\n",
    "        self.gradients=[]\n",
    "        dE_dsigmaL=(-1)*(np.multiply(y,1/y_hat)-np.multiply(1-y,1/(1-y_hat)))\n",
    "#         dE_dsigmaL=np.multiply(dE_dsigmaL_init,y)-np.multiply(dE_dsigmaL_init,1-y)\n",
    "        dE_dsigmaL=np.nan_to_num(dE_dsigmaL)\n",
    "        dE_dsigmaL_dsumL=np.multiply(dE_dsigmaL,self.activation_backward(self.layers[-1],-1))\n",
    "        dE_dsigmaL_dsumL=np.nan_to_num(dE_dsigmaL_dsumL)\n",
    "        #average\n",
    "        #gradient for the last layer\n",
    "        #update weight with this gradient\n",
    "        dE_dsigmaL_dsumL_dw=np.matmul(self.layers[-2].T,dE_dsigmaL_dsumL)*(1/self.layers[-2].shape[0])\n",
    "        self.gradients.insert(0,dE_dsigmaL_dsumL_dw)\n",
    "        \n",
    "        for i in range(self.no_of_hidden_layers,0,-1):\n",
    "            #weights without bias\n",
    "            weights_wo_bias=self.weights[i][1:,:]\n",
    "            dE_dsigmaL_dsumL_dsigmal=np.matmul(dE_dsigmaL_dsumL,weights_wo_bias.T)\n",
    "            layer_wo_bias=self.layers[i][:,1:]\n",
    "            dE_dsigmaL_dsumL=np.multiply(dE_dsigmaL_dsumL_dsigmal,self.activation_backward(layer_wo_bias,i-1))\n",
    "            dE_dsigmaL_dsumL=np.nan_to_num(dE_dsigmaL_dsumL)\n",
    "            dE_dsigmal_dsuml_dw=np.matmul(self.layers[i-1].T,dE_dsigmaL_dsumL)*(1/self.layers[i-1].shape[0])\n",
    "            self.gradients.insert(0,np.copy(dE_dsigmal_dsuml_dw))\n",
    "        \n",
    "        return self.gradients\n",
    "    \n",
    "    def update_weights(self, gradients,learning_rate=0.001):\n",
    "        self.lr=learning_rate\n",
    "        for i in range(0,len(self.weights)):\n",
    "            self.weights[i]=np.copy(self.weights[i])-gradients[i]*learning_rate-self.regu*learning_rate*np.copy(self.weights[i])\n",
    "    \n",
    "    #dont use predict for now\n",
    "    def predict(self,input_arr):\n",
    "        inps = np.ones((input_arr.shape[0],self.no_of_inputs+1))\n",
    "        inps[:,1:self.no_of_inputs+1]=input_arr\n",
    "        self.layers=[]\n",
    "#         self.layers.append(self.inps)\n",
    "        layer0nodes=np.matmul(self.inps,self.weights[0])\n",
    "        layer0nodes=self.sigmoid_forward(layer0nodes)\n",
    "        layer0nodes_bias=np.ones((layer0nodes.shape[0],layer0nodes.shape[1]+1))\n",
    "        layer0nodes_bias[:,1:layer0nodes.shape[1]+1]=layer0nodes\n",
    "        #output layer nodes\n",
    "#         self.layers.append(layer0nodes_bias)\n",
    "        layer1nodes=np.matmul(layer0nodes_bias,self.weights[1])\n",
    "        layer1nodes=self.sigmoid_forward(layer1nodes)\n",
    "#         self.layers.append(layer1nodes)\n",
    "#         print(len(self.layers))\n",
    "        return layer1nodes\n",
    "        \n",
    "        \n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape\n",
    "\n",
    "network1=nn(no_of_inputs=24,no_of_outputs=3,HUs=[100],regu=0,no_of_hidden_layers=1, activation=[\"tanh\",\"sigmoid\"])\n",
    "train_data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data=train_data.values\n",
    "input_x=input_data[:,:-1]\n",
    "y_label=input_data[:,-1]\n",
    "y_encod=onehotencoding(y_label)\n",
    "val_input=val_data.values\n",
    "val_x=val_input[:,:-1]\n",
    "y_val_label=val_input[:,-1]\n",
    "y_val_encod=onehotencoding(y_val_label)\n",
    "# y_encod=y_encod[[0],:]\n",
    "# input_x=input_x[[0],:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_encod;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(input_x,y_encod,no_of_epochs=10000,batchSize=100):\n",
    "    print(input_x.shape)\n",
    "    print(y_encod.shape)\n",
    "    no_of_batches=int(input_x.shape[0]/batchSize)\n",
    "    lr=0.01\n",
    "    for i in range(0,no_of_epochs):\n",
    "        loss=0\n",
    "        for batch_no in range(0,no_of_batches):\n",
    "            idx = np.random.randint(input_x.shape[0], size=batchSize)\n",
    "            input_x_train=input_x[idx,:]\n",
    "            y_encod_train=y_encod[idx,:]\n",
    "            y_pred=network1.feed_forward(input_x_train,dropout=0.5)\n",
    "\n",
    "            loss=loss+network1.compute_cross_entropy_err(y_pred,y_encod_train)\n",
    "        #         print(y_pred)\n",
    "            grads=network1.compute_gradients(y_pred,y_encod_train)\n",
    "        #     if(i%1000==0):\n",
    "        #         print(grads)\n",
    "        #         print(network1.get_weights())\n",
    "            network1.update_weights(grads,learning_rate= lr)\n",
    "        if i%50==0:\n",
    "            loss=loss*1/no_of_batches\n",
    "            print(\"train loss: \"+ str(loss))\n",
    "            \n",
    "            #checking val loss\n",
    "            y_pred_val=network1.feed_forward(val_x)\n",
    "            val_loss=network1.compute_cross_entropy_err(y_pred_val,y_val_encod)\n",
    "            print(\"val loss: \"+ str(val_loss))\n",
    "        if (i+1)%1000==0:\n",
    "            lr=lr*0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1800, 24)\n",
      "(1800, 3)\n",
      "train loss: 1.4442653367932707\n",
      "val loss: 1.1288081813908317\n",
      "train loss: 1.442669604901945\n",
      "val loss: 1.1838704075051656\n",
      "train loss: 1.4440087836784663\n",
      "val loss: 1.2000219659521978\n",
      "train loss: 1.3809457538158136\n",
      "val loss: 1.2032819784189608\n",
      "train loss: 1.472502750571847\n",
      "val loss: 1.2111435001494721\n",
      "train loss: 1.396491416666424\n",
      "val loss: 1.2130155781422707\n",
      "train loss: 1.4206746079405648\n",
      "val loss: 1.2105635357485671\n",
      "train loss: 1.4064639657685425\n",
      "val loss: 1.2161074305046364\n",
      "train loss: 1.4346846525633994\n",
      "val loss: 1.2140021228907347\n",
      "train loss: 1.4504699247691455\n",
      "val loss: 1.2173449967702015\n",
      "train loss: 1.4040760340134868\n",
      "val loss: 1.2163993280934453\n",
      "train loss: 1.4584328240024251\n",
      "val loss: 1.2149993625852171\n",
      "train loss: 1.4121693922366323\n",
      "val loss: 1.2129638087762298\n",
      "train loss: 1.3555897894553086\n",
      "val loss: 1.2112295126806556\n",
      "train loss: 1.3829023855784268\n",
      "val loss: 1.2110891088151854\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-3c496ba7fc14>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_encod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mno_of_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-17-413af731f88d>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(input_x, y_encod, no_of_epochs, batchSize)\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0minput_x_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0my_encod_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_encod\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0my_pred\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnetwork1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_x_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mnetwork1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_cross_entropy_err\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_encod_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-317641021f41>\u001b[0m in \u001b[0;36mfeed_forward\u001b[0;34m(self, input_arr, dropout)\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mu1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0mlayer0nodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0mlayer0nodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer0nodes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m         \u001b[0mlayer0nodes_bias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer0nodes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlayer0nodes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0mlayer0nodes_bias\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlayer0nodes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayer0nodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-317641021f41>\u001b[0m in \u001b[0;36mactivation_forward\u001b[0;34m(self, z, act_index)\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftplus_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mact_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m\"tanh\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mact_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m\"relu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-317641021f41>\u001b[0m in \u001b[0;36mtanh_forward\u001b[0;34m(self, z)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtanh_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;31m#         print(\"tanh forwad\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtanh_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(input_x,y_encod,no_of_epochs=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights=network1.get_weights()\n",
    "np.asarray(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28800, 3)\n",
      "0.7351388888888889\n"
     ]
    }
   ],
   "source": [
    "def trainAccuracy(input_x,y_encod):\n",
    "    y_pred=network1.feed_forward(input_x)\n",
    "    y_pred_encod=(y_pred == y_pred.max(axis=1)[:,None]).astype(int)\n",
    "    print(y_pred_encod.shape)\n",
    "#     print(np.argmax(y_pred_encod,axis=1).shape)\n",
    "    correct_pred_encod=np.multiply(y_pred_encod,y_encod)\n",
    "    accuracy=np.sum(correct_pred_encod)/input_x.shape[0]\n",
    "    print(accuracy)\n",
    "    \n",
    "trainAccuracy(input_x,y_encod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessData_test(df):\n",
    "    df['post_day'] = df['post_day'].factorize(sort=True)[0]\n",
    "    df['basetime_day'] = df['basetime_day'].factorize(sort=True)[0]\n",
    "#     df=df.head(5)\n",
    "    df_norm=(df-train_mean)/train_std\n",
    "    df_norm.fillna(0,inplace=True)\n",
    "    return df_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('./../data/test.csv')\n",
    "test_data=preprocessData_test(df_test)\n",
    "input_data=test_data.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:42: RuntimeWarning: overflow encountered in exp\n"
     ]
    }
   ],
   "source": [
    "def testOutput(input_x_test):\n",
    "    y_pred=network1.feed_forward(input_x_test)\n",
    "    y_pred_arr=np.argmax(y_pred,axis=1)\n",
    "    y_pred_arr=y_pred_arr+1\n",
    "    file=open(\"submission.csv\",\"w\")\n",
    "    file.write(\"Id,predicted_class\\n\")\n",
    "    print(len(y_pred_arr))\n",
    "    for i in range(0,len(y_pred_arr)):\n",
    "        file.write(str(i+1)+\",\"+str(y_pred_arr[i])+\"\\n\")\n",
    "    file.close()\n",
    "\n",
    "testOutput(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 2)\n",
      "(5, 2)\n",
      "[[0.49765536 0.50368654 0.49381384]\n",
      " [0.49764957 0.50367189 0.49383668]\n",
      " [0.49764852 0.50367562 0.4938206 ]\n",
      " [0.49765452 0.5036914  0.49379497]\n",
      " [0.49765335 0.50368932 0.4937968 ]]\n",
      "[array([[ 0.01624345, -0.00611756, -0.00528172],\n",
      "       [-0.01072969,  0.00865408, -0.02301539],\n",
      "       [ 0.01744812, -0.00761207,  0.00319039]]), array([[-0.0024937 ,  0.01462108, -0.02060141],\n",
      "       [-0.00322417, -0.00384054,  0.01133769],\n",
      "       [-0.01099891, -0.00172428, -0.00877858],\n",
      "       [ 0.00042214,  0.00582815, -0.01100619]])]\n",
      "[array([[ 1.        , -0.60865379,  0.23565675],\n",
      "       [ 1.        ,  1.18539118,  1.39296744],\n",
      "       [ 1.        ,  0.98290729,  0.32057733],\n",
      "       [ 1.        , -0.92479789, -1.00327539],\n",
      "       [ 1.        , -0.63484679, -0.94592614]]), array([[1.        , 0.50672107, 0.49670536, 0.50236961],\n",
      "       [1.        , 0.50695686, 0.49838439, 0.4929705 ],\n",
      "       [1.        , 0.50282263, 0.49998708, 0.49328017],\n",
      "       [1.        , 0.50216523, 0.49837905, 0.50320046],\n",
      "       [1.        , 0.50163763, 0.49889722, 0.50157791]]), array([[0.49765536, 0.50368654, 0.49381384],\n",
      "       [0.49764957, 0.50367189, 0.49383668],\n",
      "       [0.49764852, 0.50367562, 0.4938206 ],\n",
      "       [0.49765452, 0.5036914 , 0.49379497],\n",
      "       [0.49765335, 0.50368932, 0.4937968 ]])]\n"
     ]
    }
   ],
   "source": [
    "print(input_x.shape)\n",
    "input_x_part=input_x\n",
    "print(input_x_part.shape)\n",
    "y_pred=network1.feed_forward(input_x_part)\n",
    "print(y_pred)\n",
    "weights=network1.get_weights()\n",
    "print(weights)\n",
    "layers=network1.get_layers()\n",
    "print(layers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.99066523 -1.98536177 -1.97555777]\n",
      " [-1.99064225 -2.01479624 -2.02496096]\n",
      " [-1.99063813 -2.01481138 -2.02502692]\n",
      " [-2.00942613 -2.01487543 -1.97548412]\n",
      " [-1.99065726 -2.01486698 -2.0251245 ]]\n"
     ]
    }
   ],
   "source": [
    "dE_dsigmaL=(-1)*(np.multiply(y_encod,1/y_pred)+np.multiply(1-y_encod,1/(1-y_pred)))\n",
    "print(dE_dsigmaL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.         -1.98536177 -0.        ]\n",
      " [-0.         -0.         -2.02496096]\n",
      " [-0.         -0.         -2.02502692]\n",
      " [-2.00942613 -0.         -0.        ]\n",
      " [-0.         -0.         -2.0251245 ]]\n",
      "[[-0.         -0.49631346 -0.        ]\n",
      " [-0.         -0.         -0.50616332]\n",
      " [-0.         -0.         -0.5061794 ]\n",
      " [-0.50234548 -0.         -0.        ]\n",
      " [-0.         -0.         -0.5062032 ]]\n",
      "[[0.2499945  0.24998641 0.24996173]\n",
      " [0.24999448 0.24998652 0.24996201]\n",
      " [0.24999447 0.24998649 0.24996181]\n",
      " [0.2499945  0.24998637 0.2499615 ]\n",
      " [0.24999449 0.24998639 0.24996152]]\n",
      "(4, 3)\n"
     ]
    }
   ],
   "source": [
    "dE_dsigmaL=np.multiply(dE_dsigmaL,y_encod)\n",
    "print(dE_dsigmaL)\n",
    "dE_dsigmaL_dsumL=np.multiply(dE_dsigmaL,network1.sigmoid_backward(layers[-1]))\n",
    "print(dE_dsigmaL_dsumL)\n",
    "print(network1.sigmoid_backward(layers[-1]))\n",
    "print(weights[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.50672107 0.49670536 0.50236961]\n",
      " [1.         0.50695686 0.49838439 0.4929705 ]\n",
      " [1.         0.50282263 0.49998708 0.49328017]\n",
      " [1.         0.50216523 0.49837905 0.50320046]\n",
      " [1.         0.50163763 0.49889722 0.50157791]]\n",
      "[[1.         1.         1.         1.         1.        ]\n",
      " [0.50672107 0.50695686 0.50282263 0.50216523 0.50163763]\n",
      " [0.49670536 0.49838439 0.49998708 0.49837905 0.49889722]\n",
      " [0.50236961 0.4929705  0.49328017 0.50320046 0.50157791]]\n",
      "[[-0.         -0.49631346 -0.        ]\n",
      " [-0.         -0.         -0.50616332]\n",
      " [-0.         -0.         -0.5061794 ]\n",
      " [-0.50234548 -0.         -0.        ]\n",
      " [-0.         -0.         -0.5062032 ]]\n",
      "[[-0.1004691  -0.09926269 -0.30370918]\n",
      " [-0.05045209 -0.0502985  -0.1530104 ]\n",
      " [-0.05007169 -0.04930431 -0.15157809]\n",
      " [-0.0505561  -0.04986656 -0.15062244]]\n",
      "(5, 3)\n"
     ]
    }
   ],
   "source": [
    "print(layers[-2])\n",
    "print(layers[-2].T)\n",
    "print(dE_dsigmaL_dsumL)\n",
    "dE_dsigmaL_dsumL_dw=np.matmul(layers[-2].T,dE_dsigmaL_dsumL)*(1/layers[-2].shape[0])\n",
    "print(dE_dsigmaL_dsumL_dw)\n",
    "#update weight with this gradient\n",
    "print(dE_dsigmaL_dsumL.shape)\n",
    "#weights without bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.         -0.49631346 -0.        ]\n",
      " [-0.         -0.         -0.49383668]\n",
      " [-0.         -0.         -0.4938206 ]\n",
      " [-0.49765452 -0.         -0.        ]\n",
      " [-0.         -0.         -0.4937968 ]]\n",
      "[[-0.00322417 -0.01099891  0.00042214]\n",
      " [-0.00384054 -0.00172428  0.00582815]\n",
      " [ 0.01133769 -0.00877858 -0.01100619]]\n",
      "[[ 0.00190611  0.00085578 -0.00289259]\n",
      " [-0.00559897  0.00433519  0.00543526]\n",
      " [-0.00559879  0.00433505  0.00543508]\n",
      " [ 0.00160452  0.00547366 -0.00021008]\n",
      " [-0.00559852  0.00433484  0.00543482]]\n",
      "(5, 3)\n",
      "(5, 4)\n"
     ]
    }
   ],
   "source": [
    "print(dE_dsigmaL_dsumL)\n",
    "weights_wo_bias=weights[1][1:,:]\n",
    "print(weights_wo_bias.T)\n",
    "dE_dsigmaL_dsumL_dsigmal=np.matmul(dE_dsigmaL_dsumL,weights_wo_bias.T)\n",
    "print(dE_dsigmaL_dsumL_dsigmal)\n",
    "print(dE_dsigmaL_dsumL_dsigmal.shape)\n",
    "print(network1.sigmoid_backward(layers[-2]).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2.01468662 -2.0021407  -1.99843484]\n",
      " [-2.0147238  -1.99790979 -2.00170064]]\n",
      "[[-0.50364489 -0.5005346  -0.4996084 ]\n",
      " [-0.50365405 -0.4994769  -0.5004248 ]]\n",
      "[[1.         0.50537419 0.49874734]\n",
      " [1.         0.50550913 0.48727871]]\n",
      "(3, 3)\n",
      "[[-0.50364947 -0.50000575 -0.5000166 ]\n",
      " [-0.25456542 -0.2527237  -0.25272925]\n",
      " [-0.24830572 -0.24651238 -0.24651236]]\n",
      "(2, 3)\n",
      "[[ 0.00423022 -0.00211824]\n",
      " [ 0.00426253 -0.00213153]]\n",
      "(2, 2)\n",
      "(2, 3)\n",
      "[[1.         0.50537419 0.49874734]\n",
      " [1.         0.50550913 0.48727871]]\n",
      "[[ 0.00105743 -0.00052956]\n",
      " [ 0.0010655  -0.00053254]]\n",
      "(2, 3)\n",
      "(2, 2)\n",
      "[[ 0.00106147 -0.00053105]\n",
      " [ 0.00030971 -0.00015447]\n",
      " [ 0.0008667  -0.0004333 ]]\n",
      "[[ 0.01624345 -0.00611756]\n",
      " [-0.00528172 -0.01072969]\n",
      " [ 0.00865408 -0.02301539]]\n"
     ]
    }
   ],
   "source": [
    "dE_dsigmaL=(-1)*(np.multiply(y_encod,1/y_pred)+np.multiply(1-y_encod,1/(1-y_pred)))\n",
    "print(dE_dsigmaL)\n",
    "\n",
    "dE_dsigmaL_dsumL=np.multiply(dE_dsigmaL,network1.sigmoid_backward(layers[-1]))\n",
    "print(dE_dsigmaL_dsumL)\n",
    "print(layers[1])\n",
    "print(weights[1].shape)\n",
    "#average\n",
    "dE_dsigmaL_dsumL_dw=np.matmul(layers[-2].T,dE_dsigmaL_dsumL)*(1/layers[-2].shape[0])\n",
    "print(dE_dsigmaL_dsumL_dw)\n",
    "#update weight with this gradient\n",
    "print(dE_dsigmaL_dsumL.shape)\n",
    "#weights without bias\n",
    "weights_wo_bias=weights[1][1:,:]\n",
    "dE_dsigmaL_dsumL_dsigmal=np.matmul(dE_dsigmaL_dsumL,weights_wo_bias.T)\n",
    "print(dE_dsigmaL_dsumL_dsigmal)\n",
    "print(dE_dsigmaL_dsumL_dsigmal.shape)\n",
    "print(network1.sigmoid_backward(layers[-2]).shape)\n",
    "#layer without bias\n",
    "print(layers[-2])\n",
    "layer_wo_bias=layers[-2][:,1:]\n",
    "dE_dsigmal_dsuml=np.multiply(dE_dsigmaL_dsumL_dsigmal,network1.sigmoid_backward(layer_wo_bias))\n",
    "print(dE_dsigmal_dsuml)\n",
    "print(layers[0].shape)\n",
    "print(dE_dsigmal_dsuml.shape)\n",
    "dE_dsigmal_dsuml_dw=np.matmul(layers[0].T,dE_dsigmal_dsuml)*(1/layers[0].shape[0])\n",
    "print(dE_dsigmal_dsuml_dw)\n",
    "print(weights[0])\n",
    "# dE_dsigmaL_dsumL_dw_mean=np.mean(dE_dsigmaL_dsumL_dw,axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.73105858 0.73105858 0.73105858 0.73105858]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "range(0, 10)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "range(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 3., 3., 1., 3.])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=np.array([])\n",
    "a=np.append(a,2)\n",
    "a=np.append(a,3)\n",
    "a.shape[0]\n",
    "a[-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "9\n",
      "8\n",
      "7\n",
      "6\n",
      "5\n",
      "4\n",
      "3\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "for i in range(10,1,-1):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def a(b=[2,3]):\n",
    "    c=b[0]\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1],\n",
       "       [0, 1, 0]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.binomial(1, 0.5, size=(2,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
